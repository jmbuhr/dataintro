{
  "hash": "3444c55d884673803c3d7ed5d586a708",
  "result": {
    "markdown": "---\naliases:\n  - probability-and-hypothesis-testing.html\n---\n\n# Probability and Hypothesis Testing\n\n\n\n\n\n\n> ... in which we reason about the nature of randomness and\n  discover various statistical tests.\n\n\n{{< youtube LBU22HxJm6I >}}\n\n\n\n## Motivation\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-2_3bcc09b9d27f7e703b26e593a00adef5'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\nIn the first four lectures we covered the fundamentals of handling data with R.\nNow, we will shift our focus away from the **how** and towards the **why** of\ndata analysis. We will talk about different statistical tests, common mistakes,\nhow to avoid them and how to spot them in other research. But of course, we will\ndo so using R. So you will still learn one or the other useful function or\ntechnique along the way. In most instances it should be clear when I use R\nsolely to demonstrate an idea from statistics and the code is just included for\nthe curious, or whether the code is something you will likely also use for your\nown analysis. I am open for questions if things are unclear in any of the two\ncases. For purely aesthetic code I might also speed up the typing in the edit.\n\nFor the longer text parts it might be helpful to look at the script while watching the video or pause frequently to take your own notes (Rmarkdown is great for your lecture notes as well!).\n\n\n## Statistically Significant...\n\n> ...you keep using that word. I don't think it means what you think it means.\n\n![statistically significant](images/statistically-significant.jpg)\n\nYou will hear the phrases \"statistically significant\", \"significant\" or even\n\"very significant\" thrown around quite a bit in academic literature . And while\nthey are often used carelessly, they have a clearly defined meaning. A meaning\nwe will uncover today. This meaning is related to the concept of so called\np-values, which have an equally bad reputation for frequently being misused. The\np in p-value stands for **probability**, so in order to understand p-values, we\nneed to understand probability and learn how to deal with randomness, chance, or\nluck if you will. So...\n\n## Getting our Hands dirty with Probability\n\n> To understand statistics means understanding the nature of randomness first.\n\n\n::: {.cell fig.asp='1' hash='05-probability-and-hypothesis-testing_cache/html/chess-board_92435f74fed64cc68e6431942b8e6cef'}\n::: {.cell-output-display}\n![A ggplot chessboard](05-probability-and-hypothesis-testing_files/figure-html/chess-board-1.png){width=100%}\n:::\n:::\n\n\nSay you and you friend are playing a game of chess, when your friend proudly\nproclaims:\\\n\"I am definitely the better player!\".\\\n\"Proof it!\", you reply.\\\n\"That's easy\", she says: \"I won 7 out of the 8 rounds we played to today.\"\\\n\"Pah! That's just luck.\" is your less witty and slightly stubborn response.\n\nAs expected, we shall be using R to resolve this vital conflict.\n\n![R rainbow](images/paste-5E31A2FF.png)\n\n### Definitions: Hypothesis\n\nBoth of you involuntarily uttered an hypothesis, a testable assumption. And we\nwant to test these hypothesis using statistics. The first hypothesis (\"I am the\nbetter player.\") is what we call the **alternative hypothesis** ($H_1$). The\nname can be a bit confusing, because most often, this is your actual scientific\nhypothesis, the thing you are interested in. So, alternative to what? It is\nalternative to the so called **null hypothesis** ($H_0$), which is the second\nstatement (\"This is just luck\"). The null hypothesis provides a sort of baseline\nfor all our findings. It usually goes along the lines of \"What if our\nobservations are just based on chance alone?\", where \"chance\" can be any source\nof random variation in our system.\n\nThe tricky part is that there is no way to directly test the alternative\nHypothesis, all we can test is the null hypothesis. Because for any null\nhypothesis we discard, there are always multiple alternative hypothesis that\ncould explain our data. In our example, even if we end up discarding the idea of\nour friend's chess success being only down to luck, this does not prove the\nalternative hypothesis that she is the better player (she could still be\ncheating for example). Do keep this in mind when we transfer this to a more\nscientific setting. Just because we show that something is unlikely to have\narisen by chance does not mean that your favorite alternative hypothesis is\nautomatically true.\n\nSo, after these words of warning, let's test some null hypothesis!\n\n### Testing the Null Hypothesis with a Simulation\n\nWe will start off by building a little simulation. Before testing any\nhypothesis, it is important to have defined $H_0$ and $H_1$ properly, which we did in\nthe previous section. But we need to be a little more specific. Winning by\nchance would entail a completely random process, which we can model with a coin\nflip. R has the lovely function `sample` to take any number of things from a\nvector, with or without replacement after taking each thing:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-3_49274ef2861c71eb8b4fe7df8b5491f6'}\n\n```{.r .cell-code}\ncoin <- c(\"heads\", \"tails\")\nsample(coin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tails\" \"heads\"\n```\n:::\n:::\n\n\nNot giving it a number of things to draw just shuffles the vector, which is\nfairly boring in the case of just two tings. We can't sample 10 things from a\nvector of only two elements\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-4_a15ddeb974f5782e20e29722db4c42d6'}\n\n```{.r .cell-code}\nsample(coin, size = 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\n```\n:::\n:::\n\n\nBut we can, if we put the thing back every time:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-5_9247fb4257c3510e960b9a88e3620d32'}\n\n```{.r .cell-code}\nsample(coin, 10, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"tails\" \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\"\n[10] \"heads\"\n```\n:::\n:::\n\n\nSo, let's make this a little more specific to our question:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-6_b9c81375b1e7777ffa9f28ca3d732378'}\n\n```{.r .cell-code}\nwinner <- c(\"you\", \"friend\")\nrandom_winners <- sample(winner, size = 8, replace = TRUE)\nrandom_winners\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"friend\" \"friend\" \"you\"    \"you\"    \"friend\" \"you\"    \"friend\" \"friend\"\n```\n:::\n\n```{.r .cell-code}\nrandom_winners == \"friend\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n```\n:::\n\n```{.r .cell-code}\n1 + TRUE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-7_6e5a3b4688b92b2a6f9cab8adcee9632'}\n\n```{.r .cell-code}\n1 + FALSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-8_727c66b7cde22eca1aa7360dc0718f70'}\n\n```{.r .cell-code}\nsum(random_winners == \"friend\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n:::\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-9_32b0e7851d04df51e429e7cdaebd2624'}\n\n```{.r .cell-code}\nmean(random_winners == \"friend\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.625\n```\n:::\n:::\n\n\nIf we were to run this script a million times, the resulting proportion\nof random wins for both of you would be very, very close to 50-50 because we\nused a fair coin. However, we don't have the time to play this much Chess and we\nsure don't have the money to run a million replicates for each experiment in the\nlab. But here, in our little simulated world, we have near infinite\nresources (our simulation is not to computationally costly).\n\n> One trick used above: When we calculate e.g. a sum or mean,\n  R automatically converts TRUE to 1 and FALSE to 0.\n  \nLet's create a function that returns a random number of wins your friend would have\ngotten by pure chance for a number of rounds `N`.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-10_8aaf7b9c8a82c4aebe1a7e812b4964d9'}\n\n```{.r .cell-code}\nget_n_win <- function(N) {\n  winner <- c(\"you\", \"friend\")\n  random_winners <- sample(winner, size = N, replace = TRUE)\n  sum(random_winners == \"friend\")\n}\n\nget_n_win(8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\nThis number is different every time, so how does it change?\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-11_0ca40de5b404299ab25674e6c44bb549'}\n\n```{.r .cell-code}\nresult <- map_dbl(rep(8, 1000), get_n_win)\nhead(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 5 5 2 3 5\n```\n:::\n:::\n\n\nA histogram is a type of plot that shows how often each value occurs in a\nvector. Usually, the values are put into bins first, grouping close values\ntogether for continuous values, but in this case it makes sense to just have one\nvalue per bin because we are dealing with discrete values (e.g. no half-wins).\nHistograms can either display the raw counts or the frequency e.g. as a\npercentage. In ggplot, we use `geom_bar` when we don't need any binning, just\ncounting occurrences, and `geom_histogram` when we need to bin continuous\nvalues.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-12_97a8957edb37b172bab62f7a321db95d'}\n\n```{.r .cell-code}\ntibble(result) %>% \n  ggplot(aes(x = result)) +\n  geom_bar() +\n  labs(x = \"N wins for friend\",\n       title = \"Throwing a coin 8 times\") +\n  scale_x_continuous(breaks = 0:8)\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n\nAs expected, the most common number of wins out of 8 is 4 (unless I got really\nunlucky when compiling this script). Let us see, how this **distribution**\nchanges for different values of `N`. First, we set up a grid of numbers (all\npossible combinations) so that we can run a bunch of simulations:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-13_c389584e2edc71338c3e4af132d64a23'}\n\n```{.r .cell-code}\nsimulation <- crossing(\n   N = 1:15,\n   rep = 1:1000\n) %>% \n  mutate(\n    wins = map_dbl(N, get_n_win)\n  )\n```\n:::\n\n\nAnd then we use our trusty ggplot to visualize all the distributions.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-14_63ef0571b5cf7daa21efb483ab1a4faa'}\n\n```{.r .cell-code}\nsimulation %>% \n  ggplot(aes(wins)) +\n  geom_bar() +\n  facet_wrap(~N, labeller = label_both) +\n  labs(title = \"Flipping a coin N times\")\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-14-1.png){width=100%}\n:::\n:::\n\n\nWith a fair coin, the most common number of wins should be half of the number of\ncoin flips. Note, how it is still possible to flip a coin 15 times and and not\nwin a single time. It is just very unlikely and the bars are so small that we\ncan't see them.\n\nLet us go back to the original debate. The first statement: \"I am better.\" is\nsomething that can never be definitively proven. Because there is always the\npossibility, no matter how small, that the same result could have arisen by pure\nchance alone. Even if she wins 100 times and we don't take a single game from\nher, this sort of outcome is still not *impossible* to appear just by flipping a\ncoin. But what we can do, is calculate, how likely a certain event is under the\nassumption of the null hypothesis (only chance). And we can also decide on some\nthreshold $\\alpha$ at which we reject the null hypothesis. This is called the\n**significance threshold**. When we make an observation and then calculate that\nthe probability for an observation like this or more extreme is smaller than the\nthreshold, we deem the result **statistically significant**. And the probability\nthus created is called the **p-value**.\n\nFrom our simulation, we find the that probability to win 7 out of 8 rounds under\nthe null hypothesis is:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-15_6ab30afac64ff25ba52ade9ba10c8679'}\n\n```{.r .cell-code}\nsimulation %>% \n  filter(N == 8) %>% \n  summarise(\n    mean(wins >= 7)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  `mean(wins >= 7)`\n              <dbl>\n1             0.038\n```\n:::\n:::\n\nWhich is smaller than the commonly used significance threshold of $\\alpha=0.05$\n(i.e. $5\\%$). So with 7 out of 8 wins, we would **reject the null hypothesis**.\nDo note that this threshold, no matter how commonly and thoughtlessly it is used\nthroughout academic research, is completely arbitrary.\n\n### Getting precise with the Binomial Distribution\n\nNow, this was just from a simulation with 1000 trials, so the number can't be\narbitrarily precise, but there is a mathematical formula for this probability.\nWhat we created by counting the number of successes in a series of yes-no-trials\nis a **binomial distribution**. For the most common distributions, R provides a\nset of functions. the functions starting with `d` give us the probability\ndensity function. In the case of discrete values like counting wins, this is\nequivalent to the actual probability, but for continuous values we obtain the\nprobability by taking the integral. We get these integrals with the\ncorresponding functions starting with `p` (for probability).\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-16_d8dcb4eadbac3f72748c5e4823c07f9e'}\n\n```{.r .cell-code}\ndbinom(x = 7, size = 8, prob = 0.5) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03125\n```\n:::\n:::\n\n\nThis is the probability to win **exactly 7 out of 8** games. But what we wanted\nwas the probability for **7 or more out of 8**! So we move to the integral. This\npart can get a bit confusing, because the default for `pbinom` is\n`lower.tail = TRUE`, which according to the help page means that probabilities\nit returns $P[X \\le x]$.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-17_ea27bc5320c82a2da6109e4a9800d031'}\n\n```{.r .cell-code}\npbinom(q = 7, size = 8, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9960938\n```\n:::\n:::\n\n\nIf we set `lower.tail` to `FALSE` , we get $P[X > x]$, so the probability for a\nrandom variable X being bigger than a number x. So to get the probability that\nwe are interested in, we need to replace the 7 with a 6 as well:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-18_edc46af343efb7994fa5c43115833eb0'}\n\n```{.r .cell-code}\npbinom(q = 6, size = 8, prob = 0.5, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03515625\n```\n:::\n:::\n\n\nOur simulation was pretty close! So the exact values agrees and we reject the\nnull hypothesis of both opponents being equally good.\nHere is the full graph for the probability density function of the binomial\ndistribution.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-19_4c012f52d481e7554fa3e27078e32526'}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = function(x) dbinom(x = x, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-19-1.png){width=100%}\n:::\n:::\n\n\nAnd the integral, the probability $P[X \\le x]$.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-20_0b44d7112b9c922af3d5212b785925f8'}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = function(q) pbinom(q = q, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-20-1.png){width=100%}\n:::\n:::\n\n\nThere is two more functions I want to showcase from this family. The third is the\nso called **quantile function**. Quantiles divide a probability distribution\ninto pieces of equal probability. One example for a quantile is the 50th\npercentile, also known as the median, which divides the values such that half of\nthe values are above and half are below. And we can keep dividing the two halves\nas well, so that we end up with more quantiles. Eventually, we arrive at the\nquantile function. It is the inverse of the probability function, so you obtain\nit by swapping the axis.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-21_1ccff5721a856d0109c70e97295a3297'}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = function(p) qbinom(p = p, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 10, limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-21-1.png){width=100%}\n:::\n:::\n\n\nQuantiles will also be useful for\ndeciding if a random sample follows a certain distribution\nwith quantile-quantile plots.\n\nLastly, there is always also an `r` variant of the function, which gives us any\nnumber of random numbers from the distribution.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-22_a680f782f416361c2454878b464c8d33'}\n\n```{.r .cell-code}\nrbinom(10, 8, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5 5 6 4 4 2 5 3 5 3\n```\n:::\n:::\n\n\n\n### But how much better? Understanding Effect Size and Power, False Positives and False Negatives\n\nWe decided to abandon the null hypothesis that both players are equally good,\nwhich equates to a 50% win-chance for each player. But we have not determined\n**how much better** she is. And how much better does she need to be for us to\nreliably discard the null hypothesis after just 8 games? The generalization of\nthe **how much better** part, the **true difference**, is called the **effect\nsize**.\n\nOur ability to decide that something is statistically significant when there is\nin fact a true difference is called the statistical **power**. It depends on the\neffect size, our significance threshold $\\alpha$ and the sample size $n$ (the\nnumber of games). We can explore the concept with another simulation.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-23_cf9b1ec7427adbf27cc313263283fcac'}\n\n```{.r .cell-code}\nreps <- 10000\nsimulation <- crossing(\n  N = c(8, 100, 1000, 10000),\n  true_prob = c(0.5, 0.8, 0.9)\n) %>% \n  rowwise() %>% \n  mutate(\n    wins = list(rbinom(n = reps, size = N,  prob = true_prob)),\n  ) %>% \n  unnest(wins) %>% \n  mutate(\n    p = pbinom(q = wins - 1, size = N, prob = 0.5, lower.tail = FALSE)\n  )\n\nhead(simulation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n      N true_prob  wins     p\n  <dbl>     <dbl> <int> <dbl>\n1     8       0.5     3 0.855\n2     8       0.5     3 0.855\n3     8       0.5     4 0.637\n4     8       0.5     1 0.996\n5     8       0.5     3 0.855\n6     8       0.5     6 0.145\n```\n:::\n:::\n\n\nI also introduced a new piece of advanced dplyr syntax. `rowwise` is similar to\n`group_by` and essentially puts each row into its own group. This can be useful\nwhen working with list columns or running a function with varying arguments and\nallows us to treat the inside of `mutate` a bit like as if we where using one of\nthe `map` functions. For more information, see [the documentation\narticle](https://dplyr.tidyverse.org/articles/rowwise.html).\n\nIt leaves us with 10000 simulated numbers of wins at N games for different\ntrue probabilities of her winning (i.e. how much better our friend is). We then\ncalculate the probability to have this or a greater number of wins under the\nnull hypothesis (equal probability for win and loss), in other words: the\np-value.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-24_608b1d68180226807708491fcf6659e3'}\n\n```{.r .cell-code}\nsimulation %>%\n  ggplot(aes(p)) +\n  geom_histogram() +\n  facet_wrap(~ true_prob + N,\n             labeller = label_both,\n             scales = \"free_y\",\n             ncol = 4) +\n  geom_vline(xintercept = 0.05, color = \"red\") +\n  labs(x = \"p-value\",\n       y = \"frequency\") +\n  scale_y_continuous(breaks = NULL)\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-24-1.png){width=100%}\n:::\n:::\n\n\nWe notice a couple of things in this plot. As the number of games\nplayed approaches very high numbers, the p-values for the\ncase where the null hypothesis is in fact true (both players have\nthe same chance of winning), start following a uniform distribution,\nmeaning for a true null hypothesis, all p-values are equally likely.\nThis seems counterintuitive at first, but is a direct consequence\nof the definition of the p-value.\nThe consequence of this is, that if we apply our regular significance\nthreshold of 5%, by definition we will say that there is a true\ndifference, even though there is none (i.e. the null hypothesis is true\nbut we falsely reject it and favor of our alternative hypothesis).\nThis is called a **false positive**. By definition, we will get at\nleast $\\alpha$ false positives in all of our experiments.\nLater, we will learn, why the real number of false positives is\neven higher.\nAnother name for false positives is **Type I errors**.\n\nOn the other side of the coin, there are also cases\nwhere there is a true difference (we used winning probabilities\nof 0.8 and 0.9), but we don't reject the null hypothesis because\nwe get a p-values larger than $alpha$.\nThese are all **false negatives** and their rate is sometimes\nreferred to as $\\beta$.\nAnother name for false negatives is **Type II errors**.\nPeople don't particularly like talking about negative things like errors,\nso instead you will often see the inverse of $\\beta$, the\n**Statistical Power** $1-\\beta$.\nThe proportion of correctly identified positives out of\nthe actual positives is also shown on the plot below.\nFor example, say her true win probability is 90%\nand we play 8 games. If this experiment runs in an infinite\nnumber of parallel universes, we will conclude that she is\nbetter than chance in 80% of those.\nWe could set our significance threshold higher to detect\nmore of the true positives, but this would\nalso increase our false positives.\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-25_1906fd8922b7be6c006295caea671076'}\n\n```{.r .cell-code}\nsimulation %>%\n  group_by(true_prob, N) %>%\n  summarise(signif = mean(p <= 0.05)) %>% \n  ggplot(aes(true_prob, signif, fill = true_prob == 0.5)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(label = signif), vjust = -0.2) +\n  facet_wrap(~N,\n             labeller = label_both) +\n  scale_y_continuous(expand = expansion(c(0, 0.1))) +\n  scale_fill_viridis_d() +\n  labs(y = \"Proportion of significant results\")\n```\n\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-25-1.png){width=100%}\n:::\n:::\n\n\nThere are also packages out there, which have a function\nto compute the power for the binomial test, but I think\nthe simulation was way more approachable.\nThe cool thing about simulations is also, that they work\neven when there is no analytical solution, so you\ncan use them to play around when planning an experiment.\n\n## P-Value Pitfalls\n\nLet us look into some of the pitfalls of p-values.\nRemember from the definition of p-values, that we will get\na significant result even if there is no true difference in\n5% of cases (assuming we use this as our alpha)?\nWell, what if we test a bunch of things?\nThis is called **Multiple Testing** and there is a problem\nassociated with it:\n\nIf you test 20 different things, and your statistical\ntest will produce a significant result by chance alone\nin 5% of cases, the expected number of significant results is 1.\nSo we are not very surprised.\nSpeaking of surprised: In his book, available for free online,\n[\"Statistics done wrong\"](https://www.statisticsdonewrong.com/), Alex Reinhart\ndescribes p-values as a \"measure of surprise\":\n\n> »A p value is not a measure of how right you are,\n  or how significant the difference is;\n  it’s a measure of how surprised you should be if there is no actual difference\n  between the groups, but you got data suggesting there is.\n  A bigger difference, or one backed up by more data,\n  suggests more surprise and a smaller p value.«\n> --- Alex Reinhart [@reinhartStatisticsDoneWrong2015]\n\nSo, we are not very surprised, but if you focus to hard on\nthe one significant result, trouble ensues.\nIn a \"publish or perish\" mentality, this can easily happen,\nand negative findings are not published nearly enough,\nso most published findings are likely exaggerated.\nJohn Bohannon showcased this beautifully by\nrunning a study on chocolate consumption and\ngetting it published:\n[I Fooled Millions Into Thinking Chocolate Helps Weight Loss. Here's How.](https://io9.gizmodo.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800)\n\nWhat can we do about this?\n\n### Multiple Testing Correction\n\nThe simplest approach is to take all p-values calculate\nwhen running a large number of comparisons and\ndividing them by the number of tests performed.\nThis is called the **Bonferroni correction**\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-26_e473e5b1942a018f916b1bd728ef5764'}\n\n```{.r .cell-code}\np_values <- c(0.5, 0.05, 0.3, 0.0001, 0.003)\np.adjust(p_values, method = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.0000 0.2500 1.0000 0.0005 0.0150\n```\n:::\n:::\n\n\nOf course, this looses some statistical power\n(remember, no free lunch).\nA slightly more sophisticated approach to\ncontrolling the false discovery rate (FDR)\nis the Benjamini-Hochberg procedure. It retains\na bit more power. Here is what happens:\n\n- Sort all p-values in ascending order.\n- Choose a FDR $q$ you are willing to accept\n  and call the number of tests done $m$.\n- Find the largest p-value with:\n  $p \\leq iq/m$ with its index $i$.\n- This is your new threshold for significance\n- Scale the p-values accordingly\n\nAnd this is how you do it in R:\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-27_c87dd17dd679b0ae9f7427ae4f36223c'}\n\n```{.r .cell-code}\np.adjust(p_values, method = \"fdr\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.50000000 0.08333333 0.37500000 0.00050000 0.00750000\n```\n:::\n:::\n\n\n### Other forms of p-hacking\n\nThis sort of multiple testing is fairly obvious.\nYou will notice it, when you end up with a large\nnumber of p-values, for example when doing a genetic\nscreening and testing thousands of genes.\nOther related problems are harder to spot.\nFor a single research question there are often different\nstatistical tests that you could run, but trying\nthem all out and then choosing the one that best\nagrees with your hypothesis is not an option!\nLikewise, simply looking at your data is a form of\ncomparison if it influences your choice of statistical test.\nIdeally, you first run some exploratory experiments\nthat are not meant to test your hypothesis, then\ndecide on the tests you need, the sample size you\nwant for a particular power and then run the actual\nexperiments designed to test your hypothesis.\n\nAt this point, here is another shout-out to\nAlex Reinharts book [@reinhartStatisticsDoneWrong2015].\nIt is a very pleasant read\nand also shines more light on some of the other\nforms of **p-hacking**.\n\n## Bayesian Statistics and the Base Rate Fallacy\n\nThere is another more subtle problem called the\nbase rate fallacy. As an example, we assume\na medical test, testing for a certain condition.\nIn medical testing, different words are used\nfor the same concepts we defined above^[This is a slightly annoying trend\nin statistics; as it enters different fields, people come\nup with new names for old things\n(perhaps the most notorious field for this is\nmachine learning).].\n\nHere, we have:\n\n- Sensitivity = Power = true positive rate = $1-\\beta$\n- Specificity = true negative rate = $1-\\alpha$\n\nLet us assume a test with a\nsensitivity of 90% and a specificity\nof 92%. When we visit the doctor to\nget a test, and get a positive result,\nwhat is the probability, that we are in\nfact positive (i.e. a true positive)?\nWell, the test has a specificity of 92%,\nso if we where negative, it would have detected\nthat in 92% of cases, does this mean, that we can\nbe 92% certain, that we are actually positive?\n\nWell, **no**. What we are ignoring here\nis **base rate**, which for diseases is called\nthe **prevalence**. It is the proportion at\nwhich a disease exists in the general population.\n\nSo, let us say, we are picking 1000 people\nat random from the population and testing\nthem. We are dealing with a hypothetical\ncondition that affects 1% of people,\nso we assume 10 people in our sample to be positive.\nOf those 10 people, 9 will be tested positive\n(due to our sensitivity),\nthose will be our true positives.\nThe remaining 1 will be a false negative.\nHowever, we are of course also testing\nthe negatives (if we knew ahead of time\nthere would be no point in testing) and of\nthose due to our specificity, 8% will also be\ntested positive, which is 0.08 * 990, so\nwe get 79 false positives.\nBecause there are so many negatives in our\nsample, even a relatively high specificity\nwill produce a lot of false positives.\nSo that actual probability of being\npositive with a positive test result is\n\n$$\\frac{true~positives}{true~positives + false~positives}=10\\%$$\n\n\n::: {.callout-warning}\nTODO: fix the plot!\n(the `waffle` package is broken)\n:::\n\n\n::: {.cell hash='05-probability-and-hypothesis-testing_cache/html/unnamed-chunk-28_62b1733e64eae29a76794f9786920394'}\n::: {.cell-output-display}\n![](05-probability-and-hypothesis-testing_files/figure-html/unnamed-chunk-28-1.png){width=100%}\n:::\n:::\n\n\nFormally, this is described by Bayes's Formula\n\n$$P(A|B)=\\frac{P(B|A)*P(A)}{P(B)}$$\n\nRead: The probability of A given B is the probability\nof B given A times the probability of A divided by\nthe probability of B.\n\nIn bayesian statistics, the prevalences are known\nas **priors**.\n\n## Concepts discussed today\n\nAfter today you should be familiar with the following concepts:\n\n- Null and alternative hypothesis\n- P-values and statistical significance\n- Binomial distribution\n- Probability density, probability and quantile functions\n- Effect size and statistical power\n- False positives, false negatives\n- Multiple testing and p-hacking\n- Bayes's Theorem\n\n## Exercises\n\n## A Fair Coin\n\nWe have a regular old coin and flip it 100 times.\nGiven a significance threshold $\\alpha$ of 0.05,\nwith what probability do we (mistakenly) reject the null hypothesis i.e.\nconclude that the coin is not fair even though it is?\nCan you show this with a simulation?\nAs a tip I can tell you that due to the vectorized nature\nof the functions involved you won't need a `map` or loop.\nThe shortest version I could think of uses only 3 functions.\n\n## An Unfair Game\n\nWe are playing a game where you have to roll the most sixes in order to win.\nSomeone is trying to fool us and is using a loaded die.\nThe die is manipulated such that the chance of rolling\na six is 35% instead of the usual 1/6.\nAt the same significance threshold as above, what is the\nchance of us rejecting the null hypothesis (= a fair die)\nand thus concluding correctly that we are being tricked\nafter rolling the die 20 times?\nYou will have to run a simulation here again.\n\n## Discovering a new Distribution\n\nThe binomial distribution was concerned with sampling\nwith replacement (you can get head or tails any number\nof times without using up the coin). In this exercise\nyou will explore sampling **without** replacement.\nThe common model for this is an urn with two\ndifferent colored balls in it.\nThe resulting distribution is called the\n**hypergeometric distribution** and the\ncorresponding R functions are `<r/d/p/q>hyper`\n\n- Imagine you are a zoo manager.\n  - We got a gift from another zoo! It consists\n    of 8 red pandas and 2 giant pandas.\n    What is the probability that they end up\n    properly separated, if we randomly take 8 animals,\n    put them in one enclosure and put the rest in another?\n  - Our penguin colony hatched eggs and we have\n    a bunch of newcomers. We have have 15 males and\n    10 females. If we look at a random subset of\n    12 penguins, what does the distribution of the\n    number of males look like? Which number is most\n    likely? How likely is it, to get at least\n    9 males in the sample?\n\n## Resources\n\n- <https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108>\n- <https://jimgruman.netlify.app/post/education-r/>\n- [P-Value histograms blogpost by David Robinson](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/)\n- [\"Statistics done wrong\"](https://www.statisticsdonewrong.com/)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}