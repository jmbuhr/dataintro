{
  "hash": "9eb7a2f46e6dec7309f3c0cf34ff91fa",
  "result": {
    "markdown": "# Fallacies, Correlation and Regression\n\n\n\n\n\n\n> ... in which we hear Stories of Warplanes,\n  Correlation and Regression and explore the Datasaurus Dozen.\n  \n\n\n{{< youtube vX1WGlFNtBo >}}\n\n\n\n## Setup\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-2_b79f63164cdc60c88c726ffa561e75fc'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(broom)\n```\n:::\n\n\n## Data Considerations\n\n### 1943\n\nIt is 1943. The second World War is well underway, ravaging large parts of Europe.\nMilitary aircraft that had first entered the stage in World War I are now\nreaching their peak importance as they rain fire from the skies. But the Allied\nforces are facing a problem. As warplanes get better, so do anti-aircraft\nsystems. In an effort to improve the survival of their fleet, the US military\nstarts examining the planes returning from skirmishes with the opposing forces.\nThey characterize the pattern of bullet holes in the metal hull, meticulously\nnoting down each hit that the plane sustained. The resulting picture is better\nsummarized in the modern, redrawn version below.\n\n![Figure from Wikipedia @SurvivorshipBias2020.](images/survivorship-bias.png)\n\nAfter taking a look at the data they gathered, the military is ready to rush\ninto action. To improve the endurance of their aircraft, the plan is to\nreinforce the parts of the plane that were most often hit by bullets. With\nstronger wings and a sturdier body of the plane, they think, surely more pilots\nwill come back from their missions safely. They were wrong.\n\nBut the pilots where in luck. The military also consulted with the Statistics\nResearch Group at Columbia University. A man named Abraham Wald worked there. In\nhis now unclassified report \"A method of estimating plane vulnerability based on\ndamage of survivors\", he argued against the generals' conclusion\n[@waldReprintMethodEstimating1980]. Instead of the most-hit parts of the planes,\nthe least-hit parts are to be reinforced.\n\n![Cover of \"A method of estimating plane vulnerability based on damage of survivors\" @waldReprintMethodEstimating1980](images/paste-57122EF0.png)\n\n> Instead of the most-hit parts, the least-hit parts are to be reinforced.\n\nThe reason for this seemingly counterintuitive result is what is now known as\n*survivorship bias*. The data that was collected contained only survivors, those\nplanes that sustained damage not severe enough to hinder them from coming back\nafter their mission. The aircraft that where hit in other places simply didn't\nmake it back. Consequently, Wald advised to reinforce the engines and the fuel\ntanks.\n\n### Thinking further\n\nThis is but one of a multitude of biases, specifically a selection bias, that\nwill influence the quality of the inferences you can draw from available data.\nKeep in mind, data is not objective and never exists in a vacuum. There is\nalways context to consider. The way the data was collected is just one of them.\nA lot of these ideas seem obvious in hindsight, which incidentally is another\nbias that social psychologists call *hindsight bias*, but they can sometimes be\nhard to spot.\n\nA common saying is that music was better back in the days, or that all the old\nmusic still holds up while the new stuff on the radio just sounds the same.\nWell, not quite. This is also survivorship bias at work. All the bad and\nforgettable songs from the past just faded into oblivion, never to be mentioned\nagain, while the songs people generally agreed to be good survived the ravages\nof time unscathed.\nA similar thing happens with success in general, not just songs.\nIf you ask any CEO high up the corporate ladder, a millionaire, or the author of a\nbook that reads \"How to get rich\", they are sure to have a witty anecdote about\nhow their persistence, or their brilliance, or charisma got them to where they are\nnow. What we are not seeing is all the people just as witty, just as charismatic\nor even just as persistent that where simply not as lucky. Very few people will\ntell you this. Because it takes a whole lot of courage to admit that ones\nsuccess is based on luck and privilege.\n\nAnd to take it back to the scientific context: When you are planning an\nexperiment for the lab, always ask whether your data collection process can in\nsome way be biased towards what you are trying to show.\n\nI leave you with this:\n\n![](./images/survivorship-tweet.png)\n\nAnd from this cautionary tale we jump straight back into RStudio.\n\n## Sidenotes\n\n### Glue and Inline R Code\n\nUsing `paste` to create a text in which the\nvalues of variables are inserted can be painful.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-3_f996b0dd89346ef05afa9a595d6eac31'}\n\n```{.r .cell-code}\nname <- \"Jannik\"\nage <- 26\ntext <- paste(name, \"is\", age, \"years old.\")\ntext\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Jannik is 26 years old.\"\n```\n:::\n:::\n\n\nThe `glue` package makes it a breeze.\nEverything inside of curly braces in the text inside of the `glue`\nfunction will be evaluated as regular R code,\nenabling us to write text quite naturally: \n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-4_c43e049e60a95355b57f4bab8f0ff961'}\n\n```{.r .cell-code}\ntext <- glue(\"{name} is {age} years old.\")\ntext\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJannik is 26 years old.\n```\n:::\n:::\n\n\nI hope you are not too confused by the package and it's main function having the\nsame name.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-5_3e2859d07a195f49cecf6ea092e86012'}\n\n```{.r .cell-code}\nglue(\"{name} is {age + 10} years old.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJannik is 36 years old.\n```\n:::\n:::\n\n\n### Inline R code\n\nUsing the a backtick followed by the letter `r` we\ncan add the results of code right into the text sections\nof Rmarkdown reports:\n\n1 + 1 = 2.\n\nJannik is 26 years old.\n\n### Best Practices\n\nSpeaking of being careful.\nThere is one rule I can give you to make your data analysis more secure:\n\n> **Your raw data is sacred!**\n  Do not ever modify it or save over it.\n\nThis is even more important when,\nfor example, using excel to preview a csv file.\nUnder no circumstances should you hit the save button\nin excel when you are looking at the raw data.\nWith approximately one-fifth of genomic research papers containing\nerrors in the gene lists, because excel converted genes\nsuch as _SEPT2_ (Septin 2) into dates, you can see why [@ziemannGeneNameErrors2016].\nBiologists have since given up and renamed the genes that\nwhere commonly converted into dates... but the point still stands.\nThis caution is of course also necessary when analyzing data\nwith R, not just excel. When we read in the raw data and\nsave a processed version, we create a new file, or even\nbetter, a new folder for it. A good convention for example\nwould be do divide your data into a `raw` and `derived` folder.\n\n## Covariance, Correlation and Regression\n\n![Source: https://xkcd.com/552/](images/correlation.png)\n\nLast week, we talked about a measure of the spread of a\nrandom variable called the **variance**.\n\n$$var(X) = \\frac{\\sum_{i=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}$$\n\nToday, we are extending this idea to 2 random variables.\nBecause the normal distribution is so common, we are using\ntwo normally distributed variables.\n\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-6_d0dca6394ff98254898f1c7713a7a7bd'}\n\n```{.r .cell-code}\nN <- 50\ndf <- tibble(\n  x = rnorm(N),\n  y = rnorm(N)\n)\n\nm_x <- mean(df$x)\nm_y <- mean(df$y)\n\nggplot(df, aes(x, y)) +\n  geom_vline(xintercept = m_x, alpha = 0.8, color = \"midnightblue\") +\n  geom_hline(yintercept = m_y, alpha = 0.8, color = \"midnightblue\") +\n  geom_point(fill = \"white\", color = \"black\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n\nWe also added lines for the\nmeans of the two random\nvariables. Maybe I should have mentioned\nthis more clearly earlier on,\nbut the general convention in statistics is that random variables\nare uppercase and concrete values from the distribution have the\nsame letter but lowercase.\n\nWe now get the **covariance** of X and Y as:\n\n$$cov(X,Y)=\\text{E}\\left[(X-\\text{E}\\left[X\\right])(Y-\\text{E}\\left[Y\\right])\\right]$$\n\nThe expected value $E[X]$ is just a fancy way of saying\nthe mean of X.\nIf we asses the contribution of individual points towards the\ncovariance, we can understand it quite intuitively.\nA point that has a higher x than the mean of X and a higher\ny than the mean of Y (top right quadrant) will push the covariance towards\npositive values. Likewise, a point in the bottom left quadrant\nwill have negative differences with the X and Y mean, which cancel\neach other out to result in a positive covariance.\nThe bottom right and top left quadrants push towards a negative\ncovariance. A mix of positive and negative contributions will\nresult in a covariance with a small absolute value.\n\nThe covariance has one problem: It will have weird units\n(X times Y) and the scale is different depending on the random\nvariables.\nSo what we do is standardize it by dividing by both standard\ndeviations and get the **correlation coefficient**:\n\n$$cor(X,Y)=\\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}$$\n\nIt can assume values between -1 and 1. It's full name is\n_Pearson product-moment correlation coefficient_, or\n_pearsons R_. We can square it to get $R^2$ (obviously),\nwhich indicates the strength of the correlation with\nvalues between 0 and 1 independent of the direction.\nWe will meet it again later.\n\nLet us apply our knowledge to a new dataset.\n\n### Introducing the Dataset\n\nThe `dplyr` package includes and example dataset of Star Wars\ncharacters. Unfortunately, it was created a while ago,\nso the is no baby yoda, but 87 other characters are present.\n\n![I guess it is the baby yoda show now.](images/baby_yoda.gif)\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-7_5981932680fe6154cd1d790ae4195a9c'}\n\n```{.r .cell-code}\nstarwars\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   <chr>     <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 <NA>       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 <NA>       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 <NA>       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n#   vehicles <list>, starships <list>\n```\n:::\n:::\n\n\nLet's look at some correlations:\n\n### Pearson vs. Spearman (not a Boxing Match)\n\nTo compute pearsons correlation, we use the `cor` function in R.\nInstead of filtering out `NA`, we can use\n`use = \"complete.obs\"` to ignore rows with missing values in the computation.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-8_f3c3d1c79bdbeddc11d5de9a1eeae896'}\n\n```{.r .cell-code}\npearson <- cor(starwars$height, starwars$mass, use = \"complete.obs\")\npearson\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1338842\n```\n:::\n:::\n\n\nWhen I first did this I was surprised that the correlation was so\nlow. We are after all talking about height and mass, which\nI assumed to be highly correlated.\nLet us look at the data to see what is going on.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-9_b9bc547b9e1e6692530c99e321d275f9'}\n\n```{.r .cell-code}\nlabel_text <- glue(\"Pearson correlation: {round(pearson, 2)}\")\n\njabba <- filter(starwars, str_detect(name, \"Jabba\"))\njabba_text <- list(x = 1100, y = 120)\n\nstarwars %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 500, y = 75, label = label_text,\n           hjust = 0) +\n  annotate(geom = \"curve\",\n           x = jabba_text$x, y = jabba_text$y,\n           xend = jabba$mass, yend = jabba$height,\n           curvature = .3,\n           arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\",\n           x = jabba_text$x,\n           y = jabba_text$y, label = \"Jabba the Hutt\",\n           hjust = 1.1) +\n  xlim(0, 1500) +\n  labs(x = \"mass [kg]\",\n       y = \"height [cm]\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n\nThis is the culprit! We have a massive outlier,\nin all senses of the word \"massive\".\nLuckily, there is another method to asses correlation.\nSpearman's method is more resistant to outliers,\nbecause the data is transformed into ranks first,\nwhich negates the massive effect of outliers.\n\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-10_bb1510622623e9debc08d6dfa876898d'}\n\n```{.r .cell-code}\nspearman <- cor(starwars$height, starwars$mass,\n                use = \"complete.obs\", method = \"spearman\")\nspearman\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7516794\n```\n:::\n:::\n\n\nVisually, this is what the points look like\nafter rank transformation:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-11_86553c8f5c7d722e996b98cc7395cc46'}\n\n```{.r .cell-code}\nlabel_text <- glue(\"Spearman rank correlation: {round(spearman, 2)}\")\n\nstarwars %>% \n  mutate(mass = rank(mass),\n         height = rank(height)) %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 0, y = 75, label = label_text,\n           hjust = 0) +\n  labs(x = \"rank(mass)\",\n       y = \"rank(height)\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-11-1.png){width=100%}\n:::\n:::\n\n\nApart from `cor`, there is also `cor.test`, which gives more information. \nIf we so fancy, we can use `broom` to turn the test output into\na tidy format as well.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-12_ac98b8e8665ea992d8ea873c95e298f1'}\n\n```{.r .cell-code}\ncortest <- cor.test(starwars$mass, starwars$height,\n                # method = \"spearman\",\n                use = \"complete.obs\")\n\ncortest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  starwars$mass and starwars$height\nt = 1.02, df = 57, p-value = 0.312\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1265364  0.3770395\nsample estimates:\n      cor \n0.1338842 \n```\n:::\n:::\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-13_b5475fad12890a2ed02acd7896b86fbb'}\n\n```{.r .cell-code}\ntidy(cortest)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>      <chr>      \n1    0.134      1.02   0.312        57   -0.127     0.377 Pearson's… two.sided  \n```\n:::\n:::\n\n\nThere is another way we can specify which features to correlate.\n`corr` also takes a matrix or data frame as it's x argument instead\nof x and y.\nWe then end up with the pairwise correlation coefficients\nfor all columns of the dataframe.\n\nThis is known as a correlation matrix, and we can create it for\nmore than two features, as long as all features are numeric\n(after all, what is the correlation between 1,4 and \"cat\" \"dog\"?).\nUnfortunately there are only three numeric columns\nin the `starwars` dataset, which makes for a pretty boring\ncorrelation matrix.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-14_24408c3223530d6285cdf228691c6793'}\n\n```{.r .cell-code}\nstarwars %>%\n  select(where(is.numeric)) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  height  mass birth_year\n   <int> <dbl>      <dbl>\n1    172    77       19  \n2    167    75      112  \n3     96    32       33  \n4    202   136       41.9\n5    150    49       19  \n6    178   120       52  \n```\n:::\n:::\n\n\nSo let's look at another built-in dataset instead.\n`mtcars` has some data about cars, like their\nengine displacement or miles per gallon.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-15_2b06497a468a118f9a797a06421c9d91'}\n\n```{.r .cell-code}\nmtcars %>% \n  ggplot(aes(disp, mpg)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-15-1.png){width=100%}\n:::\n:::\n\n\nThis makes for a much more interesting correlation matrix:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-16_4b0d1617a124a4d8115c967a14171a74'}\n\n```{.r .cell-code}\ncor(mtcars) %>% \n  as_tibble(rownames = \"feature\") %>% \n  pivot_longer(-feature) %>% \n  ggplot(aes(feature, name, fill = value)) +\n  geom_raster() +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"blue\", high = \"red\",\n                       mid = \"white\", midpoint = 0)\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-16-1.png){width=100%}\n:::\n:::\n\n\nIf you are working a lot with correlations, it is certainly\nworth checking out the `corrr` package from the tidymodels framework:\n\n<aside>\n<a href=\"https://corrr.tidymodels.org/\">\n![](images/corrr.png){width=200}\n</a>\n</aside>\n\nIts functions make these steps easier.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-17_69bfa3db0a0460688cb95cc1902eb2a2'}\n\n```{.r .cell-code}\ncorrr::correlate(mtcars) %>% \n  corrr::stretch()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 121 × 3\n   x     y          r\n   <chr> <chr>  <dbl>\n 1 mpg   mpg   NA    \n 2 mpg   cyl   -0.852\n 3 mpg   disp  -0.848\n 4 mpg   hp    -0.776\n 5 mpg   drat   0.681\n 6 mpg   wt    -0.868\n 7 mpg   qsec   0.419\n 8 mpg   vs     0.664\n 9 mpg   am     0.600\n10 mpg   gear   0.480\n# ℹ 111 more rows\n```\n:::\n:::\n\n\nAnd give use access to two different types of plots out of the box.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-18_4640bfaa35e6d36c131d39c9f83742c2'}\n\n```{.r .cell-code}\ncorrr::correlate(mtcars) %>% \n  corrr::rplot()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-18-1.png){width=100%}\n:::\n:::\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-19_da094f1094b3ef37ec6815d0a9bcfda4'}\n\n```{.r .cell-code}\ncorrr::correlate(mtcars) %>% \n  corrr::network_plot()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-19-1.png){width=100%}\n:::\n:::\n\n\n### Difference to Linear Regression\n\nFinally, linear regression is\na related concept, because both correlation and\nlinear regression quantify the strength of a linear\nrelationship. However, there are key differences.\nWhen we fit a linear model like:\n\n$$y \\sim a + x * b$$\n\nthere is no error in x. We assume x is something that\nis fixed, like the temperature we set for an experiment\nor the dosage we used. Y on the other hand is a random\nvariable. In `cov(X,Y)` and `cor(X,Y)`, X and Y are both random variables,\nusually things we observed, not set ourselves.\n\nWhile the correlation coefficient is symmetrical and translation-scale-invariant:\n\n$$cor(X,Y)=cor(Y,X)$$\n\n\n\n$$cor(X,Y)=cor(X * a +b,Y * c + d)$$\n\nThe same is **not** true for linear models!\n\nLet us look at an example where linear regression\nis more appropriate than correlation.\nIn the `data` folder we find the IMDB ratings for 10\nStar Wars movies (plus more features).\n\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-20_27f07469e2be8dbbfbeac38dea5a05cf'}\n\n```{.r .cell-code}\nratings <- read_rds(\"data/07/starwars_movies.rds\")\nratings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 25\n   Title    Rated Released   Runtime Genre Director Writer Actors Plot  Language\n   <chr>    <chr> <date>     <chr>   <chr> <chr>    <chr>  <chr>  <chr> <chr>   \n 1 Star Wa… PG    1977-05-25 121 min Acti… George … Georg… Mark … Luke… English \n 2 Star Wa… PG    1980-06-20 124 min Acti… Irvin K… Leigh… Mark … Afte… English \n 3 Star Wa… PG    1983-05-25 131 min Acti… Richard… Lawre… Mark … Afte… English \n 4 Star Wa… PG-13 2015-12-18 138 min Acti… J.J. Ab… Lawre… Daisy… As a… English \n 5 Star Wa… PG    1999-05-19 136 min Acti… George … Georg… Ewan … Two … English…\n 6 Star Wa… PG-13 2005-05-19 140 min Acti… George … Georg… Hayde… Thre… English \n 7 Star Wa… PG    2002-05-16 142 min Acti… George … Georg… Hayde… Ten … English \n 8 Star Wa… PG-13 2017-12-15 152 min Acti… Rian Jo… Rian … Daisy… The … English \n 9 Rogue O… PG-13 2016-12-16 133 min Acti… Gareth … Chris… Felic… In a… English \n10 Star Wa… PG-13 2019-12-20 141 min Acti… J.J. Ab… Chris… Daisy… In t… English \n# ℹ 15 more variables: Country <chr>, Awards <chr>, Poster <chr>,\n#   Ratings <list>, Metascore <chr>, imdbRating <dbl>, imdbVotes <dbl>,\n#   imdbID <chr>, Type <chr>, DVD <date>, BoxOffice <chr>, Production <chr>,\n#   Website <chr>, Response <chr>, year <dbl>\n```\n:::\n:::\n\n\nWe can fit a linear model to see if the production year\nhas an effect on the rating.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-21_dda450285554960c1937afa944ec4222'}\n\n```{.r .cell-code}\nmodel <- lm(imdbRating ~ year, data = ratings)\n\naugment(model) %>% \n  ggplot(aes(year, imdbRating)) +\n  geom_smooth(method = \"lm\", alpha = 0.3, color = \"midnightblue\") +\n  geom_segment(aes(x = year, y = .fitted,\n                   xend = year, yend = imdbRating),\n               alpha = 0.4) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-21-1.png){width=100%}\n:::\n:::\n\n\nWhat I added here as gray segments are the so called **residuals**.\nThey are what makes linear regression work.\nIt's full name is **Ordinary Least Squares** and the squares in\nquestion are the squares of these residuals, the word _least_\nindicates that these squares are minimized in order to find the\nbest fit line.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-22_d343b9edc065a53d703e62fe9b3cd0d3'}\n\n```{.r .cell-code}\nbroom::tidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  77.1      28.3         2.73  0.0260\n2 year         -0.0348    0.0141     -2.46  0.0393\n```\n:::\n:::\n\n\nLooks like every year decreases the estimated rating by 0.03.\n\nOne thing however is the same between correlation and\nlinear regression, and that is the $R^2$ value we get\nfrom both calculations:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-23_0e457815deb9f6a58032abb6225fb9a2'}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = imdbRating ~ year, data = ratings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1000 -0.2467  0.1261  0.3880  0.7913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 77.13043   28.29937   2.726   0.0260 *\nyear        -0.03478    0.01414  -2.460   0.0393 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6872 on 8 degrees of freedom\nMultiple R-squared:  0.4306,\tAdjusted R-squared:  0.3595 \nF-statistic: 6.051 on 1 and 8 DF,  p-value: 0.03933\n```\n:::\n:::\n\n\nWe can interpret $R^2$ as the fraction of the variance of\nthe response variable y that can be explained by the\npredictor x.\n\n## Non-linear Least Squares\n\nSo far, we only properly dealt with linear relationships\nand now it is time to get non-linear.\nWe will be creating a mechanistically driven\npredictive model, so we have a formula of which\nwe want to adjust the parameters so that it fits our data.\n\nLet's take classical Michaelis-Menten-Kinetics\nThere is a dataset for enzyme reaction rates included in R.\nBut we convert it from a \ndataframe to a tibble so that it prints\nnicer:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-24_3bdd956ac96315293feb476384309a64'}\n\n```{.r .cell-code}\npuromycin <- as_tibble(Puromycin)\npuromycin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 23 × 3\n    conc  rate state  \n   <dbl> <dbl> <fct>  \n 1  0.02    76 treated\n 2  0.02    47 treated\n 3  0.06    97 treated\n 4  0.06   107 treated\n 5  0.11   123 treated\n 6  0.11   139 treated\n 7  0.22   159 treated\n 8  0.22   152 treated\n 9  0.56   191 treated\n10  0.56   201 treated\n# ℹ 13 more rows\n```\n:::\n:::\n\n\nThe initial rate $v_0$ of the an\nenzymatic reaction was measured\nfor a control and a sample treated\nwith puromycin at different substrate\nconcentrations.\nFor every concentration we\nhave two replicates except for\none missing replicate.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-25_3e79df046b9529652fa97b514d883fbd'}\n\n```{.r .cell-code}\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-25-1.png){width=100%}\n:::\n:::\n\n\n\nFrom our Biochemistry studies, we know\nthat we can express the rate depending\non the concentration with the following\nformula:\n\n$$rate=\\frac{(Vm * conc)}{(K + conc)}$$\n\nTo make it easier to work with, let's\nturn it into a function.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-26_22cee6c981a9c52b7fbef3f406167354'}\n\n```{.r .cell-code}\nrate <- function(conc, Vm, K) {\n  Vm * conc / (K + conc)\n}\n```\n:::\n\n\nLet's pick some arbitrary starting values.\nFor example, we see that the maximal velocity\ncould be around 200.\nWe also know that K is the concentration at which the half-maximal\nvelocity is reached.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-27_ae843cc7109595054c493f7451b4d856'}\n\n```{.r .cell-code}\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ rate(conc = .x, Vm = 200, K = 0.2),\n                color = \"black\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-27-1.png){width=100%}\n:::\n:::\n\n\n`geom_function` expects a function of x or an anonymous function\nwhere the first argument is the values on the x-axis,\nso this is what we did.\nWell, I bet we can do better than guessing the function!\nWhat R can do for us is the same it did for linear least squares\nand that is minimizing the distance of our curve to the\ndatapoints.\nThis is the job of the `nls` function, which stands for\n**Nonlinear Least Squares**.\n\n### One model\n\nLet's look at just the \"treated\" data first.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-28_e70c053e925766e13766362282e326b4'}\n\n```{.r .cell-code}\ntreated <- filter(puromycin, state == \"treated\")\nmodel <- nls(rate ~ rate(conc, Vm, K),\n             data = treated,\n             start = list(Vm = 200, K = 0.3)\n             )\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: treated\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06\n```\n:::\n:::\n\n\nNlS needs starting values, so we use any guess that isn't too far off.\nIf it is completely wrong, the model doesn't know in which direction it\nshould move the parameters to improve the fit and we get an error like this:\n`Error in nls(rate ~ rate(conc, Vm, K), data = puro, subset = state ==  : singular gradient`\n\n> For this special case, R also has a self-starting model. I won't go\n  into it because it is not as useful as the general concept of fitting\n  arbitry functions, but you can check out `SSmicmen` for a model that\n  estimes the starting values automatically.\n\nAdditionally, `nls` takes an argument `subset`, which works\nlike the `dplyr` verb `filter` so that we can fit\nthe model on a subset of the data without having to create it beforehand.\n\nWe use the `broom` package to display our model parameters in a tidy tibble.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-29_5b7a3160200de5361c248d4c3b6228e5'}\n\n```{.r .cell-code}\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term  estimate std.error statistic  p.value\n  <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n1 Vm    213.       6.95        30.6  3.24e-11\n2 K       0.0641   0.00828      7.74 1.57e- 5\n```\n:::\n:::\n\n\nWith the base-R function `predict` we can make new predictions\nbased on a model and new data:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-30_dc12be35b25126789137dfba45312b79'}\n\n```{.r .cell-code}\nhead(predict(model, newdata = list(conc = seq(0, 1, 0.01))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.00000 28.69405 50.56602 67.79038 81.70621 93.18326\n```\n:::\n:::\n\n\nWe can use the same function inside of `geom_function`:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-31_cddf50ad3596e378e4676239c3520ab3'}\n\n```{.r .cell-code}\ntreated %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ predict(model, newdata = list(conc = .x)),\n                color = \"black\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-31-1.png){width=100%}\n:::\n:::\n\n\nOr alternatively create a new dataset of predictions\nbeforehand and use that with `geom_line`:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-32_820ac3dbd68a52a5704176648918b29e'}\n\n```{.r .cell-code}\npredictions <- tibble(\n  conc = seq(0, 1, 0.01),\n  rate = predict(model, newdata = list(conc = conc))\n)\n\ntreated %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions, color = \"black\")\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-32-1.png){width=100%}\n:::\n:::\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-33_9f90449da28963a19442ee048cbc6109'}\n\n```{.r .cell-code}\naugment(model) %>% \n  ggplot(aes(conc, .resid)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-33-1.png){width=100%}\n:::\n:::\n\n\n### Multiple models\n\nNow, what if we want to fit the model for both states?\nWe can resort back to our trusty `purrr` package like\nwe did in an earlier lecture.\n\nWe start out by creating a function that takes\na dataframe and fits our model:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-34_13ebad41c5e50cca972ac84cec27d445'}\n\n```{.r .cell-code}\nfit_micmen <- function(data) {\n  nls(rate ~ rate(conc, Vm, K),\n      data = data,\n      start = list(Vm = 200, K = 0.3)\n  ) \n}\n```\n:::\n\n\nAnd by nesting the data (grouped by state)\ninto a list column we can map\nthis function over each dataset.\nAnd to get the fitted parameters\nwe map the `tidy` function from `broom`\nover the fitted models.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-35_23cf1d95e97fc33a593ed92c8f1357f2'}\n\n```{.r .cell-code}\nmodels <- puromycin %>% \n  group_by(state) %>% \n  nest() %>% \n  mutate(\n    model = map(data, fit_micmen),\n    params = map(model, tidy)\n  )\n```\n:::\n\n\nLet's inspect the fitted parameters.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-36_a0925169391d7dcbedebc6efebc19850'}\n\n```{.r .cell-code}\nmodels %>% \n  select(state, params) %>% \n  unnest(params) %>% \n  select(state, term, estimate) %>% \n  pivot_wider(names_from = term, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n# Groups:   state [2]\n  state        Vm      K\n  <fct>     <dbl>  <dbl>\n1 treated    213. 0.0641\n2 untreated  160. 0.0477\n```\n:::\n:::\n\n\nTo plot our fitted models we have two options.\nFirstly, we could generate the predicted values\nfor a number of concentrations beforehand\nand then plot these:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-37_490e6411a2e3e69c4314d9bbebd318ac'}\n\n```{.r .cell-code}\nmake_predicions <- function(model) {\n  tibble(\n    conc = seq(0, 1.2, 0.01),\n    rate = predict(model, newdata = list(conc = conc))\n  )\n}\n\npredictions <- models %>% \n  mutate(\n    preds = map(model, make_predicions)\n  ) %>% \n  select(state, preds) %>% \n  unnest(preds)\n\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions)\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-37-1.png){width=100%}\n:::\n:::\n\n\nOr we use `geom_smooth`, which can take \"nls\" as a method as well.\nWe just need to make sure to pass the correct arguments.\nAnd it can be confusing, because when we are specifying\nthe formula in `geom_smooth`, it always needs to be \na formula of `y ~ x`, whereas in the normal `nls` we did\nearlier, we specified the variables in terms of their\nactual names (`rate` and `conc`).\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-38_495b7bef589400ace3aaf617541b9416'}\n\n```{.r .cell-code}\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_smooth(method = \"nls\",\n              formula = y ~ rate(conc = x, Vm, K),\n              method.args = list(start = list(Vm = 200, K = 0.3)),\n              se = FALSE\n              )\n```\n\n::: {.cell-output-display}\n![](fallacies-correlation-and-regression_files/figure-html/unnamed-chunk-38-1.png){width=100%}\n:::\n:::\n\n\nWe also need `se = FALSE`, because by default R would\ntry to plot a confidence interval around the fit-line\nlike it did for the linear model, but `nls` doesn't return one,\nso we would get an error.\n\nThe unfortunate thing about this method is that we end up\nfitting the model twice, once to get the estimated parameters\nand the likes for ourselves and a second time in ggplot\nto display the fitted lines. But in most cases this is not\na problem, because the model is not very computationally expensive.\n\n### Excursion: A weird error message\n\nFinally, I want to take minute to mention another approach\nwhich we took earlier in the series when we where fitting\nmany linear models and show you, why it unfortunately\ndoes not work here.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-39_51a4eacf203e0475e7ea96f98154d9cd'}\n\n```{.r .cell-code}\nnewmodels <- puromycin %>% \n  group_by(state) %>% \n  summarise(\n    model = list(nls(rate ~ rate(conc, Vm, K),\n                start = list(Vm = 200, K = 0.3))\n    )\n  )\n```\n:::\n\n\nAt first it looks like everything is fine.\nBecause we are inside of a dplyr verb `nls` know\nwhere to look for the columns `rate` and `conc` that it should\nfit, so we are not specifying its `data` argument.\nHowever, this fails in an unexpected way when we later try\nto make predictions with one of the models:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-40_75f189c5f6dc4bccb9fdee6352c3e657'}\n\n```{.r .cell-code}\nmake_predicions(newmodels$model[[1]])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError:\n! Obsolete data mask.\n✖ Too late to resolve `rate` after the end of `dplyr::summarise()`.\nℹ Did you save an object that uses `rate` lazily in a column in the\n  `dplyr::summarise()` expression ?\n```\n:::\n:::\n\n\nThe reason for this as follows:\nWhen `nls` fit the model it didn't remember the actual values\nof `rate` and `conc`, it just made a note that these are columns\navailable in the data.\nAnd because the data was not passed to it explicitly it\njust wrote down that the columns are available in the\nenvironment in which it was called, which at that\ntime was inside of `summarise`.\nCheck out the `data` argument here:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-41_b61c48822e79567e64eef804090e177c'}\n\n```{.r .cell-code}\nnewmodels$model[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: parent.frame()\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06\n```\n:::\n:::\n\n\nIt just says `parent.frame`, meaning \"the environment around me\".\nBut once it has left the context of `summarise`,\nthis is no longer available, so it can't find the `rate` column.\nThis is why is is always safer to pass the data\nexplicitly like we did in the approach that worked.\n\n## Exercises\n\n### The Datasaurus Dozen\n\nThe Datasaurus Dozen [@matejka2017] is a dataset\ncrafted to illustrate certain concepts.\nIt can be accessed from R via the `datasauRus` package.\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-42_5e767838433e32711610e38107d6d8bd'}\n\n```{.r .cell-code}\ndatasauRus::datasaurus_dozen\n```\n:::\n\n\n- Explore the dataset before looking at the publication\n  above (it contains spoilers...):\n  - It actually contains 13 different datasets,\n    denoted by the column `dataset`, in one tibble.\n    What are the means for x and y for the different datasets?\n    What are the standard deviations for x and y for the different datasets?\n    What are the correlations coefficients for the different datasets?\n    I bet you notice a pattern by now.\n  - Now create one (or multiple) scatterplots of the data.\n    What do you notice? what conclusions do you draw from this observation?\n    \nThere is another dataset in the package to illustrate a different\npoint:\n\n\n::: {.cell hash='fallacies-correlation-and-regression_cache/html/unnamed-chunk-43_603b1f768b13728bfc84455780f94cb6'}\n\n```{.r .cell-code}\ndatasauRus::box_plots\n```\n:::\n\n\n- First, turn it into a tidy format, much like the `datasaurus_dozen`\n  tibble.\n- Now, visualize the distributions of the values for the\n  5 different groups. Try out different versions of your plot\n  until you are satisfied, but be sure to also include a boxplot\n  and compare it to your approaches.\n  What do you find?\n\n### Fit a non-linear model\n\nI found this gloriously 2000s website for \"Statistical Reference Datasets\":\n<https://www.itl.nist.gov/div898/strd/index.html>\nby the Information Technology Laboratory.\nNot only has this official website of the United Stats Government\namazing unapologetic Word-Art,\nit also features some handy datasets to\npractice fitting non-linear models (<https://itl.nist.gov/div898/strd/nls/nls_main.shtml>)!\n\nOf these I chose one for you to explore:\n<https://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Chwirut2.dat>\n\nBecause you might come across some challenges, I am leaving \nsome tips below, hidden behind `details` panels,\nso you can choose if and when you need them:\n\n<details>\n<summary>\nTip 1\n</summary>\nYou can read in data that is separated by whitespace with `readr`s function\n`read_table`.\n</details>\n\n<details>\n<summary>\nTip 2\n</summary>\nYou have to skip the first 60 lines and set column names manually.\n</details>\n\n\n<details>\n<summary>\nTip 3\n</summary>\nThe description in the dataset header also contains\nthe function to fit and potential starting values to try out.\nNote, the `e` in the function refers to the remaining\nerror of the fit, so you don't need it in your function.\n</details>\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}