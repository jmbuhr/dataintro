# Functions

> ... which is all about functions, bringing the whole tidyverse together and
  exploring advanced dplyr data wrangling techniques.

::: {.video-container}
<iframe class="video" src="https://www.youtube.com/embed/lwnZaHczseA" allowfullscreen></iframe>
:::

## Motivation

My goal today is to bring together everything
we learned so far and solidify
our understanding of wrangling data in the tidyverse.
If all goes according to plan,
we will then have more mental capacity
freed up for the statistics starting next week.
And our understanding of data will hopefully
enable us to experiment and play with statistical
concepts without getting stuck too much
on data wrangling.
This also means that today's lecture might
be the most challenging so far, because
everything learned up until now will -- in
one way or another -- be relevant.

As you might be able to tell,
mental models are one of my favorite topics. We
are starting today with a powerful mental model 
teased at last week. I am talking about
**iteration**.

## Iteration

Iteration is the basic idea of doing one thing multiple times. This is an area
where computers shine, so in this chapter we will learn to fully utilize the
power at our fingertips.

We had our first encounter with iteration in a very implicit form. When we use
R's basic math operators, the computer is iterating behind the scenes. Take this
expression:

```{r}
1:3 + 1:3
```

This operation is vectorized. Without us having to tell R to do so, R will add
the first element of the first vector to the first element of the second vector
and so forth.

Notice, how it looks like the operation happens all at the same time. But in
reality, this is not what happens. The computer is just really fast at adding
numbers, one after the other.

The mathematical operations in R call another programming language that does the
actual addition. This other programming language is closer to the way computers
*think*, making it less fun to write for us humans, but also faster because the
instructions are easier to translate into actions for our computer processor.

When we find a task that we want to apply to multiple things, and this task is
not already vectorized, we need to make our own iteration. There are actually
two schools of though on how to talk about iteration and consequently, how to
write it in code.

I will use a realworld example to illustrate this:
**Reading in Multiple Files**

### Functional Programming

Remember the `gapminder` dataset? Well, we are working with it again, but this
time, our collaborator sent us one csv-file for each continent. As usual, we
load the tidyverse first. For the script I also load the rmarkdown package and
the knitr package to have more control over printing tables.

```{r}
library(tidyverse)
library(rmarkdown)
library(knitr)
```

We already know how to read in *one* csv-file:

```{r}
# n_max = 3 is just here for the script
read_csv("data/04/Africa.csv", n_max = 3)
```

We have a function (`read_csv`) that **takes** a file path and **returns**
(spits out) the data. In this first school of thought, the Functional
Programming style, the next idea is to have a function, that takes two things: a
function and a vector (atomic or list). And it feeds the individual elements of
the vector to the function, one after another. In mathematics, the relation
between a set of inputs and a set of outputs is called a
**map**, which is where the name of the following family of functions comes
from. In the tidyverse, these functional programming concepts live in the
`purrr` package.

<aside>
<a href="https://purrr.tidyverse.org/">
![]images/purrr.png)
</a>
</aside>

First, we create the vector of things that we want to iterate over, the things
that will be fed into our function one after the other:

```{r}
paths <- dir("data/04/", full.names = TRUE)
paths
```

Now we have 5 file paths. We can test our understanding by passing just one of
them to `read_csv` to make sure our function works:

```{r}
read_csv(paths[[1]], n_max = 3)
```

It is time for the map function!

```{r}
all_datasets <- map(paths, read_csv)
```

That's it! We now have a list that contains all five datasets.

But we lost the information about which file the elements of the list came from,
which is the name if the continent! We can fix this by using a named list
instead of a regular list to pass to `map`:

```{r}
names(paths) <- basename(paths) %>%
  str_remove("\\.csv")

my_data <- map(paths, read_csv)
```

The function `str_remove` is part of the `stringr` package
in the tidyverse. It's functions, which handle
all kinds of operations on text, start with `str_`.

<aside>
<a href="https://stringr.tidyverse.org/">

```{r stringr-logo, echo=FALSE, out.extra="class=external"}
include_graphics("images/stringr.png")
```

</a>
</aside>

And now we can combine them into one big dataset
using `bind_rows`, which stacks tibbles on top of each other.
The extra argument `.id` determines the name for the column
in which we store the names of the list
items.

```{r}
my_data %>%
  bind_rows(.id = "continent")
```

There is yet another shortcut we can employ. The `purrr` package contains
various variants of the `map` function. `map` itself will always return a list,
whereas variants like `map_chr` always return an atomic character
vector,`map_dbl` always returns numbers, `map_lgl` always return logical (yes or
no, TRUE / FALSE) vectors. Combining a list into one dataframe (by rows) is so
common that there is also a special `map` function for this: `map_dfr`.

```{r}
gapminder <- map_dfr(paths, read_csv, .id = "continent")
gapminder
```

Now that was efficient! It only took us a couple of lines.
We can even do the whole thing in one chain of functions
using the pipe.
But keep in mind that this is not the way we came
up with it.
No one expects you to come up with the shortest and
most concise solution on the first go.
But sometimes it can pay off to revisit your first 
solution that involves multiple temporary variables
we don't really need for the rest of the script
and clean it up a bit.
This might make it easier to come back to
your code in the future and be able to read
what is going on faster (because there are less lines
to read).

```{r, eval=FALSE}
gapminder <-
  dir("data/04/", full.names = TRUE) %>% 
  set_names(function(name) basename(name) %>% str_remove("\\.csv")) %>%
  map_dfr(read_csv, .id = "continent")
```

I want to take a moment to introduce yet another
shortcut we can take.
Notice, how in `set_names` I created a function to process
the names without giving it a name.
This is called an anonymous function.
Sometimes people also call it a lambda function,
because it originates from something called
lambda calculus.
Because the tilde symbol `~` is the
closest we get to $\lambda$ on an English keyboard,
this is used to create anonymous functions
on the fly.
The don't even have to worry about names
for our arguments, it automatically
creates a function where the argument
is named ".x" (and ".y" if you need multiple
arguments).
See the documentation of `map`,
especially the `.f` argument,
for more information.

```{r}
gapminder <-
  dir("data/04/", full.names = TRUE) %>% 
  set_names(~ basename(.x) %>% str_remove("\\.csv")) %>%
  map_dfr(read_csv, .id = "continent")
```

### The Imperative Programming Approach

There are other ways we could have gone about this.
A common construct in
programming languages is the so called **for-loop**.
For every element of a
vector, the body of the loop runs.
For our example, it would look like this:

```{r, eval=FALSE}
paths <- dir("data", full.names = TRUE)
results <- vector("list", length(paths))

for (i in 1:length(paths)) {
  data <- read_csv(paths[i])
  results[[i]] <- data
}

names(results) <- basename(paths) %>% str_remove("\\.csv")
gapminder <- bind_rows(results, .id = "continent")
```

The for-loop-version has a lot more code, especially *boilerplate*, code that is
just there to make the construct work and doesn't convey our intentions with the
code. Furthermore, the loop focuses the object that is iterated over (the file
paths), while the `map`-version focuses on what is happening (the function,
`read_csv`). But the loop still works. If you can't think of a way to solve a
problem with a `map` function, it is absolutely OK to use for-loops.

The first approach with the `map` function comes from Functional Programming,
whereas the second approach is considered Imperative Programming. In general, in
Functional Programming, we **tell the computer what we want**, while in
Imperative Programming, we **tell the computer what steps to do**. So keep in
mind:

> »Of course someone has to write for-loops. It doesn't have to be you.«<br>
> --- Jenny Bryan

And don't miss the amazing `purrr` cheatsheet:
[link](https://purrr.tidyverse.org/)

## "If you copy and paste the same code more than three times, write a function."

### Noticing a Pattern

Writing our own functions can be very helpful for making our
code more readable.
It allows us to separate certain steps of your analysis
from the rest, look at them in isolation to test and
validate them, and also allows us to give them reasonable names.
Let's look at a couple of examples and get you
writing functions!
Say we have this idea, where we filter
the gapminder dataset for one country, check
how linear the relationship between year and
life expectancy is and then create a plot.

```{r}
filterd_data <- gapminder %>% 
  filter(country == "Norway")

model <- lm(lifeExp ~ year, data = filterd_data)
```

These three function from the `broom` package
tell us more about our linear model created with `lm`.
It is part of the `tidymodels` framework.

```{r}
broom::tidy(model)
```

```{r}
broom::augment(model)
```

```{r}
broom::glance(model)
```

The $R^2$ value tells us, how well a straight
line fits to our data.
It can assume values between 0 and 1.

```{r, preview=TRUE}
model_glance <- broom::glance(model)

text <- substitute(R^2 == rsq,
                   list(rsq = round(model_glance$r.squared, 2)))

filterd_data %>% 
  ggplot(aes(year, lifeExp)) +
  geom_smooth(method = "lm") +
  geom_line() +
  geom_point() +
  theme_classic() +
  labs(
    x = "Year",
    y = "Life Expectancy",
    title = "Norway",
    subtitle = text
  )
```

Then we get curious and want to know how this graph
looks for another country.
So we copy and paste our code and replace the name
of the country.
And yet again, we want to see this plot for another
country, so we copy and paste our code,
change the country name and keep going.
This is when we remember, that there is an easier
way to deal with this repetition.
We look at our code again and identify
the things that stay the same and the
things that change during each copy and pasting.
The things that stay the same will
be the body of our function,
the things that change will be
variables in the body that we pass to the
function as arguments.
In our example, there is only one thing that changes
every time: the name of the country.
So we build the following function:

```{r}
plot_life_exp_for_country <- function(country_name) {
  filterd_data <- gapminder %>% 
    filter(country == country_name)
  
  model <- lm(lifeExp ~ year, data = filterd_data)
  model_glance <- broom::glance(model)
  text <- substitute(R^2 == rsq,
                     list(rsq = round(model_glance$r.squared, 2)))
  
  filterd_data %>% 
    ggplot(aes(year, lifeExp)) +
    geom_smooth(method = "lm") +
    geom_line() +
    geom_point() +
    theme_classic() +
    labs(
      x = "Year",
      y = "Life Expectancy",
      title = country_name,
      subtitle = text
    )
}
```

Then we test the function a bunch to make sure
it works for different cases:

```{r, fig.show='hold', out.width="50%"}
plot_life_exp_for_country("India")
plot_life_exp_for_country("Mali")
```

### Where to put your Functions

At this point you might wonder where to put this function.
A good starting point is to collect your
functions near the top of your document or script,
below where you load all packages.
Another approach, especially if you have functions
that you can use for multiple projects,
is to put them in a `.R` file and load
this file at the beginning of your document.
You can do so using the function `source("path/to/file.R")`.
It runs the R file in your current sessions,
so any functions and variables defined
in there are now available to you.

```{r, eval=FALSE}
source("R/my_funs.R")
```

Just like this.

```{r, eval=FALSE}
say_hello()
```

I like to store my regular R files
(as opposed to Rmd files) in
a folder of my project called `R`.
This makes it already look like an R package,
in case I decide later on that the functions
could be helpful for others as well
or I want to share them more easily
with colleagues.
You can read more about creating your
own R packages [here](http://r-pkgs.had.co.nz/)
[@wickhamPackagesOrganizeTest2015].

## Many Models

Let us use the techniques above to shine
light on how life expectancies changed over time.
This time, we will utilize the nested
data format to store our data
alongside models for the data.

```{r}
nested_gapminder <- gapminder %>%
  nest(-country, -continent) %>% 
  mutate(
    model = map(data, ~ lm(lifeExp ~ year, data = .x))
  )

nested_gapminder
```

Then, we extract information about the model
with some functions form the broom package.

```{r}
nested_gapminder %>% 
  mutate(
    glance = map(model, broom::glance)
  )
```

Then we go ahead and unnest the information
about the model.
We can now see, which countries
stray the farthest from a straight line
by arranging by the $R^2$ values.

```{r}
gapminder_modeled <- nested_gapminder %>% 
  mutate(
    glance = map(model, broom::glance)
  ) %>% 
  unnest(glance) %>% 
  arrange(r.squared)

gapminder_modeled
```

For a more detailed walk-trough, check
out the chapter on "many models" in
R for Data Science [here](https://r4ds.had.co.nz/many-models.html)
[@wickhamDataScienceImport2017].

Let's use this information to build a
visualization.

```{r}
non_linear_countries <- gapminder_modeled %>% 
  filter(r.squared < 0.2) %>% 
  unnest(data)

non_linear_countries
```

I also like to use this opportunity to
mention a couple of packages to take your
visualizations the the next level.
Today, these are `ggrepel` to produce
labels that dodge each other,
and the `fisualize` package, which
contains color scales of tropical fish.

```{r}
plt <- gapminder %>% 
  ggplot(aes(year, lifeExp, group = country)) +
  geom_line(alpha = 0.2) +
  geom_line(data = non_linear_countries,
            mapping = aes(color = country),
            size = 1.7) +
  fishualize::scale_color_fish_d() +
  labs(y = "Life Expectancy at Birth") +
  ggrepel::geom_text_repel(data = filter(non_linear_countries, year == max(year)),
            aes(label = country, color = country),
            hjust = 0, direction = "y") +
  expand_limits(x = 2015) +
  theme_minimal() +
  guides(color = "none")

plt
```

Sometimes it is also very handy (and probably impressive
to whoever receives your report) to turn a plot
into an interactive graphic.
`plotly` is a library that produces interactive plots,
but we don't even have to learn about its intricacies
because it comes with a function to convert a ggplot
to the interactive format that works quite well for
a range (but not all) of use-cases:

```{r}
plotly::ggplotly(plt)
```

The downward slope of our highlighted countries
starting in the 1990s is a result of the ravaging
AIDS pandemic.
The prominent dips in two of the curves, orange for
Rwanda and Cambodia in gray, are the direct
consequences of genocides.
These dire realities can in no way be summarized
in just a couple of colorful lines.
I am also in no way qualified to lecture
on these topics.
A good friend of mine, [Timothy Williams](http://timothywilliams.de/),
however is a researcher and teacher in the field of conflict and violence
with a focus on genocides.
He did field work in Cambodia and Rwanda
and his book "The Complexity of Evil. Perpetration and Genocide"
will be
[published here](https://www.rutgersuniversitypress.org/the-complexity-of-evil/9781978814295) on December 18 [@williamsComplexityEvil].

## Advanced `dplyr`

Last but not least, I want to mention a powerful advanced
function in dplyr to make your data transformations even
more efficient.
It is the `across` function, which we can use
inside of dplyr verbs such as `mutate` and
`summarise` to use one or multiple functions on
multiple columns.
Let's look at an example, again with the gapminder
dataset.
Now, this is sort of a silly example,
but let's say we want the continents
and countries in UPPERCASE.
We could do:

```{r}
gapminder %>% 
  mutate(
    continent = str_to_upper(continent),
    country = str_to_upper(country)
  )
```

And now everyone is really shouty.
But we had some repetition in
our code. Wouldn't it be cool,
to just say: "Apply the function `str_to_upper`
to these columns."?

```{r}
gapminder %>% 
  mutate(
    across(c(continent, country), str_to_upper)
  )
```

Not that is raw power!
We can be even more general and ask R to
apply the function to all columns that contain text:

```{r}
gapminder %>% 
  mutate(
    across(where(is.character), str_to_upper)
  )
```

It also works with `summarise`, for example
to create summaries across a range of columns.
And we can supply more than one function to calculate
using a named list:

```{r}
gapminder %>% 
  summarise(
    across(year:gdpPercap, list(mean = mean, total = sum))
  )
```
You are on you way to becoming a true data wizard!
After today, you should be familiar with:

- importing data into R
- the concept of tidy data
- the grammar of graphics
- the basic dplyr verbs for data wrangling
- a project-based workflow
- writing and using functions

We will then use these foundations to experience
statistical concepts ourselves in the next lectures.

## Exercises

## The whole Deal

I want to get you playing around with data,
so keep in mind that the solutions for this exercise 
are not set in stone.
There is often more than one viable way of graphing
the same dataset and we will use the
Office Hour to talk about the advantages
and disadvantages of approaches that you
came up with.

### Roman emperors

The first exercise uses a dataset about roman emperors
from the tidytuesday project
([link](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-08-13)).
You can import it with:

```{r, eval=FALSE}
emperors <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv")
```

Here are a couple of questions to answer. Decide
for yourselves if a particular question is best
answered using a visualization, a table
or a simple sentence.

- What was the most popular way to rise to power?
- I what are the most common causes of death among roman
  emperors, what (or who) killed them?
- Which dynasty was the most successful?
  - Firstly, how often did each dynasty reign?
  - Secondly, how long where the reigns?
  - Which dynasty would you rather be a part of,
    if your goal is to live the longest?

### Dairy Products in the US

Another dataset ([link](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-29#milk_products_facts))
concerns dairy product consumption per person in the US
across a number of years.
Load it with

```{r, eval=FALSE}
dairy <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv")
```

- All masses are given in lbs (pounds),
  can you convert them to kg?
- Which products lost their customer base over time,
  which ones won?

Above all, have some fun! If you make interesting
findings along the way, go ahead and produce plots
to highlight it.


## Resources

- [purrr documentation](https://purrr.tidyverse.org/)
- [stringr documentation](https://stringr.tidyverse.org/)
- [dplyr documentation](https://dplyr.tidyverse.org/)

