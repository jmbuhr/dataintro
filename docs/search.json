[{"path":"index.html","id":"hello-and-welcome","chapter":"Hello and welcome!","heading":"Hello and welcome!","text":"… latest iteration introductory R course, learn analyse data style.Warning: site still progress reflects state lectures previous year (WS20).\nStay tuned updates year’s lecture (WS21)!","code":""},{"path":"index.html","id":"prerequisites","chapter":"Hello and welcome!","heading":"0.1 Prerequisites","text":"find resources course Introduction Data Analysis R.\ninformation follows.","code":""},{"path":"index.html","id":"structure","chapter":"Hello and welcome!","heading":"0.2 Structure","text":"","code":""},{"path":"intro.html","id":"intro","chapter":"Lesson 1 Introduction","heading":"Lesson 1 Introduction","text":"… get started R RStudio,\nexplore basic structures operations R\nbuild first plot discovering Grammar\nGraphics.","code":""},{"path":"intro.html","id":"what-you-will-learn","chapter":"Lesson 1 Introduction","heading":"1.1 What You will Learn","text":"Throughout scientific career — potentially outside — encounter various forms data. Maybe experiment measured fluorescence molecular probe, simply count penguins local zoo. Everything data form another. raw numbers without context meaningless tables numbers boring look , often hide actual structure data.course learn handle different kinds data. learn create pretty insightful visualizations, compute different statistics data also statistical concepts mean. penguins p-values, got covered.course held English, concepts covered directly transfer research , working language English. said, feel free ask questions language understand, German also fine. Latin little rusty, thought.course, using programming language R. R language particularly well suited data analysis, initially designed statisticians interactive nature language, makes easier get started. don’t fret first encounter programming, take one step time.datasets chosen illustrate various concepts tools particularly centered around Biology. Rather, chose general datasets require less introduction enable us focus learning R statistics. talking penguins, racing games life expectancy instead intricate molecular measurements.","code":""},{"path":"intro.html","id":"first-things-first-installing-r","chapter":"Lesson 1 Introduction","heading":"1.2 First Things First: Installing R","text":"\ngetting ahead . First, need install R. can download installer operating system : https://cran.r-project.org/. Feel free post question get stuck. already gives ability execute R code use interactive R console, way comfortable use R inside called IDE (Integrated Development Environment). IDEs give neat things like autocompletion, window plots help panel. main IDE R called RStudio. using course can download : https://www.rstudio.com/products/rstudio/download/#download","code":""},{"path":"intro.html","id":"executing-r-code","chapter":"Lesson 1 Introduction","heading":"1.3 Executing R Code","text":"can now execute commands R console bottom left. example can calculate mathematical expression:generate numbers one 10:rarely type directly console. want results reproducible, write code script first, next person1 can see replicate analysis. see reproducibility quite near dear , pop twice. scientists, sure understand importance.create new script, click little button top left corner. script can type regular R code, won’t get executed straight away. send line code console executed, hit Ctrl+Enter. Go ahead, try :paste function combines text, just like + combines numbers. code can comments tell future self wrote piece code way . line starting number symbol # ignored R.","code":"\n1 + 1## [1] 2\n1:10##  [1]  1  2  3  4  5  6  7  8  9 10\npaste(\"Hello\", \"World!\")## [1] \"Hello World!\"\n# This line will be ignored\n43 - 1 # as will be the part after this #, but not before it## [1] 42"},{"path":"intro.html","id":"building-blocks-of-r","chapter":"Lesson 1 Introduction","heading":"1.4 Building Blocks of R","text":"Now time introduce fundamental datatypes R. going cover called atomic datatypes first introduce others appear.","code":""},{"path":"intro.html","id":"atomic-datatypes","chapter":"Lesson 1 Introduction","heading":"1.4.1 Atomic Datatypes","text":"First numbers (internally called numeric double), whole numbers (integer)well rarely used complex numbers (complex)Text data however used often (character, string). Everything enclosed quotation marks treated text. Double single quotation marks fine.Logical values can contain yes , rather TRUE FALSE programming terms (boolean, logical).special types mix type. Like NULL value NA Assigned.NA contagious. computation involving NA return NA (R way knowing answer):functions can remove NAs giving us answer:can ask datatype object function typeof:also concept called factors (factor) categorical data, talk later, get deeper vectors.","code":"\n12\n12.5\n1L # denoted by L\n1 + 3i # denoted by the small i for the imaginary part\n\"It was night again.\"\n'This is also text'\nTRUE\nFALSE\nNULL\nNA\nNA + 1## [1] NA\nmax(NA, 12, 1)## [1] NA\nmax(NA, 12, 1, na.rm = TRUE)## [1] 12\ntypeof(\"hello\")## [1] \"character\""},{"path":"intro.html","id":"variables","chapter":"Lesson 1 Introduction","heading":"1.4.2 Variables","text":"Often, want store result computation reuse, give sensible name make code readable. variables . can assign value variable using assignment operator <- (RStudio, shortcut : Alt+Minus):Executing code give output, use name variable, can see content.can operations variables:NOTE careful order execution! R enables work interactively execute code write script order Ctrl+Enter, execute (=“source”) whole script, executed top bottom.Furthermore, code executed automatically, change dependency expression later . second assignment x doesn’t change y.Variable names can contain letters (capitalization matters), numbers (first character) underscores _.2A depiction various naming styles .3","code":"\nmy_number <- 42\nmy_number## [1] 42\nx <- 41\ny <- 1\nx + y## [1] 42\nx <- 1\ny <- x + 1\nx <- 1000\ny## [1] 2\n# snake_case\nmain_character_name <- \"Kvothe\"\n\n# or camelCase\nbookTitle <- \"The Name of the Wind\"\n\n# you can have numbers in the name\nx1 <- 12"},{"path":"intro.html","id":"functions","chapter":"Lesson 1 Introduction","heading":"1.4.3 Functions","text":"R, everything exists object, everything something function.Functions main workhorse data analysis. example, mathematical functions, like sin, cos etc.Functions take arguments (sometimes called parameters) sometimes also return things. sin function takes just one argument x returns sine. returned value us. can use directly another computation store variable. don’t anything return value, R simply prints console.Note, = inside function parenthesis gives x = 0 function separate x defined outside function. example:learn function R, execute ? function name press F1 mouse function. actually one important things learn today, help pages can … well… incredibly helpful.can pass arguments name order appearance. following two expressions equivalent.","code":"\nsin(x = 0)## [1] 0\nx <- 10\ncos(x = 0)## [1] 1\n# x outside of the function is still 10\nx## [1] 10\n?sin\nsin(x = 12)\nsin(12)"},{"path":"intro.html","id":"vectors","chapter":"Lesson 1 Introduction","heading":"1.4.4 Vectors","text":"vector ordered collection things datatype, datatype something like numbers (numeric), text (character also called string) whole numbers (integer).basic datatypes R vectors, means can contain one entry. can create vector combining things data type function c combine.atomic datatype mentioned can vector, atomic vectors can store data type. example, can character vectoror vector logical valuesBut vector , say text numbers. try combine data different type, R force data permissive type. Numbers can easily converted text, text can converted numbers, makes everything text example:Note quotation marks around numbers, marking text. try use numbers, get error message:cases encounter error messages. Programming languages unlike human languages. computer always understand, want , unless use exactly right grammar vocabulary. error messages R’s way telling us, didn’t understand, asked something impossible. Even experienced programmers fond advice:\nFigure 1.1: Maybe important programming advice.\nsolve error message, need explicitly tell R convert text number:","code":"\nx <- c(1, 2, 3, 4, 5, 6)\nx## [1] 1 2 3 4 5 6\nc(\"This\", \"is\", \"a\", \"character\", \"vector\")## [1] \"This\"      \"is\"        \"a\"         \"character\" \"vector\"\nc(TRUE, FALSE, TRUE, TRUE)## [1]  TRUE FALSE  TRUE  TRUE\nc(\"Some text\", 42, 12)## [1] \"Some text\" \"42\"        \"12\"\n\"12\" + 1## Error in \"12\" + 1: non-numeric argument to binary operator\nas.numeric(\"12\") + 1## [1] 13"},{"path":"intro.html","id":"subsetting","chapter":"Lesson 1 Introduction","heading":"1.4.4.1 Subsetting","text":"can look , change, subsets vectors using square brackets [] like :assign names elements, can also reference name.Pass vector indices (names) square brackets get (set) multiple elements:Using logical vector yields elements vector TRUE:","code":"\nmy_elements <- c(\"first\", \"second\", \"third\")\nmy_elements[2]## [1] \"second\"\nmy_elements[3] <- \"new element\"\nmy_elements## [1] \"first\"       \"second\"      \"new element\"\nnames(my_elements) <- c(\"e1\", \"e2\", \"e3\")\nmy_elements##            e1            e2            e3 \n##       \"first\"      \"second\" \"new element\"\nmy_elements[\"e3\"]##            e3 \n## \"new element\"\nmy_elements[c(1, 3)]##            e1            e3 \n##       \"first\" \"new element\"\nmy_elements[c(TRUE, TRUE, FALSE)]##       e1       e2 \n##  \"first\" \"second\""},{"path":"intro.html","id":"vectorization","chapter":"Lesson 1 Introduction","heading":"1.4.5 Vectorization","text":"basic mathematical operations R lot functions vectorized. means, operate every element vector. , every element multiplied 2 result printed console.original vector x changed ., assigning result back x, thus overwriting previous content. right hand side (RHS) executed first:Now x changed:handy way creating vectors numbers : operator specify range values:using seq function additional (optional) parameters:Now : Look documentation/help page seq find create vector even numbers 2 100.","code":"\nx * 2## [1]  2  4  6  8 10 12\nx## [1] 1 2 3 4 5 6\nx <- x * 2\nx## [1]  2  4  6  8 10 12\n1:5## [1] 1 2 3 4 5\nseq(from = 1, to = 10)##  [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"intro.html","id":"functions-and-packages-making-our-lives-easier","chapter":"Lesson 1 Introduction","heading":"1.5 Functions and Packages – Making our lives easier","text":"just learned functions sin, seq max. wait, ! sense functions R (kind language two verbs?!), also powerful way:can define functions!syntax (\\(\\leftarrow\\) grammar programming languages) follows.function ends reaches return keyword. also ends reaches end function body implicitly returns last expression. written bit shorter fact often see people omitting explicit return end:can call freshly defined function:Got error like Error add(23, 19) : find function \"add\"? Check fact execute code defines function (.e. put cursor line function keyword hit Ctrl+Enter.).Now : Define function takes one argument, vector numbers, devides element length vector (hint: length function get length) returns resulting scaled vector.one using R. welcoming helpful community . people also write bunch functions put together called package. people even went step . tidyverse collection packages play well together also iron quirkier ways R works.4 provide consistent interface enable us learn less special cases. R function install.packages(\"<package_name_here>\") installs packages CRAN curated set R packages.","code":"\nname_for_the_function <- function(parameter1, parameter2, ...) { # etc.\n  # body of the function\n  # things happen\n  result <- parameter1 + parameter2\n  # Something the function should return to the caller\n  return(result)\n}\nadd <- function(x, y) {\n  x + y\n}\nadd(23, 19)## [1] 42"},{"path":"intro.html","id":"the-tidyverse","chapter":"Lesson 1 Introduction","heading":"1.5.1 The Tidyverse","text":"Go ahead install tidyverse packages withThis one exception effort everything script just console. don’t want R trying install package every time run script, needs happen . can either turn comment, delete script, type console. can also use RStudio’s built-panel package installation.make functions package available R session, run library function name package.convention , keep library-calls top script, ,others, can see straight away, packages needed. Don’t worry messages pop . just tidyverse telling us two ’s functions (lag filter) functions names another package (case base-R) loaded tidyverse second, R now use tidyverse functions. “masking” means.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)"},{"path":"intro.html","id":"literate-programming-rmarkdown","chapter":"Lesson 1 Introduction","heading":"1.6 Literate Programming: Rmarkdown","text":"5There another package like install. called Rmarkdown.Rmarkdown enables us, combine text code produce range output formats like pdf, html, word documents, presentations etc. fact, whole website, including slides, created Rmarkdown. Sounds exciting? Let’s dive !Open new Rmarkdown document file extension .Rmd New File menu top left corner RStudio: File → New File → R Markdown choose html output format. particularly like html, don’t worry page breaks easily works screens different sizes, like phone.Rmarkdown document consists three things:Metadata:\nInformation document author date format called YAML. YAML header starts ends three minus signs ---.Text:\nRegular text interpreted markdown, meaning supports things like creating headings prefixing line #, text bold output surrounding **.Code chunks:\nStarting ```{r} ending ``` (backticks). interpreted R code. write code like .R script file. can insert new chunks button top right editor window use shortcut Ctrl+Alt+.Use document thoughts alongside code data analysis. Future (reviewer number 2) happy! run code inside chunks, use,little play button chunk, tried true Ctrl+Enter run one line, Ctrl+Shift+Enter run whole chunk. chunks can large small want, try maintain sensible structure.","code":"\ninstall.packages(\"rmarkdown\")"},{"path":"intro.html","id":"our-first-dataset-the-palmer-penguins","chapter":"Lesson 1 Introduction","heading":"1.7 Our First Dataset: The Palmer Penguins","text":"6So let’s explore first dataset together fresh Rmarkdown document. setup chunk special. gets executed automatically chunk document run. makes good place load packages. dataset working today actually comes package, need install well (Yes, lot installing today, ):populate setup chunk withThis gives us penguins dataset:7Let’s talk shape penguins object. str function reveals structure object us.","code":"\ninstall.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins\nstr(penguins)## tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n##  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n##  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n##  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n##  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n##  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n##  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."},{"path":"intro.html","id":"lists-and-dataframes","chapter":"Lesson 1 Introduction","heading":"1.7.1 Lists and dataframes","text":"Previously, atomic vectors, elements type, like vector numbers, individual elements contain things (hence name atomic): c(1, 2, 3). next general thing list, can create function list(...). Lists can contain arbitrary elements, even lists:print output suggests something else: vectors, subset using [], need double square brackets [[]].Using single brackets just give us subset list, actual element list (yes, can confusing):penguins variable contains called data.frame. reason talked lists just now dataframes built top lists, elements columns. dataframes form rectangular data format like spreadsheet know excel, constraint elements list need length. can demonstrate creating dataframe list.Notice another thing: gave names elements list. nice two reasons. Firstly, print output already suggests, can now use dollar syntax $ refer individual elements name instead position RStudio’s autocomplete helps us .Secondly, names become column names turn dataframe:one last difference penguins. also tibble, built top dataframes makes object look nicer print console. Compare following executing console:dataset contains data 344 penguins. 3 different species, collected 3 islands Palmer Archipelago, Antarctica8.","code":"\nmy_list <- list(1, \"hello\", c(1, 2, 3), list(42, \"text\"))\nmy_list## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] \"hello\"\n## \n## [[3]]\n## [1] 1 2 3\n## \n## [[4]]\n## [[4]][[1]]\n## [1] 42\n## \n## [[4]][[2]]\n## [1] \"text\"\nmy_list[[3]]## [1] 1 2 3\nmy_list[3]## [[1]]\n## [1] 1 2 3\nnew_list <- list(x = 1:3, y = c(10, 42, 3), third = c(\"hello\", \"from\", \"R\"))\nnew_list## $x\n## [1] 1 2 3\n## \n## $y\n## [1] 10 42  3\n## \n## $third\n## [1] \"hello\" \"from\"  \"R\"\nnew_list$x## [1] 1 2 3\nmy_first_df <- as.data.frame(new_list)\nmy_first_df##   x  y third\n## 1 1 10 hello\n## 2 2 42  from\n## 3 3  3     R\npenguins\nas.data.frame(penguins)"},{"path":"intro.html","id":"translating-data-into-visualizations","chapter":"Lesson 1 Introduction","heading":"1.8 Translating Data into Visualizations","text":"probably took course want build cool visualizations data. order , let us talk can describe visualizations. Just like language grammar, smart people came grammar graphics,9 slightly modified turned R package can talk also create visualizations using grammar.10The package called ggplot2 already loaded included tidyverse. looking code, can describe need order create graphic.can build plot step step. data foundation plot, just gives us empty plotting canvas. assigning individual steps going variable, can sequentially add elements, can one step shown ., add aesthetic mapping plot. creates relation features dataset (like flipper length penguin) visual property, like position x-axis, color shape.Still, plot empty, coordinate system certain scale. geometric objects represent aesthetics. Elements plot added using + operator geometric elements ggplot knows start geom_. Let’s add points:Look help page geom_point find aesthetics understands. exact way features mapped aesthetics regulated scales starting scale_ name aesthetic:can add change labels (like x-axis-label) adding labs function.overall look plot regulated themes like premade theme_ functions finely regulated theme() function, uses element functions create look individual elements. Autocomplete helps us lot (Ctrl+Space).","code":"\nggplot(penguins, aes(flipper_length_mm, bill_length_mm,\n                     color = species,\n                     shape = sex)) +\n  geom_point(size = 2.5) +\n  labs(x = \"Flipper length [mm]\",\n       y = \"Bill length [mm]\",\n       title = \"Penguins!\",\n       subtitle = \"The 3 penguin species can differentiated by their flipper and bill lengths\") +\n  theme_minimal() +\n  scale_color_brewer(type = \"qual\")\nplt <- ggplot(penguins)\nplt\nplt <- ggplot(penguins,\n              aes(x = flipper_length_mm,\n                  y = bill_length_mm,\n                  color = species,\n                  shape = sex))\nplt\nplt <- plt +\n  geom_point()\nplt\nplt <- plt +\n  scale_color_brewer(type = \"qual\")\nplt\nplt <- plt +\n    labs(x = \"Flipper length [mm]\",\n         y = \"Bill length [mm]\",\n         title = \"Penguins!\",\n         subtitle = \"The 3 penguin species can differentiated by their flipper and bill lengths\")\nplt <- plt + \n  theme_minimal() +\n  theme(legend.text = element_text(face = \"bold\"))\nplt"},{"path":"intro.html","id":"the-community-there-to-catch-you.","chapter":"Lesson 1 Introduction","heading":"1.9 The Community: There to catch You.","text":"Coding can incredibly rewarding, also incredibly frustrating.Luckily, R community !11In video give brief overview resources linked . Come back anytime reference.","code":""},{"path":"intro.html","id":"exercises","chapter":"Lesson 1 Introduction","heading":"1.10 Exercises","text":"course graded, need way confirming indeed take part course. order get confirmation, send solutions minimum 5 8 exercises Friday following lecture upload Monday. week like create fresh Rmarkdown document solutions code well questions arose lecture. help lot improving course.done solving exercises, hit knit button (top editor panel) send resulting html document via discord email (confirm looks way expected beforehand).today’s tasks:Write section text previous experience data analysis /programming (optional, can use information customize course).Write section text previous experience data analysis /programming (optional, can use information customize course).Write code loads tidyverse palmer penguins data set.Write code loads tidyverse palmer penguins data set.Produce scatterplot (meaning plot points) bill length vs. bill depth, colorcoded species.\nImaginary bonus points manage use colors image (hint: look help page scale_color_manual() find ). Even bonus points also look theme() function ’s arguments, theme_<...>() functions make plot prettier.\nProduce scatterplot (meaning plot points) bill length vs. bill depth, colorcoded species.Imaginary bonus points manage use colors image (hint: look help page scale_color_manual() find ). Even bonus points also look theme() function ’s arguments, theme_<...>() functions make plot prettier.Create vector odd numbers 1 99 store variable.\nCreate second variable contains squares first.\nStore variables named list turn list tibble (enhanced version data.frame\nDiscover shortcut three steps using function tibble. Specifically, look third bullet point description ?tibble::tibble (two colons :: specify package function coming . need tibble(...) code tibble package loaded automatically tidyverse. , specify directly send correct help page).\nCreate scatterplot two variables stored tibble using ggplot.\ngeom_ function need add plot add line connects points?\nCreate vector odd numbers 1 99 store variable.Create second variable contains squares first.Create second variable contains squares first.Store variables named list turn list tibble (enhanced version data.frameStore variables named list turn list tibble (enhanced version data.frameDiscover shortcut three steps using function tibble. Specifically, look third bullet point description ?tibble::tibble (two colons :: specify package function coming . need tibble(...) code tibble package loaded automatically tidyverse. , specify directly send correct help page).Discover shortcut three steps using function tibble. Specifically, look third bullet point description ?tibble::tibble (two colons :: specify package function coming . need tibble(...) code tibble package loaded automatically tidyverse. , specify directly send correct help page).Create scatterplot two variables stored tibble using ggplot.Create scatterplot two variables stored tibble using ggplot.geom_ function need add plot add line connects points?geom_ function need add plot add line connects points?Check metadata (YAML) Rmarkdown document make sure contains name author: .\ncouple YAML options can try feel adventurous.\nCheck metadata (YAML) Rmarkdown document make sure contains name author: .couple YAML options can try feel adventurous.","code":""},{"path":"intro.html","id":"solutions","chapter":"Lesson 1 Introduction","heading":"1.11 Solutions","text":"Office Hour solutions questions Friday, Nov 6, 2020 10:00. Find link discord.","code":""},{"path":"intro.html","id":"learn-more","chapter":"Lesson 1 Introduction","heading":"1.12 Learn more:","text":"() resources can also found dedicated Resources page.","code":""},{"path":"intro.html","id":"tidyverse","chapter":"Lesson 1 Introduction","heading":"1.13 Tidyverse","text":"R Data Science12R4DS online CommunityRStudio Cheat Sheets!Modern Dive13RStudio Education","code":""},{"path":"intro.html","id":"rmarkdown","chapter":"Lesson 1 Introduction","heading":"1.14 Rmarkdown","text":"https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdfhttps://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdfhttps://bookdown.org/yihui/rmarkdown-cookbook/https://bookdown.org/yihui/rmarkdown/https://pandoc.org/MANUAL.html#pandocs-markdownhttps://reproducible-analysis-workshop.readthedocs.io/en/latest/7.RMarkdown-knitr.htmlhttps://rmarkdown.rstudio.com/index.html","code":""},{"path":"intro.html","id":"r-in-general","chapter":"Lesson 1 Introduction","heading":"1.15 R in general","text":"Advanced R14Hands Programming R15R Packages16Data Visualization: Practical Introduction17Graph Cookbook18","code":""},{"path":"intro.html","id":"statistics","chapter":"Lesson 1 Introduction","heading":"1.16 Statistics","text":"Intuitive Biostatistics19Statistics Done Wrong20","code":""},{"path":"intro.html","id":"talks-podcasts-blogs-videos","chapter":"Lesson 1 Introduction","heading":"1.17 Talks, Podcasts, Blogs, Videos","text":"David Robinson\nYouTube\nwebsite\nDavid RobinsonYouTubewebsiteJulia Silge\nYouTube\nwebsite\nJulia SilgeYouTubeYouTubewebsitewebsiteAlison Hill\nwebsite\nAlison Hillwebsite","code":""},{"path":"intro.html","id":"misc","chapter":"Lesson 1 Introduction","heading":"1.18 Misc","text":"Cute insightful illustrations21Happy Git R","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"Lesson 2 Data Wrangling","heading":"Lesson 2 Data Wrangling","text":"… explore typical data analysis workflow\ntidyverse, wrangle different kinds data \nlearn factors.Note:\ntry vocal code plain English type learning “translations” symbols keywords can help , . , programming can feel lot like conversation digital assistant helpful friend. boundary human languages computer languages blurry might think.get deeper R,\nlet’s talk little bit Home working R:\nRStudio.","code":""},{"path":"data-wrangling.html","id":"making-ourselves-at-home-in-rstudio","chapter":"Lesson 2 Data Wrangling","heading":"2.1 Making Ourselves at Home in RStudio","text":"","code":""},{"path":"data-wrangling.html","id":"important-settings","chapter":"Lesson 2 Data Wrangling","heading":"2.1.1 Important Settings","text":"highlighted 3 settings consider important change default values. first two might seem odd first glance.workspace RStudio save .RData contains objects created session, , can see Environment pane (default top right panel, bottom right setup). want load objects created last session current session automatically? reason reproducibility. want make sure everything analysis needs script. creates variables plots raw data sole source truth. Given raw data script, everyone able reproduce results. third setting (text encoding) also concerned collaboration. makes sure text files special characters (like German umlauts) look different operating systems like Windows, iOS Linux.","code":""},{"path":"data-wrangling.html","id":"a-project-based-workflow","chapter":"Lesson 2 Data Wrangling","heading":"2.1.2 A Project-based Workflow","text":"Last week simply went ahead created script file Rmarkdown file folder computer. R known, script ? know, look, tell read file save plot? main folder R starts called working directory. find , current working directory , execute function getwd() get working directory:look slightly different depending operating system. next function want know, never use:“use ?” might ask. Let’s assume use function set working directory seen :Now, give file test analysis, chances low work. likely called Jannik even likely don’t folder structure .want instead absolute file paths, relative file paths start folder scripts . RStudio Projects . basically just folder special file ends .Rproj, soon R finds file folder, working directory automatically folder. share folder, still just work.Go ahead use blue R button top right corner create New Project. recommend either one project whole course one project per week. Projects also convenient save files open forth.one thing didn’t tell Rmarkdown documents, yet. working directory always folder , even subdirectory project. way also means don’t necessarily need project work Rmarkdown, one anyway makes easier keep track files consistent structure.","code":"\ngetwd()## [1] \"/home/jannik/Documents/projects/teaching/dataintro\"\nsetwd(\"some/file/path\")\nsetwd(\"/home/jannik/Documents/projects/teaching/dataIntro20/_lectures/lecture2\")"},{"path":"data-wrangling.html","id":"working-in-style","chapter":"Lesson 2 Data Wrangling","heading":"2.1.3 Working in Style","text":"Now cosy project today, let’s also make sure working style feeling right home RStudio. blogpost provides excellent gifs different settings RStudio even Rmarkdown troubleshooting. also fond rsthemes package additional appearances.","code":""},{"path":"data-wrangling.html","id":"a-data-analysis-workflow","chapter":"Lesson 2 Data Wrangling","heading":"2.2 A Data Analysis Workflow","text":"getting close importing first dataset file R. Generally, first thing needs happen data analysis cover today. data provided already pretty tidy start build visualizations. communicate-part also covered, working Rmarkdown , designed communicate findings. Later, also look less tidy data, defined “tidy data” .\nFigure 2.1: Figure Wickham Grolemund.22\n","code":""},{"path":"data-wrangling.html","id":"reading-data-with-readr","chapter":"Lesson 2 Data Wrangling","heading":"2.3 Reading Data with readr","text":"package responsible loading data tidyverse called readr, start loading whole tidyverse.Note, general, also load just readr package library(readr), need rest tidyverse later anyways. also option load package rather use one function package prefixing function package name two colons (::) Like : readr::read_csv(\"...\").Without ado, let’s download data today. fact, multiple ways go . download whole course folder GitHub following link top right corner website using download button:find data folder ./_lectures/lecture2/data/.can navigate file GitHub. Follow link: files, click one files click raw:can copy link displayed browser address field download file straight R:folder download data (./data) must created advance. example relative path (absolute path start / drive letter like C:).Now can finally load data store variable. working file paths, RStudio’s autocompletion especially helpful. can trigger Tab Ctrl+Space.readr also tell datatypes guessed columns. Let’s inspect dataset:gapminder dataset23 excerpt gapminder project contains life expectancy birth 142 countries 5 year intervals 1952 2007. also contains population Gross Domestic Product (GDP) per Inhabitant. built visualization later .read_csv can even read data url straight away, without need us download file , usually want copy data locally. curious run :, went smoothly. always case. now look common hurdles importing data","code":"\nlibrary(tidyverse)\ndownload.file(\"https://raw.githubusercontent.com/jmbuhr/dataIntro20/master/_lectures/lecture2/data/gapminder.csv\",\n              \"data/gapminder.csv\")\ngapminder <- read_csv(\"data/02/gapminder.csv\")\ngapminder## # A tibble: 1,704 x 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,694 more rows\nread_csv(\"https://raw.githubusercontent.com/jmbuhr/dataIntro20/master/_lectures/lecture2/data/gapminder.csv\")"},{"path":"data-wrangling.html","id":"common-hurdles-when-importing-data","chapter":"Lesson 2 Data Wrangling","heading":"2.3.1 Common Hurdles when Importing Data","text":"function just used called read_csv, reads file format consists comma separated values. Look raw file text editor (word) like notepad RStudio see . file extension .csv can sometimes lying…German, comma used separate decimal numbers (vs. dot English), lot Software output different type csv-file configured German. still call csv, actually separated semicolons! special function :looking autocompletion options pop typing function name, might noticed similar function read.csv read.csv2. functions come R, without packages like tidyverse. can course use well, tidyverse functions provide consistent experience less surprising quirks. teaching tidyverse first, allows learn less edge cases.look yet another file data/gapminder_tsv.txt, notice file extension doesn’t tell us much format, text (opposed binary format computers can read). look file:notice values separated \", special sequence stands tab character. read_tsv function job.separator (also called delimiter) even obscure, can use general function read_delim. Say co-worker misunderstood us thought tsv stands “Tilde separated values,” can still read file.ways raw data can messy hard read depending machine can’t show . One common thing encounter though measurement machines writing additional information first couple lines actual data (like time measurement). example:first 2 lines part data. Reading file normally csv produce something weird: first line contain commata, assume file contains one column also report bunch parsing failures. Parsing act turning data represented raw text useful format, like table numbers.can fix telling R skip first 2 lines entirely:using n_max argument functions save space lecture script.video forgot mention also included excel file practice. can read using function readxl package. package automatically installed tidyverse, loaded along packages via library(tidyverse). can either load library(readxl) refer single function package without loading whole thing using double colons (::):Now, learned ways raw data can structured, let us go back original data read saved variable gapminder.","code":"\nread_csv2(\"data/gapminder_csv2.csv\")\nread_lines(\"data/02/gapminder_tsv.txt\", n_max = 3)## [1] \"country\\tcontinent\\tyear\\tlifeExp\\tpop\\tgdpPercap\"    \n## [2] \"Afghanistan\\tAsia\\t1952\\t28.801\\t8425333\\t779.4453145\"\n## [3] \"Afghanistan\\tAsia\\t1957\\t30.332\\t9240934\\t820.8530296\"\nread_lines(\"data/02/obscure_file.tsv\", n_max = 3)## [1] \"country~continent~year~lifeExp~pop~gdpPercap\"    \n## [2] \"Afghanistan~Asia~1952~28.801~8425333~779.4453145\"\n## [3] \"Afghanistan~Asia~1957~30.332~9240934~820.8530296\"\nread_delim(\"data/02/obscure_file.tsv\", delim = \"~\")\nread_lines(\"data/02/gapminder_messier.csv\", n_max = 4)## [1] \"# Some comment about the data\"                   \n## [2] \"And maybe a personal note\"                       \n## [3] \"country,continent,year,lifeExp,pop,gdpPercap\"    \n## [4] \"Afghanistan,Asia,1952,28.801,8425333,779.4453145\"\nread_csv(\"data/02/gapminder_messier.csv\", n_max = 3)## # A tibble: 3 x 1\n##   `# Some comment about the data`\n##   <chr>                          \n## 1 And maybe a personal note      \n## 2 country                        \n## 3 Afghanistan\nread_csv(\"data/02/gapminder_messier.csv\", skip = 2, n_max = 3)## # A tibble: 3 x 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\nreadxl::read_xlsx(\"data/02/gapminder.xlsx\")"},{"path":"data-wrangling.html","id":"wrangling-data-with-dplyr","chapter":"Lesson 2 Data Wrangling","heading":"2.4 Wrangling Data with dplyr","text":"number ways can manipulate data. course mean manipulate ’s original sense, malicious one. sometimes referred data wrangling within tidyverse, job dplyr package (short data plyer, tool see logo).dplyr provides functions various operations data. Theses functions sometimes also called dplyr verbs. take tibble data.frame input (plus additional parameters) always return tibble.\nFigure 2.2: 24\n","code":""},{"path":"data-wrangling.html","id":"select","chapter":"Lesson 2 Data Wrangling","heading":"2.4.1 select","text":"first verb introduce used select columns. hence, called select. first argument always data, followed arbitrary number column names. can recognize functions take arbitrary number additional arguments ... autocompletion help page.might confusing don’t need quotation marks around column names like select element vector name :concept known quasiquotation data masking. quite unique R, allows functions known content data passed use environment computations search variable names. variable country doesn’t exist global environment, exist column gapminder tibble. dplyr functions always look data first search names.help page select tells us different ways can select columns. couple examples without output, rum R session confirm think . (look help pages , quite well written).","code":"\nselect(gapminder, country, year, pop)## # A tibble: 1,704 x 3\n##    country      year      pop\n##    <chr>       <dbl>    <dbl>\n##  1 Afghanistan  1952  8425333\n##  2 Afghanistan  1957  9240934\n##  3 Afghanistan  1962 10267083\n##  4 Afghanistan  1967 11537966\n##  5 Afghanistan  1972 13079460\n##  6 Afghanistan  1977 14880372\n##  7 Afghanistan  1982 12881816\n##  8 Afghanistan  1987 13867957\n##  9 Afghanistan  1992 16317921\n## 10 Afghanistan  1997 22227415\n## # … with 1,694 more rows\nc(first = 1, second = 2)[\"first\"]## first \n##     1\nselect(gapminder, where(is.numeric))\nselect(gapminder, country:lifeExp)\nselect(gapminder, starts_with(\"c\"))\nselect(gapminder, c(1, 3, 4))"},{"path":"data-wrangling.html","id":"filter","chapter":"Lesson 2 Data Wrangling","heading":"2.4.2 filter","text":"\nFigure 2.3: 25\nselecting columns natural ask select rows. achieved function filter. example, can filter years smaller 2000:rows country “New Zealand”:","code":"\nfilter(gapminder, year < 2000)## # A tibble: 1,420 x 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,410 more rows\nfilter(gapminder, country == \"New Zealand\")## # A tibble: 12 x 6\n##    country     continent  year lifeExp     pop gdpPercap\n##    <chr>       <chr>     <dbl>   <dbl>   <dbl>     <dbl>\n##  1 New Zealand Oceania    1952    69.4 1994794    10557.\n##  2 New Zealand Oceania    1957    70.3 2229407    12247.\n##  3 New Zealand Oceania    1962    71.2 2488550    13176.\n##  4 New Zealand Oceania    1967    71.5 2728150    14464.\n##  5 New Zealand Oceania    1972    71.9 2929100    16046.\n##  6 New Zealand Oceania    1977    72.2 3164900    16234.\n##  7 New Zealand Oceania    1982    73.8 3210650    17632.\n##  8 New Zealand Oceania    1987    74.3 3317166    19007.\n##  9 New Zealand Oceania    1992    76.3 3437674    18363.\n## 10 New Zealand Oceania    1997    77.6 3676187    21050.\n## 11 New Zealand Oceania    2002    79.1 3908037    23190.\n## 12 New Zealand Oceania    2007    80.2 4115771    25185."},{"path":"data-wrangling.html","id":"mutate","chapter":"Lesson 2 Data Wrangling","heading":"2.4.3 mutate","text":"back manipulating columns, time creating new ones changing old ones. dplyr verb called muate. example, might want calculate total GDP GDP per Capita population:Notice, none functions changed original variable gapminder. take input return output, makes easier reason code later chain pieces code together. change ? Use Force! … ahem, mean, assignment operator (<-)., power dplyr shines. knows pop gdpPercap columns tibble gdp refers new name freshly created column.","code":"\nmutate(gapminder, gdp = pop * gdpPercap)## # A tibble: 1,704 x 7\n##    country     continent  year lifeExp      pop gdpPercap          gdp\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>        <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n## # … with 1,694 more rows\ngapminder <- mutate(gapminder, gdp = pop * gdpPercap)"},{"path":"data-wrangling.html","id":"interlude-begind-the-magic-handling-data-with-base-r","chapter":"Lesson 2 Data Wrangling","heading":"2.4.4 Interlude: Begind the magic, handling data with base-R","text":"section meant show happens behind scenes. strictly necessary understand details order work effectively tidyverse, helps especially things don’t go planned.let’s look handling data base-R. Last week briefly covered subsetting vectors indices, names, logical vector:subsetting logical vector can used filter vector:data 2 dimensions, rows columns, subsetting works similarly. Let’s use function tibble create tibble vectors:Note:\nR whitespace sensitive (like python). means indentation doesn’t change meaning code. can use format code look pretty, started new line opening bracket tibble(.Subsetting tibble like vector selects columns:pass two arguments square brackets, separated commata, can filter rows select columns , get first row second third columns:follow logic , can also filter data. comma without argument afterwards selects columns.first glance, tidyverse way little bit shorter. apart fact don’t repeat name data object my_data (filter knows look x, whereas [] doesn’t), another advantage.","code":"\nx <- c(42, 1, 13, 29)\nnames(x) <- c(\"first\", \"second\", \"third\", \"fourth\")\nx##  first second  third fourth \n##     42      1     13     29\nx[c(1,3)]## first third \n##    42    13\nx[c(\"first\", \"second\")]##  first second \n##     42      1\nx[c(TRUE, FALSE, FALSE, TRUE)]##  first fourth \n##     42     29\nselect_this <- x < 20\nselect_this##  first second  third fourth \n##  FALSE   TRUE   TRUE  FALSE\nx[select_this]## second  third \n##      1     13\nmy_data <- tibble(\n  x = c(42, 1, 13, 29),\n  y = c(1, 2, 3, 4),\n  z = c(\"z1\", \"z2\", \"z3\", \"z4\")\n)\nmy_data## # A tibble: 4 x 3\n##       x     y z    \n##   <dbl> <dbl> <chr>\n## 1    42     1 z1   \n## 2     1     2 z2   \n## 3    13     3 z3   \n## 4    29     4 z4\nmy_data[c(1, 3)]## # A tibble: 4 x 2\n##       x z    \n##   <dbl> <chr>\n## 1    42 z1   \n## 2     1 z2   \n## 3    13 z3   \n## 4    29 z4\nmy_data[1, c(2, 3)]## # A tibble: 1 x 2\n##       y z    \n##   <dbl> <chr>\n## 1     1 z1\nmy_data[my_data$x < 20, ]## # A tibble: 2 x 3\n##       x     y z    \n##   <dbl> <dbl> <chr>\n## 1     1     2 z2   \n## 2    13     3 z3\nfilter(my_data, x < 20)## # A tibble: 2 x 3\n##       x     y z    \n##   <dbl> <dbl> <chr>\n## 1     1     2 z2   \n## 2    13     3 z3"},{"path":"data-wrangling.html","id":"the-pipe","chapter":"Lesson 2 Data Wrangling","heading":"2.4.5 The pipe %>%","text":"tidyverse functions easier compose (.e. chain together). facilitate , introduce another operator, bit like + numbers + add ggplot components, specially functions. pipe, can either type insert RStudio Ctrl+Shift+M, takes ’s left side passes first argument function right side:main tidyverse functions take data first argument, can chain together fluently. Additionally, enables autocompletion column names inside function gets data. back gapminder example:also reads much nicer head, makes reasoning code easier. Without telling code , can understand , reads like English. can often pronounce pipe “” head, -loud, ’m judging.Note:\nbase-R tidyverse way mutually exclusive. Sometimes can mix match.","code":"\nmy_data %>% select(x, z)## # A tibble: 4 x 2\n##       x z    \n##   <dbl> <chr>\n## 1    42 z1   \n## 2     1 z2   \n## 3    13 z3   \n## 4    29 z4\ngapminder %>% \n  filter(year > 2000) %>% \n  mutate(gdp = pop * gdpPercap) %>% \n  select(country, year, gdp)## # A tibble: 284 x 3\n##    country      year           gdp\n##    <chr>       <dbl>         <dbl>\n##  1 Afghanistan  2002  18363410424.\n##  2 Afghanistan  2007  31079291949.\n##  3 Albania      2002  16153932130.\n##  4 Albania      2007  21376411360.\n##  5 Algeria      2002 165447670333.\n##  6 Algeria      2007 207444851958.\n##  7 Angola       2002  30134833901.\n##  8 Angola       2007  59583895818.\n##  9 Argentina    2002 337223430800.\n## 10 Argentina    2007 515033625357.\n## # … with 274 more rows"},{"path":"data-wrangling.html","id":"arrange","chapter":"Lesson 2 Data Wrangling","heading":"2.4.6 arrange","text":"simple thing might want table sort based column. arrange :helper function desc marks column arranged descending order. can arrange multiple columns, first important.also introduced rename verb without warning. says , order names might confusing. new name comes first (like creating new column mutate). can also rename within select:","code":"\ngapminder %>% \n  arrange(year)## # A tibble: 1,704 x 7\n##    country     continent  year lifeExp      pop gdpPercap           gdp\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>         <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.   6567086330.\n##  2 Albania     Europe     1952    55.2  1282697     1601.   2053669902.\n##  3 Algeria     Africa     1952    43.1  9279525     2449.  22725632678.\n##  4 Angola      Africa     1952    30.0  4232095     3521.  14899557133.\n##  5 Argentina   Americas   1952    62.5 17876956     5911. 105676319105.\n##  6 Australia   Oceania    1952    69.1  8691212    10040.  87256254102.\n##  7 Austria     Europe     1952    66.8  6927772     6137.  42516266683.\n##  8 Bahrain     Asia       1952    50.9   120447     9867.   1188460759.\n##  9 Bangladesh  Asia       1952    37.5 46886859      684.  32082059995.\n## 10 Belgium     Europe     1952    68    8730405     8343.  72838686716.\n## # … with 1,694 more rows\ngapminder %>% \n  arrange(desc(year), pop) %>% \n  select(country, year, pop) %>% \n  rename(population = pop)## # A tibble: 1,704 x 3\n##    country                year population\n##    <chr>                 <dbl>      <dbl>\n##  1 Sao Tome and Principe  2007     199579\n##  2 Iceland                2007     301931\n##  3 Djibouti               2007     496374\n##  4 Equatorial Guinea      2007     551201\n##  5 Montenegro             2007     684736\n##  6 Bahrain                2007     708573\n##  7 Comoros                2007     710960\n##  8 Reunion                2007     798094\n##  9 Trinidad and Tobago    2007    1056608\n## 10 Swaziland              2007    1133066\n## # … with 1,694 more rows\ngapminder %>% select(country, year, population = pop)## # A tibble: 1,704 x 3\n##    country      year population\n##    <chr>       <dbl>      <dbl>\n##  1 Afghanistan  1952    8425333\n##  2 Afghanistan  1957    9240934\n##  3 Afghanistan  1962   10267083\n##  4 Afghanistan  1967   11537966\n##  5 Afghanistan  1972   13079460\n##  6 Afghanistan  1977   14880372\n##  7 Afghanistan  1982   12881816\n##  8 Afghanistan  1987   13867957\n##  9 Afghanistan  1992   16317921\n## 10 Afghanistan  1997   22227415\n## # … with 1,694 more rows"},{"path":"data-wrangling.html","id":"summarise","chapter":"Lesson 2 Data Wrangling","heading":"2.4.7 summarise","text":"condense one multiple columns summary values, use summarise: Like mutate, can calculate multiple things one step.condensing whole columns one value, flattening tibble style Super Mario jumping mushrooms, often need. rather know summaries within certain groups. example maximal gdp per country. group_by .","code":"\ngapminder %>% \n  summarise(\n    last_year = max(year),\n    average_pop = mean(pop),\n    minimal_gdp = min(gdp)\n  )## # A tibble: 1 x 3\n##   last_year average_pop minimal_gdp\n##       <dbl>       <dbl>       <dbl>\n## 1      2007   29601212.   52784691."},{"path":"data-wrangling.html","id":"group_by","chapter":"Lesson 2 Data Wrangling","heading":"2.4.8 group_by","text":"group_by considered adverb, doesn’t change data changes subsequent functions handle data. example, tibble groups, summaries calculated within groups:example, let’s look range life expectancy country:summarize removes one level grouping. data grouped multiple features, means groups remain. can make sure data longer grouped ungroup.Groups also work within mutate filter. example, can get rows gdp per Person highest per country:can use groups mutate find , percentage ’s continent country’s population makes per year:Sometimes want refer size current group inside mutate summarise. function just called n(). example, wonder many rows data per year.shortcut group_by summarise n() count function:general, might find solving particular problem couple steps elegant solution. discouraged ! simply means always learn, tools already know now get long way set right track.think learned enough dplyr verbs now. can treat little ggplot visualization.","code":"\ngapminder %>% \n  group_by(country)## # A tibble: 1,704 x 7\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap          gdp\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>        <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n## # … with 1,694 more rows\ngapminder %>% \n  group_by(country) %>% \n  summarise(lower_life_exp = min(lifeExp),\n            upper_life_exp = max(lifeExp))## # A tibble: 142 x 3\n##    country     lower_life_exp upper_life_exp\n##    <chr>                <dbl>          <dbl>\n##  1 Afghanistan           28.8           43.8\n##  2 Albania               55.2           76.4\n##  3 Algeria               43.1           72.3\n##  4 Angola                30.0           42.7\n##  5 Argentina             62.5           75.3\n##  6 Australia             69.1           81.2\n##  7 Austria               66.8           79.8\n##  8 Bahrain               50.9           75.6\n##  9 Bangladesh            37.5           64.1\n## 10 Belgium               68             79.4\n## # … with 132 more rows\ngapminder %>% \n  group_by(continent, year) %>% \n  summarise(mean_gdpPercap = mean(gdpPercap)) %>% \n  ungroup()## # A tibble: 60 x 3\n##    continent  year mean_gdpPercap\n##    <chr>     <dbl>          <dbl>\n##  1 Africa     1952          1253.\n##  2 Africa     1957          1385.\n##  3 Africa     1962          1598.\n##  4 Africa     1967          2050.\n##  5 Africa     1972          2340.\n##  6 Africa     1977          2586.\n##  7 Africa     1982          2482.\n##  8 Africa     1987          2283.\n##  9 Africa     1992          2282.\n## 10 Africa     1997          2379.\n## # … with 50 more rows\ngapminder %>%\n  group_by(country) %>% \n  filter(gdpPercap == max(gdpPercap))## # A tibble: 142 x 7\n## # Groups:   country [142]\n##    country     continent  year lifeExp       pop gdpPercap           gdp\n##    <chr>       <chr>     <dbl>   <dbl>     <dbl>     <dbl>         <dbl>\n##  1 Afghanistan Asia       1982    39.9  12881816      978.  12598563401.\n##  2 Albania     Europe     2007    76.4   3600523     5937.  21376411360.\n##  3 Algeria     Africa     2007    72.3  33333216     6223. 207444851958.\n##  4 Angola      Africa     1967    36.0   5247469     5523.  28980597822.\n##  5 Argentina   Americas   2007    75.3  40301927    12779. 515033625357.\n##  6 Australia   Oceania    2007    81.2  20434176    34435. 703658358894.\n##  7 Austria     Europe     2007    79.8   8199783    36126. 296229400691.\n##  8 Bahrain     Asia       2007    75.6    708573    29796.  21112675360.\n##  9 Bangladesh  Asia       2007    64.1 150448339     1391. 209311822134.\n## 10 Belgium     Europe     2007    79.4  10392226    33693. 350141166520.\n## # … with 132 more rows\ngapminder %>% \n  group_by(continent, year) %>% \n  mutate(pctPop = pop / sum(pop) * 100)## # A tibble: 1,704 x 8\n## # Groups:   continent, year [60]\n##    country     continent  year lifeExp      pop gdpPercap          gdp pctPop\n##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>        <dbl>  <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.  0.604\n##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.  0.591\n##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.  0.605\n##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.  0.605\n##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.  0.608\n##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.  0.624\n##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.  0.494\n##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.  0.483\n##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.  0.521\n## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.  0.657\n## # … with 1,694 more rows\ngapminder %>% \n  group_by(year) %>% \n  summarise(n = n())## # A tibble: 12 x 2\n##     year     n\n##    <dbl> <int>\n##  1  1952   142\n##  2  1957   142\n##  3  1962   142\n##  4  1967   142\n##  5  1972   142\n##  6  1977   142\n##  7  1982   142\n##  8  1987   142\n##  9  1992   142\n## 10  1997   142\n## 11  2002   142\n## 12  2007   142\ngapminder %>% \n  count(year)## # A tibble: 12 x 2\n##     year     n\n##    <dbl> <int>\n##  1  1952   142\n##  2  1957   142\n##  3  1962   142\n##  4  1967   142\n##  5  1972   142\n##  6  1977   142\n##  7  1982   142\n##  8  1987   142\n##  9  1992   142\n## 10  1997   142\n## 11  2002   142\n## 12  2007   142"},{"path":"data-wrangling.html","id":"visualization-and-our-first-encounter-with-factors","chapter":"Lesson 2 Data Wrangling","heading":"2.5 Visualization and our First Encounter with factors","text":"facet_wrap function slices plot theses subplots, style plot sometimes referred small multiples. point might wonder: “control order facets?” answer : factor! time vector can thought representing discrete categories (ordered unordered), can express turning vector factor factor function. enables R’s functions handle appropriately. Let’s create little example. start character vector.Note new information R gives us, Levels, possible values can put factor. automatically ordered alphabetically creation. can also pass vector levels creation.factor can contain elements levels, omitted whale shark, turned NA. tidyverse contains forcats package help factors. functions package start fct_.example, fct_relevel function, keeps levels let’s us change order:Using action, get:saved plot variable called plt need later. Let’s make plot bit prettier adding color! gapminder package provided dataset also included nice color palette. included .csv file data/ folder can practice importing data . also take shortcut getting straight package (gapminder::country_colors). , using head function look first couple rows tibble look first couple elements named vector package.Notice, csv read translates tibble two columns, namely country color. need ggplot assign colors countries gapminder package provides: names vector. names countries values called hexadecimal (short Hex) color codes (https://www.w3schools.com/colors/colors_hexadecimal.asp). want translate tibble named vector:Two things happened sort new. set_names handy way take vector return vector names. fits better tidyverse syntax “old” way assigning names(x) <- c(\"new\", \"names\", \"vector\"). secondly pull can though pipeable dollar. pulls column tibble element named list.Now, color vector ready go, can make plot pretty. Remember variable saved plot ? can add ggplot functions just like regular ggplot. guide generalization legend, setting none adding legend 142 different colors (/countries) fill whole plot.also added theme modified axis titles. might already notice number pronounced dips lines. investigate rather bleak reality talk modeling iteration next week.","code":"\ngapminder %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line(alpha = 0.3) +\n  facet_wrap(~ continent)\nanimals <- c(\"cat\", \"dog\", \"parrot\", \"whale shark\", \"bear\")\nanimals## [1] \"cat\"         \"dog\"         \"parrot\"      \"whale shark\" \"bear\"\nanimals <- factor(animals)\nanimals## [1] cat         dog         parrot      whale shark bear       \n## Levels: bear cat dog parrot whale shark\nanimals <- c(\"cat\", \"dog\", \"parrot\", \"whale shark\", \"bear\")\nfactor(animals, levels = c(\"parrot\", \"cat\", \"dog\", \"bear\"))## [1] cat    dog    parrot <NA>   bear  \n## Levels: parrot cat dog bear\nanimals <- factor(animals)\nfct_relevel(animals, c(\"parrot\", \"dog\"))## [1] cat         dog         parrot      whale shark bear       \n## Levels: parrot dog bear cat whale shark\nplt <- gapminder %>% \n  mutate(continent = fct_relevel(continent, \"Europe\", \"Oceania\")) %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line(alpha = 0.3) +\n  facet_wrap(~ continent)\n\nplt\ncountry_colors <- read_csv(\"data/02/country_colors.csv\")\nhead(country_colors)## # A tibble: 6 x 2\n##   country          color  \n##   <chr>            <chr>  \n## 1 Nigeria          #7F3B08\n## 2 Egypt            #833D07\n## 3 Ethiopia         #873F07\n## 4 Congo, Dem. Rep. #8B4107\n## 5 South Africa     #8F4407\n## 6 Sudan            #934607\nhead(gapminder::country_colors)##          Nigeria            Egypt         Ethiopia Congo, Dem. Rep. \n##        \"#7F3B08\"        \"#833D07\"        \"#873F07\"        \"#8B4107\" \n##     South Africa            Sudan \n##        \"#8F4407\"        \"#934607\"\ncountry_colors <- country_colors %>% \n  mutate(color = set_names(color, country)) %>% \n  pull(color)\n\nhead(country_colors)##          Nigeria            Egypt         Ethiopia Congo, Dem. Rep. \n##        \"#7F3B08\"        \"#833D07\"        \"#873F07\"        \"#8B4107\" \n##     South Africa            Sudan \n##        \"#8F4407\"        \"#934607\"\ngapminder$year %>% head()## [1] 1952 1957 1962 1967 1972 1977\ngapminder %>% pull(year) %>% head()## [1] 1952 1957 1962 1967 1972 1977\nplt +\n  aes(color = country) +\n  scale_color_manual(values = country_colors, guide = guide_none()) +\n  theme_minimal() + \n  labs(x = \"\",\n       y = \"Life Expectancy at Birth\",\n       title = \"Life Expectancy over Time\")"},{"path":"data-wrangling.html","id":"exercises-1","chapter":"Lesson 2 Data Wrangling","heading":"2.6 Exercises","text":"Drink cup coffee tea, relax, \njust worked quite long video.Familiarize folders computer.\nMake sure understand, directories files live.RStudio, create new RStudio project course.\nInside project folder, create folder\ndata.\nCreate new Rmarkdown document top level \nproject folder today’s exercises, including\nquestions came course.\nDownload data today one ways taught.\ncan refer script anytime.\nMake sure important settings set\nfeeling right home RStudio.\nInside project folder, create folder\ndata.Create new Rmarkdown document top level \nproject folder today’s exercises, including\nquestions came course.Download data today one ways taught.\ncan refer script anytime.Make sure important settings set\nfeeling right home RStudio.file ./data/exercise1.txt unfamiliar format.\nFind structured read tibble.\nCreate scatterplot x y column ggplot2.\nLook help page geom_point.\ndifference geom_point(aes(color = <something>)) geom_point(color = <something>)?\nrelevant hint section ...-argument.\nMake plot pretty coloring points,\nkeeping mind distinction.\nFind structured read tibble.Create scatterplot x y column ggplot2.Look help page geom_point.\ndifference geom_point(aes(color = <something>)) geom_point(color = <something>)?\nrelevant hint section ...-argument.Make plot pretty coloring points,\nkeeping mind distinction.Read gapminder dataset readr\nUsing combination dplyr verbs / \nvisualizations ggplot2,\nanswer following questions:\ncontinent highest life expectancy average\ncurrent year? two options .\nFirst, calculate simple mean countries\ncontinent. , remember countries\ndifferent population sizes, really need\nweighted mean using R’s function weighted.mean().\nrelationship GDP per capita \nlife expectancy? visualization might helpful.\npopulation countries change time?\nMake plot informative adding color,\nfacets labels (geom_text). Can find ,\nadd country name label last year?\nHint: look data argument geom_-functions\n.\nUsing combination dplyr verbs / \nvisualizations ggplot2,\nanswer following questions:continent highest life expectancy average\ncurrent year? two options .\nFirst, calculate simple mean countries\ncontinent. , remember countries\ndifferent population sizes, really need\nweighted mean using R’s function weighted.mean().relationship GDP per capita \nlife expectancy? visualization might helpful.population countries change time?\nMake plot informative adding color,\nfacets labels (geom_text). Can find ,\nadd country name label last year?\nHint: look data argument geom_-functions\n.","code":""},{"path":"data-wrangling.html","id":"resources","chapter":"Lesson 2 Data Wrangling","heading":"2.7 Resources","text":"","code":""},{"path":"data-wrangling.html","id":"package-documentation","chapter":"Lesson 2 Data Wrangling","heading":"2.8 Package Documentation","text":"tidyverse websiteThe readr package website cheatsheetThe dplyr package website cheatsheet","code":""},{"path":"data-wrangling.html","id":"getting-help","chapter":"Lesson 2 Data Wrangling","heading":"2.9 Getting Help","text":"find helpR4DS online learning community","code":""},{"path":"tidy-data.html","id":"tidy-data","chapter":"Lesson 3 Tidy Data","heading":"Lesson 3 Tidy Data","text":"… explore concept Tidy Data, learn \nadvanced data wrangling techniques get sneak peak \niteration.","code":""},{"path":"tidy-data.html","id":"tidy-data-1","chapter":"Lesson 3 Tidy Data","heading":"3.1 Tidy data","text":"","code":""},{"path":"tidy-data.html","id":"what-and-why-is-tidy-data","chapter":"Lesson 3 Tidy Data","heading":"3.1.1 What and Why is Tidy Data?","text":"one concept also lends ’s name \ntidyverse want talk .\nTidy Data way turning datasets uniform shape.\nmakes easier develop work tools get consistent interface.\nknow turn dataset tidy dataset,\nhome turf can express ideas fluently code.\nGetting can sometimes tricky,\ngive important tools.\n…»Tidy datasets alike, every messy dataset messy way.« — Hadley Wickhamfreely adapted :»Happy families alike; every unhappy family unhappy way.« — Leo TolstoySo, can recognize tidy data?\nFigure 3.1: Figure https://r4ds..co.nz/tidy-data.html26\ntidy data, variable (feature) forms ’s column.\nobservation forms row.\ncell single value (measurement).\nFurthermore, information things belongs one table.tidyr package contained tidyverse provides small example datasets demonstrate means practice. Hadley Wickham Garrett Grolemund use book well (https://r4ds..co.nz/tidy-data.html).27table1, table2, table3, table4a, table4b,\ntable5 display number TB cases documented \nWorld Health Organization Afghanistan, Brazil,\nChina 1999 2000.\nfirst tidy format, others :nicely qualifies tidy data.\nEvery row uniquely identified country year,\ncolumns properties specific country\nspecific year.Now gets interesting. table2 still looks organized,\ntidy (definition).\nNote, doesn’t say format useless — ’s places —\nfit snugly tools.\ncolumn type feature country,\nrather actual features hidden column \nvalues count column.\norder make tidy, dataset need get wider.table3, two features jammed one column.\nannoying, can’t easily calculate\nvalues; stored text \nseparated slash like cases/population.\nIdeally, want separate column two.table4a table4b split data two different tables,\nmakes harder calculate .\ndata closely related, want one table.\nanother principle tidy data violated.\nNotice column names?\n1999 feature Afghanistan can .\nRather, value feature (namely year),\nvalues 1999 column fact\nvalues feature population (table4a)\ncases (table4b).table5, problem table3 \nadditionally opposite problem!\ntime, feature one column (namely year)\nspread across two columns (century year).\nwant unite one.","code":"\nlibrary(tidyverse)\n\n# The paged_table function from the rmarkdown package prints the tables\n# for the rmarkdown html document format so that it looks nice in the script.\n# Alternatively you could use\n# knitr's kable function\npaged_table(table1)\npaged_table(table2)\npaged_table(table3)\npaged_table(table4a)\npaged_table(table4b)\npaged_table(table5)"},{"path":"tidy-data.html","id":"making-data-tidy","chapter":"Lesson 3 Tidy Data","heading":"3.1.2 Making Data Tidy","text":"tidyr package.Note: go data better another format course.\nExamples might involve matrices benefit matrix math \nmultidimensional equivalent: arrays.Let’s make data tidy!","code":""},{"path":"tidy-data.html","id":"pivot_wider","chapter":"Lesson 3 Tidy Data","heading":"3.1.3 pivot_wider","text":"Starting table2, wants wider.\nAccordingly, use function pivot_wider.\npowerful, two \nimportant arguments (well, data) \ncolumn contains names new columns\ncolumn contains values \nnewly created columns shown .","code":"\ntable2 %>% \n  pivot_wider(names_from = type, values_from = count)## # A tibble: 6 x 4\n##   country      year  cases population\n##   <chr>       <int>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583"},{"path":"tidy-data.html","id":"separate","chapter":"Lesson 3 Tidy Data","heading":"3.1.4 separate","text":"make table3 tidy, need separate function,\nfollowed mutate get new columns \ncorrect datatype.\nRun step step see mutate\nafterwards necessary.parse_ functions come readr.\nParsing act turning raw text usable data,\nparse_number function particularly good\njob extracting numbers, naïve approach \n.numeric might fail e.g.","code":"\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\") %>% \n  mutate(cases = parse_number(cases),\n         population = parse_number(population))## # A tibble: 6 x 4\n##   country      year  cases population\n##   <chr>       <int>  <dbl>      <dbl>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\nsome_text = \"take my number: 12\"\nas.numeric(some_text)## [1] NA\nparse_number(some_text)## [1] 12"},{"path":"tidy-data.html","id":"left_join","chapter":"Lesson 3 Tidy Data","heading":"3.1.5 left_join","text":"Now time join table4a tabl4b together.\n, need operation known databases\njoin.\nfact, whole concept \ntidy data closely related databases \nsomething called Codd`s normal forms28 throwing \nreferences just case interested \ntheoretical foundations.\nwithout ado:","code":"\n# the suffix argument is necessary because the columns\n# in the two tables have the same names\ntable4 <- left_join(table4a, table4b, by = \"country\",\n                    suffix = c(\"_cases\", \"_population\"))\ntable4## # A tibble: 3 x 5\n##   country     `1999_cases` `2000_cases` `1999_population` `2000_population`\n##   <chr>              <int>        <int>             <int>             <int>\n## 1 Afghanistan          745         2666          19987071          20595360\n## 2 Brazil             37737        80488         172006362         174504898\n## 3 China             212258       213766        1272915272        1280428583"},{"path":"tidy-data.html","id":"pivot_longer","chapter":"Lesson 3 Tidy Data","heading":"3.1.6 pivot_longer","text":"Now can deal next problem:\nMaking table longer \nyear, population cases become \ncolumns , independent features.\nConsequently, use function pivot_longer:done yet, table4 putting skills\ntest.\nway following steps \npivot_wider straight away, step \nstep easier follow.\ncan revisit part check \narguments pivot_wider fancy challenge.\nnext step already know: separating\ncolumn two.\nseparator changed.Now back problem table2,\nknow make table wider :!\nNow one last look table5, contains case\nhaven’t encountered yet: feature separated\nacross two columns, need opposite \nsepareate: unite.","code":"\n# note that cols takes what is called a tidyselect specification.\n# you know this from the selection function and it's help page.\n# here, I am selecting everything BUT the country column.\nlong_table4 <- table4 %>% \n  pivot_longer(cols = -country)\nlong_table4## # A tibble: 12 x 3\n##    country     name                 value\n##    <chr>       <chr>                <int>\n##  1 Afghanistan 1999_cases             745\n##  2 Afghanistan 2000_cases            2666\n##  3 Afghanistan 1999_population   19987071\n##  4 Afghanistan 2000_population   20595360\n##  5 Brazil      1999_cases           37737\n##  6 Brazil      2000_cases           80488\n##  7 Brazil      1999_population  172006362\n##  8 Brazil      2000_population  174504898\n##  9 China       1999_cases          212258\n## 10 China       2000_cases          213766\n## 11 China       1999_population 1272915272\n## 12 China       2000_population 1280428583\nlong_table4_separated <- long_table4 %>% \n  separate(col = name, into = c(\"year\", \"type\"), sep = \"_\")\nlong_table4_separated## # A tibble: 12 x 4\n##    country     year  type            value\n##    <chr>       <chr> <chr>           <int>\n##  1 Afghanistan 1999  cases             745\n##  2 Afghanistan 2000  cases            2666\n##  3 Afghanistan 1999  population   19987071\n##  4 Afghanistan 2000  population   20595360\n##  5 Brazil      1999  cases           37737\n##  6 Brazil      2000  cases           80488\n##  7 Brazil      1999  population  172006362\n##  8 Brazil      2000  population  174504898\n##  9 China       1999  cases          212258\n## 10 China       2000  cases          213766\n## 11 China       1999  population 1272915272\n## 12 China       2000  population 1280428583\nlong_table4_separated %>% \n  pivot_wider(names_from = type, values_from = value)## # A tibble: 6 x 4\n##   country     year   cases population\n##   <chr>       <chr>  <int>      <int>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583"},{"path":"tidy-data.html","id":"unite","chapter":"Lesson 3 Tidy Data","heading":"3.1.7 unite","text":"","code":"\ntable5 %>% \n  unite(col = year, c(century, year), sep = \"\") %>% \n  mutate(year = parse_number(year))## # A tibble: 6 x 3\n##   country      year rate             \n##   <chr>       <dbl> <chr>            \n## 1 Afghanistan  1999 745/19987071     \n## 2 Afghanistan  2000 2666/20595360    \n## 3 Brazil       1999 37737/172006362  \n## 4 Brazil       2000 80488/174504898  \n## 5 China        1999 212258/1272915272\n## 6 China        2000 213766/1280428583"},{"path":"tidy-data.html","id":"another-example","chapter":"Lesson 3 Tidy Data","heading":"3.1.8 Another Example","text":"Let us look one last example data needs tidying,\nalso provided tidyr package example:lot columns!\n76 weeks song entered top 100 (assume USA)\n’s position recorded.\nmight format made data entry easier,\nprevious person wanted make plots excel,\nwide format used denote multiple traces.\nevent, style visualizations grammar \ngraphics, want column represent feature,\ndata needs get longer:time leaving visualisation .\ngreate opportunity play around ggplot!whole tidy data idea might seem like just another\nway moving numbers around.\nbuild mental model ,\ntruly transform way able think data.\ndata wrangling dplyr, shown last\nweek, also data visualization ggplot,\njourney began first week \nstill well underway.","code":"\nhead(billboard) %>% paged_table()\nlong_billboard <- billboard %>% \n  pivot_longer(starts_with(\"w\"),\n               names_to = \"week\",\n               names_pattern = \"wk(\\\\d+)\",\n               values_to = \"placement\",\n               values_drop_na = TRUE) %>% \n  mutate(week = parse_integer(week),\n         date = as.Date(date.entered) + 7 * (week - 1)) %>% \n  select(-date.entered)\n\nlong_billboard %>% \n  head() %>% \n  paged_table()"},{"path":"tidy-data.html","id":"more-shapes-for-data","chapter":"Lesson 3 Tidy Data","heading":"3.2 More Shapes for Data","text":"tidyr package provides tools dealing\ndata various shapes.\njust discovered first set operations called pivoting\nget feel tidy data obtain\nvarious formats.\ndata always rectangular\nlike can show spreadsheet.\nSometimes data already comes nested\nform, sometimes create nested data serves\npurpose (see next week).\n, mean nested?","code":""},{"path":"tidy-data.html","id":"nested-data","chapter":"Lesson 3 Tidy Data","heading":"3.2.1 Nested Data","text":"Remember lists can contain elements\ntype, even lists?\nWell, list contains\nlists, call nested. E.g.nested list always fun work ,\nstraightforward way represent\ndata rectangular, flat format,\nlikely want .\ndeal data rectangling today well.\nfirst, another implication\nnested lists:\ndataframes (tibbles) built top\nlists, can nest !\ncan sometimes come really handy.\ndataframe contains column \natomic vector list (list list),\ncall list column:, output code tells us,\nlist column y contains \natomic character vector length 1,\nlogical vector length 1 \ndouble vector length 4.\noverall length column 3,\nfit tibble.can get better view \neither pulling column \nsubsetting functions already learned\n(like $, [[]], pull), using\nstr learn structure\nsimply using RStudio’s Environment panel\ninspect variable.\nClick name view new window\n(can done code using\nfunction View Ctrl+Clicking variable name)\nuse little blue arrow expand list.extends even ,\ngo full inception !\nlist column can contain tibbles (/dataframes)\nwell!can still use data nested ,\nlong remember chain\nsubsetting functions together.handy way subsett nested structures\npluck function:incredibly useful next week,\ntalk iteration.Creating tibble hand unlikely\ngoing way end \nnested tibble.\nLet’s create one already existing tibble.\ncan use long version \nbillboard dataset created .Notice, similar group_by,\nexcept much explicit.\nresulting groups now separated\ntibble \nlist column named data.\n, RStudio’s environment panel\nmakes easier inspect data.can get back original shape\nunnesting tibbleUnnesting one special case \ngeneral idea data rectangling, \ntidyr package provides functions.\nUnfortunately, can’t explore today.","code":"\nlist(\n  c(1, 2),\n  list(\n    42, list(\"hi\", TRUE)\n  )\n)## [[1]]\n## [1] 1 2\n## \n## [[2]]\n## [[2]][[1]]\n## [1] 42\n## \n## [[2]][[2]]\n## [[2]][[2]][[1]]\n## [1] \"hi\"\n## \n## [[2]][[2]][[2]]\n## [1] TRUE\nexample <- tibble(\n  x = 1:3,\n  y = list(\"hello\", TRUE, c(1,2,3,4))\n)\nexample## # A tibble: 3 x 2\n##       x y        \n##   <int> <list>   \n## 1     1 <chr [1]>\n## 2     2 <lgl [1]>\n## 3     3 <dbl [4]>\ninclude_graphics(\"images/environment.png\")\nnested_tibble <- tibble(\n  id = 1:2,\n  df = list(\n    tibble(x = 1:10, y = x^2),\n    tibble(x = seq(1, 100, 3), y = sqrt(x))\n  )\n)\n\nnested_tibble## # A tibble: 2 x 2\n##      id df               \n##   <int> <list>           \n## 1     1 <tibble [10 × 2]>\n## 2     2 <tibble [34 × 2]>\n# take the column df, take the second element\nnested_tibble$df[[2]]## # A tibble: 34 x 2\n##        x     y\n##    <dbl> <dbl>\n##  1     1  1   \n##  2     4  2   \n##  3     7  2.65\n##  4    10  3.16\n##  5    13  3.61\n##  6    16  4   \n##  7    19  4.36\n##  8    22  4.69\n##  9    25  5   \n## 10    28  5.29\n## # … with 24 more rows\nnested_tibble %>% pluck(\"df\", 2)## # A tibble: 34 x 2\n##        x     y\n##    <dbl> <dbl>\n##  1     1  1   \n##  2     4  2   \n##  3     7  2.65\n##  4    10  3.16\n##  5    13  3.61\n##  6    16  4   \n##  7    19  4.36\n##  8    22  4.69\n##  9    25  5   \n## 10    28  5.29\n## # … with 24 more rows\nnested_tibble$df[[2]] %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point() + \n  theme_minimal()\nnested_billboard <- long_billboard %>% \n  nest(data = c(-artist, -track))\n\nhead(nested_billboard)## # A tibble: 6 x 3\n##   artist       track                   data             \n##   <chr>        <chr>                   <list>           \n## 1 2 Pac        Baby Don't Cry (Keep... <tibble [7 × 3]> \n## 2 2Ge+her      The Hardest Part Of ... <tibble [3 × 3]> \n## 3 3 Doors Down Kryptonite              <tibble [53 × 3]>\n## 4 3 Doors Down Loser                   <tibble [20 × 3]>\n## 5 504 Boyz     Wobble Wobble           <tibble [18 × 3]>\n## 6 98^0         Give Me Just One Nig... <tibble [20 × 3]>\nnested_billboard %>% \n  unnest(data) %>% \n  head()## # A tibble: 6 x 5\n##   artist track                    week placement date      \n##   <chr>  <chr>                   <int>     <dbl> <date>    \n## 1 2 Pac  Baby Don't Cry (Keep...     1        87 2000-02-26\n## 2 2 Pac  Baby Don't Cry (Keep...     2        82 2000-03-04\n## 3 2 Pac  Baby Don't Cry (Keep...     3        72 2000-03-11\n## 4 2 Pac  Baby Don't Cry (Keep...     4        77 2000-03-18\n## 5 2 Pac  Baby Don't Cry (Keep...     5        87 2000-03-25\n## 6 2 Pac  Baby Don't Cry (Keep...     6        94 2000-04-01"},{"path":"tidy-data.html","id":"a-sneak-peak-at-iteration","chapter":"Lesson 3 Tidy Data","heading":"3.3 A Sneak Peak at Iteration","text":"Next week going explore\nconcept iteration, R one\noperation multiple things.\ntoday, already giving \nsneak peak nested data can help\nus respect.\nalso make sure get \ncomfortable writing functions.Let us take just one dataset \nmany datasets:contains performance one song.\ncan create plot :Moreover, can create function,\ntakes data creates\nplot :can now pass song data \nnested dataframe :ambitious, want \nplot songs!\nmap function purrr package\ntakes list function \napplies function elements \nlist. really powerful (fun).\nlearn next week,\npreview:now can look individual plots:might handy include title\nplot, let’s modify\nplotting function little bit:now use function map2 instead map,\niterates two vectors \ntime also pass plot title:don’t care return value\nfunction, rather side-effect\n, use walk function place map.\nside-effect anything changes state\nprogram world around ,\nopposed pure functions, depend\narguments return value.\nWriting file example side effect:try home, feels quite good\nachieve much one key-press!","code":"\none_track <- nested_billboard$data[[1]]\none_track## # A tibble: 7 x 3\n##    week placement date      \n##   <int>     <dbl> <date>    \n## 1     1        87 2000-02-26\n## 2     2        82 2000-03-04\n## 3     3        72 2000-03-11\n## 4     4        77 2000-03-18\n## 5     5        87 2000-03-25\n## 6     6        94 2000-04-01\n## 7     7        99 2000-04-08\nggplot(one_track, aes(week, placement)) +\n  geom_line() +\n  theme_linedraw()\nplot_song_performance <- function(song_data) {\n  ggplot(song_data, aes(week, placement)) +\n    geom_line() +\n    theme_linedraw()\n}\nplot_song_performance(nested_billboard$data[[1]])\nsong_performance <- nested_billboard %>% \n   mutate(\n      plt = map(data, plot_song_performance)\n   )\n\nsong_performance %>% head()## # A tibble: 6 x 4\n##   artist       track                   data              plt   \n##   <chr>        <chr>                   <list>            <list>\n## 1 2 Pac        Baby Don't Cry (Keep... <tibble [7 × 3]>  <gg>  \n## 2 2Ge+her      The Hardest Part Of ... <tibble [3 × 3]>  <gg>  \n## 3 3 Doors Down Kryptonite              <tibble [53 × 3]> <gg>  \n## 4 3 Doors Down Loser                   <tibble [20 × 3]> <gg>  \n## 5 504 Boyz     Wobble Wobble           <tibble [18 × 3]> <gg>  \n## 6 98^0         Give Me Just One Nig... <tibble [20 × 3]> <gg>\nsong_performance$plt[[24]]\nplot_song_performance <- function(song_data, title) {\n  ggplot(song_data, aes(week, placement)) +\n    geom_line() +\n    theme_linedraw() +\n    labs(title = title)\n}\nsong_performance <- nested_billboard %>% \n   mutate(\n      plt = map2(data, track,  plot_song_performance)\n   )\n\nsong_performance$plt[[1]]\ndir.create(\"plts\")\n\nsave_plot <- function(name, plot) {\n  ggsave(paste0(\"plts/\", name, \".png\"), plot)\n}\n\nwalk2(song_performance$track, song_performance$plt, save_plot)"},{"path":"tidy-data.html","id":"exercises-2","chapter":"Lesson 3 Tidy Data","heading":"3.4 Exercises","text":"","code":""},{"path":"tidy-data.html","id":"tidy-data-2","chapter":"Lesson 3 Tidy Data","heading":"3.5 Tidy Data","text":"Imagine second whole pandemic thing \ngoing planning vacation.\ncourse, want choose safest airline possible,\ndownload data incident reports.\ncan find data folder29.Instead type_of_event n_events columns\nlike one column per type event,\nvalues count event.airlines least fatal accidents,\nstandardized distance theses airlines covered,\ntwo time ranges?Even something go wrong, airlines\nbest record comes fatalities per fatal accident?Create informative plot highlight \ndiscoveries. might beneficial \nplot e.g. highest lowest scoring Airlines.\nOne slice_ functions help .\nmake plot prettier, \nlook fct_reorder .","code":""},{"path":"tidy-data.html","id":"functions-and-iteration","chapter":"Lesson 3 Tidy Data","heading":"3.6 Functions and Iteration","text":"going strong far! Programming can sometimes\nmotivate us. R task.\nfrustrating, think need someone\nWrite function takes name, like “Jannik” \nname, spits motivational message \nincludes name.\nMake sure also test show works.Now motivational machine,\nneed make sure motivate greatest\namount people.\nUse map function apply function\nwhole vector names.","code":""},{"path":"tidy-data.html","id":"resources-1","chapter":"Lesson 3 Tidy Data","heading":"3.7 Resources","text":"tidyr documentationpurrr documentationstringr documentation\nworking text helpful cheatsheet \nregular expressions mentioned video","code":""},{"path":"functions-1.html","id":"functions-1","chapter":"Lesson 4 Functions","heading":"Lesson 4 Functions","text":"… functions, bringing whole tidyverse together \nexploring advanced dplyr data wrangling techniques.","code":""},{"path":"functions-1.html","id":"motivation","chapter":"Lesson 4 Functions","heading":"4.1 Motivation","text":"goal today bring together everything\nlearned far solidify\nunderstanding wrangling data tidyverse.\ngoes according plan,\nmental capacity\nfreed statistics starting next week.\nunderstanding data hopefully\nenable us experiment play statistical\nconcepts without getting stuck much\ndata wrangling.\nalso means today’s lecture might\nchallenging far, \neverything learned now – \none way another – relevant.might able tell,\nmental models one favorite topics. \nstarting today powerful mental model\nteased last week. talking \niteration.","code":""},{"path":"functions-1.html","id":"iteration","chapter":"Lesson 4 Functions","heading":"4.2 Iteration","text":"Iteration basic idea one thing multiple times. area\ncomputers shine, chapter learn fully utilize \npower fingertips.first encounter iteration implicit form. use\nR’s basic math operators, computer iterating behind scenes. Take \nexpression:operation vectorized. Without us tell R , R add\nfirst element first vector first element second vector\nforth.Notice, looks like operation happens time. \nreality, happens. computer just really fast adding\nnumbers, one .mathematical operations R call another programming language \nactual addition. programming language closer way computers\nthink, making less fun write us humans, also faster \ninstructions easier translate actions computer processor.find task want apply multiple things, task \nalready vectorized, need make iteration. actually\ntwo schools though talk iteration consequently, \nwrite code.use realworld example illustrate :\nReading Multiple Files","code":"\n1:3 + 1:3## [1] 2 4 6"},{"path":"functions-1.html","id":"functional-programming","chapter":"Lesson 4 Functions","heading":"4.2.1 Functional Programming","text":"Remember gapminder dataset? Well, working , \ntime, collaborator sent us one csv-file continent. usual, \nload tidyverse first. script also load rmarkdown package \nknitr package control printing tables.already know read one csv-file:function (read_csv) takes file path returns\n(spits ) data. first school thought, Functional\nProgramming style, next idea function, takes two things: \nfunction vector (atomic list). feeds individual elements \nvector function, one another. mathematics, relation\nset inputs set outputs called \nmap, name following family functions comes\n. tidyverse, functional programming concepts live \npurrr package.First, create vector things want iterate , things\nfed function one :Now 5 file paths. can test understanding passing just one \nread_csv make sure function works:time map function!’s ! now list contains five datasets.lost information file elements list came ,\nname continent! can fix using named list\ninstead regular list pass map:function str_remove part stringr package\ntidyverse. ’s functions, handle\nkinds operations text, start str_.now can combine one big dataset\nusing bind_rows, stacks tibbles top .\nextra argument .id determines name column\nstore names list\nitems.yet another shortcut can employ. purrr package contains\nvarious variants map function. map always return list,\nwhereas variants like map_chr always return atomic character\nvector,map_dbl always returns numbers, map_lgl always return logical (yes \n, TRUE / FALSE) vectors. Combining list one dataframe (rows) \ncommon also special map function : map_dfr.Now efficient! took us couple lines.\ncan even whole thing one chain functions\nusing pipe.\nkeep mind way came\n.\none expects come shortest \nconcise solution first go.\nsometimes can pay revisit first\nsolution involves multiple temporary variables\ndon’t really need rest script\nclean bit.\nmight make easier come back \ncode future able read\ngoing faster (less lines\nread).want take moment introduce yet another\nshortcut can take.\nNotice, set_names created function process\nnames without giving name.\ncalled anonymous function.\nSometimes people also call lambda function,\noriginates something called\nlambda calculus.\ntilde symbol ~ \nclosest get \\(\\lambda\\) English keyboard,\nused create anonymous functions\nfly.\ndon’t even worry names\narguments, automatically\ncreates function argument\nnamed “.x” (“.y” need multiple\narguments).\nSee documentation map,\nespecially .f argument,\ninformation.","code":"\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(knitr)\n# n_max = 3 is just here for the script\nread_csv(\"data/04/Africa.csv\", n_max = 3)## # A tibble: 3 x 5\n##   country  year lifeExp      pop gdpPercap\n##   <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n## 1 Algeria  1952    43.1  9279525     2449.\n## 2 Algeria  1957    45.7 10270856     3014.\n## 3 Algeria  1962    48.3 11000948     2551.\npaths <- dir(\"data/04/\", full.names = TRUE)\npaths## [1] \"data/04//Africa.csv\"   \"data/04//Americas.csv\" \"data/04//Asia.csv\"    \n## [4] \"data/04//Europe.csv\"   \"data/04//Oceania.csv\"\nread_csv(paths[[1]], n_max = 3)## # A tibble: 3 x 5\n##   country  year lifeExp      pop gdpPercap\n##   <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n## 1 Algeria  1952    43.1  9279525     2449.\n## 2 Algeria  1957    45.7 10270856     3014.\n## 3 Algeria  1962    48.3 11000948     2551.\nall_datasets <- map(paths, read_csv)\nnames(paths) <- basename(paths) %>%\n  str_remove(\"\\\\.csv\")\n\nmy_data <- map(paths, read_csv)\nmy_data %>%\n  bind_rows(.id = \"continent\")## # A tibble: 1,704 x 6\n##    continent country  year lifeExp      pop gdpPercap\n##    <chr>     <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n##  1 Africa    Algeria  1952    43.1  9279525     2449.\n##  2 Africa    Algeria  1957    45.7 10270856     3014.\n##  3 Africa    Algeria  1962    48.3 11000948     2551.\n##  4 Africa    Algeria  1967    51.4 12760499     3247.\n##  5 Africa    Algeria  1972    54.5 14760787     4183.\n##  6 Africa    Algeria  1977    58.0 17152804     4910.\n##  7 Africa    Algeria  1982    61.4 20033753     5745.\n##  8 Africa    Algeria  1987    65.8 23254956     5681.\n##  9 Africa    Algeria  1992    67.7 26298373     5023.\n## 10 Africa    Algeria  1997    69.2 29072015     4797.\n## # … with 1,694 more rows\ngapminder <- map_dfr(paths, read_csv, .id = \"continent\")\ngapminder## # A tibble: 1,704 x 6\n##    continent country  year lifeExp      pop gdpPercap\n##    <chr>     <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n##  1 Africa    Algeria  1952    43.1  9279525     2449.\n##  2 Africa    Algeria  1957    45.7 10270856     3014.\n##  3 Africa    Algeria  1962    48.3 11000948     2551.\n##  4 Africa    Algeria  1967    51.4 12760499     3247.\n##  5 Africa    Algeria  1972    54.5 14760787     4183.\n##  6 Africa    Algeria  1977    58.0 17152804     4910.\n##  7 Africa    Algeria  1982    61.4 20033753     5745.\n##  8 Africa    Algeria  1987    65.8 23254956     5681.\n##  9 Africa    Algeria  1992    67.7 26298373     5023.\n## 10 Africa    Algeria  1997    69.2 29072015     4797.\n## # … with 1,694 more rows\ngapminder <-\n  dir(\"data/04/\", full.names = TRUE) %>% \n  set_names(function(name) basename(name) %>% str_remove(\"\\\\.csv\")) %>%\n  map_dfr(read_csv, .id = \"continent\")\ngapminder <-\n  dir(\"data/04/\", full.names = TRUE) %>% \n  set_names(~ basename(.x) %>% str_remove(\"\\\\.csv\")) %>%\n  map_dfr(read_csv, .id = \"continent\")"},{"path":"functions-1.html","id":"the-imperative-programming-approach","chapter":"Lesson 4 Functions","heading":"4.2.2 The Imperative Programming Approach","text":"ways gone .\ncommon construct \nprogramming languages called -loop.\nevery element \nvector, body loop runs.\nexample, look like :-loop-version lot code, especially boilerplate, code \njust make construct work doesn’t convey intentions \ncode. Furthermore, loop focuses object iterated (file\npaths), map-version focuses happening (function,\nread_csv). loop still works. can’t think way solve \nproblem map function, absolutely OK use -loops.first approach map function comes Functional Programming,\nwhereas second approach considered Imperative Programming. general, \nFunctional Programming, tell computer want, \nImperative Programming, tell computer steps . keep \nmind:»course someone write -loops. doesn’t .«\n— Jenny BryanAnd don’t miss amazing purrr cheatsheet:\nlink","code":"\npaths <- dir(\"data\", full.names = TRUE)\nresults <- vector(\"list\", length(paths))\n\nfor (i in 1:length(paths)) {\n  data <- read_csv(paths[i])\n  results[[i]] <- data\n}\n\nnames(results) <- basename(paths) %>% str_remove(\"\\\\.csv\")\ngapminder <- bind_rows(results, .id = \"continent\")"},{"path":"functions-1.html","id":"if-you-copy-and-paste-the-same-code-more-than-three-times-write-a-function.","chapter":"Lesson 4 Functions","heading":"4.3 “If you copy and paste the same code more than three times, write a function.”","text":"","code":""},{"path":"functions-1.html","id":"noticing-a-pattern","chapter":"Lesson 4 Functions","heading":"4.3.1 Noticing a Pattern","text":"Writing functions can helpful making \ncode readable.\nallows us separate certain steps analysis\nrest, look isolation test \nvalidate , also allows us give reasonable names.\nLet’s look couple examples get \nwriting functions!\nSay idea, filter\ngapminder dataset one country, check\nlinear relationship year \nlife expectancy create plot.three function broom package\ntell us linear model created lm.\npart tidymodels framework.\\(R^2\\) value tells us, well straight\nline fits data.\ncan assume values 0 1.get curious want know graph\nlooks another country.\ncopy paste code replace name\ncountry.\nyet , want see plot another\ncountry, copy paste code,\nchange country name keep going.\nremember, easier\nway deal repetition.\nlook code identify\nthings stay \nthings change copy pasting.\nthings stay \nbody function,\nthings change \nvariables body pass \nfunction arguments.\nexample, one thing changes\nevery time: name country.\nbuild following function:test function bunch make sure\nworks different cases:","code":"\nfilterd_data <- gapminder %>% \n  filter(country == \"Norway\")\n\nmodel <- lm(lifeExp ~ year, data = filterd_data)\nbroom::tidy(model)## # A tibble: 2 x 5\n##   term        estimate std.error statistic      p.value\n##   <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept) -185.     16.2         -11.4 0.000000460 \n## 2 year           0.132   0.00819      16.1 0.0000000176\nbroom::augment(model)## # A tibble: 12 x 8\n##    lifeExp  year .fitted  .resid   .hat .sigma .cooksd .std.resid\n##      <dbl> <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n##  1    72.7  1952    72.2  0.455  0.295   0.483 0.256        1.11 \n##  2    73.4  1957    72.9  0.566  0.225   0.470 0.250        1.31 \n##  3    73.5  1962    73.5 -0.0640 0.169   0.516 0.00209     -0.143\n##  4    74.1  1967    74.2 -0.114  0.127   0.515 0.00450     -0.249\n##  5    74.3  1972    74.9 -0.513  0.0991  0.484 0.0671      -1.10 \n##  6    75.4  1977    75.5 -0.143  0.0851  0.514 0.00434     -0.306\n##  7    76.0  1982    76.2 -0.203  0.0851  0.511 0.00872     -0.433\n##  8    75.9  1987    76.8 -0.943  0.0991  0.396 0.226       -2.03 \n##  9    77.3  1992    77.5 -0.172  0.127   0.512 0.0103      -0.377\n## 10    78.3  1997    78.2  0.168  0.169   0.512 0.0144       0.376\n## 11    79.0  2002    78.8  0.238  0.225   0.508 0.0443       0.553\n## 12    80.2  2007    79.5  0.725  0.295   0.429 0.649        1.76\nbroom::glance(model)## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.963         0.959 0.490      260. 0.0000000176     1  -7.37  20.7  22.2\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nmodel_glance <- broom::glance(model)\n\ntext <- substitute(R^2 == rsq,\n                   list(rsq = round(model_glance$r.squared, 2)))\n\nfilterd_data %>% \n  ggplot(aes(year, lifeExp)) +\n  geom_smooth(method = \"lm\") +\n  geom_line() +\n  geom_point() +\n  theme_classic() +\n  labs(\n    x = \"Year\",\n    y = \"Life Expectancy\",\n    title = \"Norway\",\n    subtitle = text\n  )\nplot_life_exp_for_country <- function(country_name) {\n  filterd_data <- gapminder %>% \n    filter(country == country_name)\n  \n  model <- lm(lifeExp ~ year, data = filterd_data)\n  model_glance <- broom::glance(model)\n  text <- substitute(R^2 == rsq,\n                     list(rsq = round(model_glance$r.squared, 2)))\n  \n  filterd_data %>% \n    ggplot(aes(year, lifeExp)) +\n    geom_smooth(method = \"lm\") +\n    geom_line() +\n    geom_point() +\n    theme_classic() +\n    labs(\n      x = \"Year\",\n      y = \"Life Expectancy\",\n      title = country_name,\n      subtitle = text\n    )\n}\nplot_life_exp_for_country(\"India\")\nplot_life_exp_for_country(\"Mali\")"},{"path":"functions-1.html","id":"where-to-put-your-functions","chapter":"Lesson 4 Functions","heading":"4.3.2 Where to put your Functions","text":"point might wonder put function.\ngood starting point collect \nfunctions near top document script,\nload packages.\nAnother approach, especially functions\ncan use multiple projects,\nput .R file load\nfile beginning document.\ncan using function source(\"path//file.R\").\nruns R file current sessions,\nfunctions variables defined\nnow available .Just like .like store regular R files\n(opposed Rmd files) \nfolder project called R.\nmakes already look like R package,\ncase decide later functions\nhelpful others well\nwant share easily\ncolleagues.\ncan read creating \nR packages .30","code":"\nsource(\"R/my_funs.R\")\nsay_hello()"},{"path":"functions-1.html","id":"many-models","chapter":"Lesson 4 Functions","heading":"4.4 Many Models","text":"Let us use techniques shine\nlight life expectancies changed time.\ntime, utilize nested\ndata format store data\nalongside models data., extract information model\nfunctions form broom package.go ahead unnest information\nmodel.\ncan now see, countries\nstray farthest straight line\narranging \\(R^2\\) values.detailed walk-trough, check\nchapter “many models” \nR Data Science .31Let’s use information build \nvisualization.also like use opportunity \nmention couple packages take \nvisualizations next level.\nToday, ggrepel produce\nlabels dodge ,\nfisualize package, \ncontains color scales tropical fish.Sometimes also handy (probably impressive\nwhoever receives report) turn plot\ninteractive graphic.\nplotly library produces interactive plots,\ndon’t even learn intricacies\ncomes function convert ggplot\ninteractive format works quite well \nrange () use-cases:downward slope highlighted countries\nstarting 1990s result ravaging\nAIDS pandemic.\nprominent dips two curves, orange \nRwanda Cambodia gray, direct\nconsequences genocides.\ndire realities can way summarized\njust couple colorful lines.\nalso way qualified lecture\ntopics.\ngood friend mine, Timothy Williams,\nhowever researcher teacher field conflict violence\nfocus genocides.\nfield work Cambodia Rwanda\nbook “Complexity Evil. Perpetration Genocide”\n\npublished December 18.32","code":"\nnested_gapminder <- gapminder %>%\n  nest(-country, -continent) %>% \n  mutate(\n    model = map(data, ~ lm(lifeExp ~ year, data = .x))\n  )\n\nnested_gapminder## # A tibble: 142 x 4\n##    continent country                  data              model \n##    <chr>     <chr>                    <list>            <list>\n##  1 Africa    Algeria                  <tibble [12 × 4]> <lm>  \n##  2 Africa    Angola                   <tibble [12 × 4]> <lm>  \n##  3 Africa    Benin                    <tibble [12 × 4]> <lm>  \n##  4 Africa    Botswana                 <tibble [12 × 4]> <lm>  \n##  5 Africa    Burkina Faso             <tibble [12 × 4]> <lm>  \n##  6 Africa    Burundi                  <tibble [12 × 4]> <lm>  \n##  7 Africa    Cameroon                 <tibble [12 × 4]> <lm>  \n##  8 Africa    Central African Republic <tibble [12 × 4]> <lm>  \n##  9 Africa    Chad                     <tibble [12 × 4]> <lm>  \n## 10 Africa    Comoros                  <tibble [12 × 4]> <lm>  \n## # … with 132 more rows\nnested_gapminder %>% \n  mutate(\n    glance = map(model, broom::glance)\n  )## # A tibble: 142 x 5\n##    continent country                  data              model  glance           \n##    <chr>     <chr>                    <list>            <list> <list>           \n##  1 Africa    Algeria                  <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  2 Africa    Angola                   <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  3 Africa    Benin                    <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  4 Africa    Botswana                 <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  5 Africa    Burkina Faso             <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  6 Africa    Burundi                  <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  7 Africa    Cameroon                 <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  8 Africa    Central African Republic <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n##  9 Africa    Chad                     <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n## 10 Africa    Comoros                  <tibble [12 × 4]> <lm>   <tibble [1 × 12]>\n## # … with 132 more rows\ngapminder_modeled <- nested_gapminder %>% \n  mutate(\n    glance = map(model, broom::glance)\n  ) %>% \n  unnest(glance) %>% \n  arrange(r.squared)\n\ngapminder_modeled## # A tibble: 142 x 16\n##    continent country data  model r.squared adj.r.squared sigma statistic p.value\n##    <chr>     <chr>   <lis> <lis>     <dbl>         <dbl> <dbl>     <dbl>   <dbl>\n##  1 Africa    Rwanda  <tib… <lm>     0.0172      -0.0811   6.56     0.175  0.685 \n##  2 Africa    Botswa… <tib… <lm>     0.0340      -0.0626   6.11     0.352  0.566 \n##  3 Africa    Zimbab… <tib… <lm>     0.0562      -0.0381   7.21     0.596  0.458 \n##  4 Africa    Zambia  <tib… <lm>     0.0598      -0.0342   4.53     0.636  0.444 \n##  5 Africa    Swazil… <tib… <lm>     0.0682      -0.0250   6.64     0.732  0.412 \n##  6 Africa    Lesotho <tib… <lm>     0.0849      -0.00666  5.93     0.927  0.358 \n##  7 Africa    Cote d… <tib… <lm>     0.283        0.212    3.93     3.95   0.0748\n##  8 Africa    South … <tib… <lm>     0.312        0.244    4.74     4.54   0.0588\n##  9 Africa    Uganda  <tib… <lm>     0.342        0.276    3.19     5.20   0.0457\n## 10 Africa    Congo,… <tib… <lm>     0.348        0.283    2.43     5.34   0.0434\n## # … with 132 more rows, and 7 more variables: df <dbl>, logLik <dbl>,\n## #   AIC <dbl>, BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\nnon_linear_countries <- gapminder_modeled %>% \n  filter(r.squared < 0.2) %>% \n  unnest(data)\n\nnon_linear_countries## # A tibble: 72 x 19\n##    continent country  year lifeExp     pop gdpPercap model  r.squared\n##    <chr>     <chr>   <dbl>   <dbl>   <dbl>     <dbl> <list>     <dbl>\n##  1 Africa    Rwanda   1952    40   2534927      493. <lm>      0.0172\n##  2 Africa    Rwanda   1957    41.5 2822082      540. <lm>      0.0172\n##  3 Africa    Rwanda   1962    43   3051242      597. <lm>      0.0172\n##  4 Africa    Rwanda   1967    44.1 3451079      511. <lm>      0.0172\n##  5 Africa    Rwanda   1972    44.6 3992121      591. <lm>      0.0172\n##  6 Africa    Rwanda   1977    45   4657072      670. <lm>      0.0172\n##  7 Africa    Rwanda   1982    46.2 5507565      882. <lm>      0.0172\n##  8 Africa    Rwanda   1987    44.0 6349365      848. <lm>      0.0172\n##  9 Africa    Rwanda   1992    23.6 7290203      737. <lm>      0.0172\n## 10 Africa    Rwanda   1997    36.1 7212583      590. <lm>      0.0172\n## # … with 62 more rows, and 11 more variables: adj.r.squared <dbl>, sigma <dbl>,\n## #   statistic <dbl>, p.value <dbl>, df <dbl>, logLik <dbl>, AIC <dbl>,\n## #   BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\nplt <- gapminder %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line(alpha = 0.2) +\n  geom_line(data = non_linear_countries,\n            mapping = aes(color = country),\n            size = 1.7) +\n  fishualize::scale_color_fish_d() +\n  labs(y = \"Life Expectancy at Birth\") +\n  ggrepel::geom_text_repel(data = filter(non_linear_countries, year == max(year)),\n            aes(label = country, color = country),\n            hjust = 0, direction = \"y\") +\n  expand_limits(x = 2015) +\n  theme_minimal() +\n  guides(color = \"none\")\n\nplt\nplotly::ggplotly(plt)"},{"path":"functions-1.html","id":"advanced-dplyr","chapter":"Lesson 4 Functions","heading":"4.5 Advanced dplyr","text":"Last least, want mention powerful advanced\nfunction dplyr make data transformations even\nefficient.\nacross function, can use\ninside dplyr verbs mutate \nsummarise use one multiple functions \nmultiple columns.\nLet’s look example, gapminder\ndataset.\nNow, sort silly example,\nlet’s say want continents\ncountries UPPERCASE.\n:now everyone really shouty.\nrepetition \ncode. Wouldn’t cool,\njust say: “Apply function str_to_upper\ncolumns.”raw power!\ncan even general ask R \napply function columns contain text:also works summarise, example\ncreate summaries across range columns.\ncan supply one function calculate\nusing named list:way becoming true data wizard!\ntoday, familiar :importing data Rthe concept tidy datathe grammar graphicsthe basic dplyr verbs data wranglinga project-based workflowwriting using functionsWe use foundations experience\nstatistical concepts next lectures.","code":"\ngapminder %>% \n  mutate(\n    continent = str_to_upper(continent),\n    country = str_to_upper(country)\n  )## # A tibble: 1,704 x 6\n##    continent country  year lifeExp      pop gdpPercap\n##    <chr>     <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n##  1 AFRICA    ALGERIA  1952    43.1  9279525     2449.\n##  2 AFRICA    ALGERIA  1957    45.7 10270856     3014.\n##  3 AFRICA    ALGERIA  1962    48.3 11000948     2551.\n##  4 AFRICA    ALGERIA  1967    51.4 12760499     3247.\n##  5 AFRICA    ALGERIA  1972    54.5 14760787     4183.\n##  6 AFRICA    ALGERIA  1977    58.0 17152804     4910.\n##  7 AFRICA    ALGERIA  1982    61.4 20033753     5745.\n##  8 AFRICA    ALGERIA  1987    65.8 23254956     5681.\n##  9 AFRICA    ALGERIA  1992    67.7 26298373     5023.\n## 10 AFRICA    ALGERIA  1997    69.2 29072015     4797.\n## # … with 1,694 more rows\ngapminder %>% \n  mutate(\n    across(c(continent, country), str_to_upper)\n  )## # A tibble: 1,704 x 6\n##    continent country  year lifeExp      pop gdpPercap\n##    <chr>     <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n##  1 AFRICA    ALGERIA  1952    43.1  9279525     2449.\n##  2 AFRICA    ALGERIA  1957    45.7 10270856     3014.\n##  3 AFRICA    ALGERIA  1962    48.3 11000948     2551.\n##  4 AFRICA    ALGERIA  1967    51.4 12760499     3247.\n##  5 AFRICA    ALGERIA  1972    54.5 14760787     4183.\n##  6 AFRICA    ALGERIA  1977    58.0 17152804     4910.\n##  7 AFRICA    ALGERIA  1982    61.4 20033753     5745.\n##  8 AFRICA    ALGERIA  1987    65.8 23254956     5681.\n##  9 AFRICA    ALGERIA  1992    67.7 26298373     5023.\n## 10 AFRICA    ALGERIA  1997    69.2 29072015     4797.\n## # … with 1,694 more rows\ngapminder %>% \n  mutate(\n    across(where(is.character), str_to_upper)\n  )## # A tibble: 1,704 x 6\n##    continent country  year lifeExp      pop gdpPercap\n##    <chr>     <chr>   <dbl>   <dbl>    <dbl>     <dbl>\n##  1 AFRICA    ALGERIA  1952    43.1  9279525     2449.\n##  2 AFRICA    ALGERIA  1957    45.7 10270856     3014.\n##  3 AFRICA    ALGERIA  1962    48.3 11000948     2551.\n##  4 AFRICA    ALGERIA  1967    51.4 12760499     3247.\n##  5 AFRICA    ALGERIA  1972    54.5 14760787     4183.\n##  6 AFRICA    ALGERIA  1977    58.0 17152804     4910.\n##  7 AFRICA    ALGERIA  1982    61.4 20033753     5745.\n##  8 AFRICA    ALGERIA  1987    65.8 23254956     5681.\n##  9 AFRICA    ALGERIA  1992    67.7 26298373     5023.\n## 10 AFRICA    ALGERIA  1997    69.2 29072015     4797.\n## # … with 1,694 more rows\ngapminder %>% \n  summarise(\n    across(year:gdpPercap, list(mean = mean, total = sum))\n  )## # A tibble: 1 x 8\n##   year_mean year_total lifeExp_mean lifeExp_total  pop_mean   pop_total\n##       <dbl>      <dbl>        <dbl>         <dbl>     <dbl>       <dbl>\n## 1     1980.    3373068         59.5       101344. 29601212. 50440465801\n## # … with 2 more variables: gdpPercap_mean <dbl>, gdpPercap_total <dbl>"},{"path":"functions-1.html","id":"exercises-3","chapter":"Lesson 4 Functions","heading":"4.6 Exercises","text":"","code":""},{"path":"functions-1.html","id":"the-whole-deal","chapter":"Lesson 4 Functions","heading":"4.7 The whole Deal","text":"want get playing around data,\nkeep mind solutions exercise\nset stone.\noften one viable way graphing\ndataset use \nOffice Hour talk advantages\ndisadvantages approaches \ncame .","code":""},{"path":"functions-1.html","id":"roman-emperors","chapter":"Lesson 4 Functions","heading":"4.7.1 Roman emperors","text":"first exercise uses dataset roman emperors\ntidytuesday project\n(link).\ncan import :couple questions answer. Decide\nparticular question best\nanswered using visualization, table\nsimple sentence.popular way rise power?common causes death among roman\nemperors, () killed ?dynasty successful?\nFirstly, often dynasty reign?\nSecondly, long reigns?\ndynasty rather part ,\ngoal live longest?\nFirstly, often dynasty reign?Secondly, long reigns?dynasty rather part ,\ngoal live longest?","code":"\nemperors <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv\")"},{"path":"functions-1.html","id":"dairy-products-in-the-us","chapter":"Lesson 4 Functions","heading":"4.7.2 Dairy Products in the US","text":"Another dataset (link)\nconcerns dairy product consumption per person US\nacross number years.\nLoad withAll masses given lbs (pounds),\ncan convert kg?products lost customer base time,\nones won?, fun! make interesting\nfindings along way, go ahead produce plots\nhighlight .","code":"\ndairy <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv\")"},{"path":"functions-1.html","id":"resources-2","chapter":"Lesson 4 Functions","heading":"4.8 Resources","text":"purrr documentationstringr documentationdplyr documentation","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"Lesson 5 Hypothesis Testing","heading":"Lesson 5 Hypothesis Testing","text":"… reason nature randomness \ndiscover various statistical tests.","code":""},{"path":"hypothesis-testing.html","id":"motivation-1","chapter":"Lesson 5 Hypothesis Testing","heading":"5.1 Motivation","text":"first four lectures covered fundamentals handling data R.\nNow, shift focus away towards \ndata analysis. talk different statistical tests, common mistakes,\navoid spot research. course, \nusing R. still learn one useful function \ntechnique along way. instances clear use R\nsolely demonstrate idea statistics code just included \ncurious, whether code something likely also use \nanalysis. open questions things unclear two\ncases. purely aesthetic code might also speed typing edit.understand statistics means understanding nature randomness first.might shocked using tidyverse today well,\nsimply makes data wrangling various demonstrations easier. \nfollowing along, don’t forget:","code":"\nlibrary(tidyverse)"},{"path":"hypothesis-testing.html","id":"statistically-significant","chapter":"Lesson 5 Hypothesis Testing","heading":"5.2 Statistically Significant…","text":"…keep using word. don’t think means think means.\nFigure 5.1: Meme person says: “Statistically significant. keep using word. don’t think means think means.”\nhear phrases “statistically significant,” “significant” even\n“significant” thrown around quite bit academic literature . \noften used carelessly, clearly defined meaning. meaning\nuncover today. meaning related concept called\np-values, equally bad reputation frequently misused. \np p-value stands probability, order understand p-values, \nneed understand probability learn deal randomness, chance, \nluck . …","code":""},{"path":"hypothesis-testing.html","id":"getting-our-hands-dirty-with-probability","chapter":"Lesson 5 Hypothesis Testing","heading":"5.3 Getting our Hands dirty with Probability","text":"\nFigure 5.2: ggplot chessboard\nSay friend playing game chess, friend proudly\nproclaims:\n“definitely better player!”\n“Proof !” reply.\n“’s easy,” says: “won 7 8 rounds played today.”\n“Pah! ’s just luck.” less witty slightly stubborn response.expected, shall using R resolve vital conflict.(ref:rainbow)33\nFigure 5.3: (ref:rainbow)\n","code":""},{"path":"hypothesis-testing.html","id":"definitions-hypothesis","chapter":"Lesson 5 Hypothesis Testing","heading":"5.3.1 Definitions: Hypothesis","text":"involuntarily uttered hypothesis, testable assumption. \nwant test hypothesis using statistics. first hypothesis (“\nbetter player.”) call alternative hypothesis (\\(H_1\\)). \nname can bit confusing, often, actual scientific\nhypothesis, thing interested . , alternative ? \nalternative called null hypothesis (\\(H_0\\)), second\nstatement (“just luck”). null hypothesis provides sort baseline\nfindings. usually goes along lines “\nobservations just based chance alone?” “chance” can source\nrandom variation system.tricky part way directly test alternative\nHypothesis, can test null hypothesis. null\nhypothesis discard, always multiple alternative hypothesis \nexplain data. example, even end discarding idea \nfriend’s chess success luck, prove \nalternative hypothesis better player (still \ncheating example). keep mind transfer \nscientific setting. Just show something unlikely \narisen chance mean favorite alternative hypothesis \nautomatically true., words warning, let’s test null hypothesis!","code":""},{"path":"hypothesis-testing.html","id":"testing-the-null-hypothesis-with-a-simulation","chapter":"Lesson 5 Hypothesis Testing","heading":"5.3.2 Testing the Null Hypothesis with a Simulation","text":"start building little simulation. testing \nhypothesis, important defined \\(H_0\\) \\(H_1\\) properly, \nprevious section. need little specific. Winning \nchance entail completely random process, can model coin\nflip. R lovely function sample take number things \nvector, without replacement taking thing:giving number things draw just shuffles vector, \nfairly boring case just two tings. can’t sample 10 things \nvector two elementsBut can, put thing back every time:, let’s make little specific question:script look different every time run , won’t specific\noutcome . time, won:times, friend won:times. run script million times, resulting proportion\nrandom wins , close 50-50 \nused fair coin. However, don’t time play much Chess \nsure don’t money run million replicates experiment \nlab. , little simulated world, near infinite\nresources (simulation computationally costly).One trick used : calculate e.g. sum mean,\nR automatically converts TRUE 1 FALSE 0.created function returns random number wins friend \ngotten pure chance number rounds N.number different every time, change?histogram type plot shows often value occurs \nvector. Usually, values put bins first, grouping close values\ntogether continuous values, case makes sense just one\nvalue per bin dealing discrete values (e.g. half-wins).\nHistograms can either display raw counts frequency e.g. \npercentage. ggplot, use geom_bar don’t need binning, just\ncounting occurrences, geom_histogram need bin continuous\nvalues.expected, common number wins 8 4 (unless got really\nunlucky compiling script). Let us see, distribution\nchanges different values N. First, set grid numbers (\npossible combinations) can run bunch simulations:use trusty ggplot visualize distributions:fair coin, common number wins half number \ncoin flips. Note, still possible flip coin 15 times \nwin single time. just unlikely bars small \ncan’t see .Let us go back original debate. first statement: “better.” \nsomething can never definitively proven. always \npossibility, matter small, result arisen pure\nchance alone. Even wins 100 times don’t take single game \n, sort outcome still impossible appear just flipping \ncoin. can , calculate, likely certain event \nassumption null hypothesis (chance). can also decide \nthreshold \\(\\alpha\\) reject null hypothesis. called \nsignificance threshold. make observation calculate \nprobability observation like extreme smaller \nthreshold, deem result statistically significant. probability\nthus created called p-value.simulation, find probability win 7 8 rounds \nnull hypothesis :smaller commonly used significance threshold \\(\\alpha=0.05\\)\n(.e. \\(5\\%\\)). 7 8 wins, reject null hypothesis.\nnote threshold, matter commonly thoughtlessly used\nthroughout academic research, completely arbitrary.","code":"\ncoin <- c(\"heads\", \"tails\")\nsample(coin)## [1] \"tails\" \"heads\"\nsample(coin, size = 10)## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\nsample(coin, size = 10, replace = TRUE)##  [1] \"tails\" \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\" \"heads\"\n## [10] \"tails\"\nwinner <- c(\"you\", \"friend\")\nrandom_winners <- sample(winner, size = 8, replace = TRUE)\nrandom_winners## [1] \"friend\" \"you\"    \"friend\" \"friend\" \"friend\" \"friend\" \"friend\" \"friend\"\nsum(random_winners == \"you\")## [1] 1\nsum(random_winners == \"friend\")## [1] 7\nget_n_wins <- function(N) {\n  winner <- c(\"you\", \"friend\")\n  random_winners <- sample(winner, size = N, replace = TRUE)\n  sum(random_winners == \"friend\")\n}\nget_n_wins(10)## [1] 5\nset.seed(42) # setting a random seed for the script\n             # this give me the same random numbers\n             # every time\nnumber_of_wins <- map_dbl(rep(8, 1000), get_n_wins)\ntibble(number_of_wins) %>% \n  ggplot(aes(number_of_wins)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 0:8) +\n  labs(title = \"Throwing a coin 8 times\")\nsimulation <-\n  crossing(N   = 1:15,\n           rep = 1:1000) %>% \n  mutate(\n    wins = map_dbl(N, get_n_wins)\n  )\n\nhead(simulation) %>% rmarkdown::paged_table()\nsimulation %>% \n  ggplot(aes(wins)) +\n  geom_bar() +\n  facet_wrap(~ N) +\n  labs(title = \"Throwing a coin N times\")\nmean(number_of_wins >= 7)## [1] 0.025"},{"path":"hypothesis-testing.html","id":"getting-precise-with-the-binomial-distribution","chapter":"Lesson 5 Hypothesis Testing","heading":"5.3.3 Getting precise with the Binomial Distribution","text":"Now, just simulation 1000 trials, number can’t \narbitrarily precise, mathematical formula probability.\ncreated counting number successes series yes--trials\nbinomial distribution. common distributions, R provides \nset functions. functions starting d give us probability\ndensity function. case discrete values like counting wins, \nequivalent actual probability, continuous values obtain \nprobability taking integral. get integrals \ncorresponding functions starting p (probability).probability win exactly 7 8 games. wanted\nprobability 7 8! move integral. \npart can get bit confusing, default pbinom \nlower.tail = TRUE, according help page means probabilities\n\\(P[X \\le x]\\).set lower.tail FALSE , get \\(P[X > x]\\), probability \nrandom variable X bigger number x. get probability \ninterested , need replace 7 6 well:simulation pretty close! exact values agrees reject \nnull hypothesis opponents equally good.Sidenote: quick visualizations function curves sometimes use\nplotting functions base-R rather ggplot. powerful\nversatile, efficient small tasks. Just make sure note\nggplot, can’t use regular repertoire themes \nlikes .full graph probability density function binomial\ndistributionAnd integral, probability \\(P[X \\le x]\\)two function want showcase family. third \ncalled quantile function. Quantiles divide probability distribution\npieces equal probability. One example quantile 50th\npercentile, also known median, divides values half \nvalues half . can keep dividing two halves\nwell, end quantiles. Eventually, arrive \nquantile function. inverse probability function, obtain\nswapping axis.Quantiles also useful \ndeciding random sample follows certain distribution\nquantile-quantile plots.Lastly, always also r variant function, gives us \nnumber random numbers distribution.","code":"\ndbinom(x = 7, size = 8, prob = 0.5)## [1] 0.03125\npbinom(q = 7, size = 8, prob = 0.5)## [1] 0.9960938\npbinom(q = 6, size = 8, prob = 0.5, lower.tail = FALSE)## [1] 0.03515625\ncurve(dbinom(x = x, 8, prob = 0.5),\n      type = \"s\", from = 0,\n      to = 8,\n      n = 9, main = \"dbinom\")\ncurve(pbinom(q = x, 8, prob = 0.5),\n      type = \"s\",\n      from = 0,\n      to = 8,\n      n = 9, main = \"pbinom\")\ncurve(qbinom(p = x, size = 8, prob = 0.5),\n      type = \"s\",\n      from = 0,\n      to = 1,\n      n = 9, main = \"qbinom\"\n      )\nrbinom(n = 10, size = 8, prob = 0.5)##  [1] 4 5 5 5 5 2 3 4 6 3"},{"path":"hypothesis-testing.html","id":"but-how-much-better-understanding-effect-size-and-power-false-positives-and-false-negatives","chapter":"Lesson 5 Hypothesis Testing","heading":"5.3.4 But how much better? Understanding Effect Size and Power, False Positives and False Negatives","text":"decided abandon null hypothesis players equally good,\nequates 50% win-chance player. determined\nmuch better . much better need us \nreliably discard null hypothesis just 8 games? generalization \nmuch better part, true difference, called effect\nsize.ability decide something statistically significant \nfact true difference called statistical power. depends \neffect size, significance threshold \\(\\alpha\\) sample size \\(n\\) (\nnumber games). can explore concept another simulation.also introduced new piece advanced dplyr syntax. rowwise similar \ngroup_by essentially puts row group. can useful\nworking list columns running function varying arguments \nallows us treat inside mutate bit like using one \nmap functions. information, see documentation\narticle.leaves us 10000 simulated numbers wins N games different\ntrue probabilities winning (.e. much better friend ). \ncalculate probability greater number wins \nnull hypothesis (equal probability win loss), words: \np-value.notice couple things plot. number games\nplayed approaches high numbers, p-values \ncase null hypothesis fact true (players \nchance winning), start following uniform distribution,\nmeaning true null hypothesis, p-values equally likely.\nseems counterintuitive first, direct consequence\ndefinition p-value.\nconsequence , apply regular significance\nthreshold 5%, definition say true\ndifference, even though none (.e. null hypothesis true\nfalsely reject favor alternative hypothesis).\ncalled false positive. definition, get \nleast \\(\\alpha\\) false positives experiments.\nLater, learn, real number false positives \neven higher.\nAnother name false positives Type errors.side coin, also cases\ntrue difference (used winning probabilities\n0.8 0.9), don’t reject null hypothesis \nget p-values larger \\(alpha\\).\nfalse negatives rate sometimes\nreferred \\(\\beta\\).\nAnother name false negatives Type II errors.\nPeople don’t particularly like talking negative things like errors,\ninstead often see inverse \\(\\beta\\), \nStatistical Power \\(1-\\beta\\).\nproportion correctly identified positives \nactual positives also shown plot .\nexample, say true win probability 90%\nplay 8 games. experiment runs infinite\nnumber parallel universes, conclude \nbetter chance 80% .\nset significance threshold higher detect\ntrue positives, \nalso increase false positives.Unfortunately, free lunch statistics.also packages , function\ncompute power binomial test, think\nsimulation way approachable.\ncool thing simulations also, work\neven analytical solution, \ncan use play around planning experiment.","code":"\nset.seed(2020)\ntrials <- 10000\nsimulation <- crossing(\n  N = c(8, 100, 1000, 10000),\n  true_prob = c(0.5, 0.8, 0.9)\n) %>%\n  rowwise() %>%\n  mutate(\n    wins = list(rbinom(size = N,\n                       prob = true_prob,\n                       n = trials)),\n    p = list(pbinom(q = wins - 1,\n                    size = N,\n                    prob = 0.5, lower.tail = FALSE))\n    ) %>% \n  ungroup()\n\nsimulation## # A tibble: 12 x 4\n##        N true_prob wins           p             \n##    <dbl>     <dbl> <list>         <list>        \n##  1     8       0.5 <int [10,000]> <dbl [10,000]>\n##  2     8       0.8 <int [10,000]> <dbl [10,000]>\n##  3     8       0.9 <int [10,000]> <dbl [10,000]>\n##  4   100       0.5 <int [10,000]> <dbl [10,000]>\n##  5   100       0.8 <int [10,000]> <dbl [10,000]>\n##  6   100       0.9 <int [10,000]> <dbl [10,000]>\n##  7  1000       0.5 <int [10,000]> <dbl [10,000]>\n##  8  1000       0.8 <int [10,000]> <dbl [10,000]>\n##  9  1000       0.9 <int [10,000]> <dbl [10,000]>\n## 10 10000       0.5 <int [10,000]> <dbl [10,000]>\n## 11 10000       0.8 <int [10,000]> <dbl [10,000]>\n## 12 10000       0.9 <int [10,000]> <dbl [10,000]>\nsimulation %>%\n  unnest(c(wins, p)) %>% \n  ggplot(aes(p)) +\n  geom_histogram() +\n  facet_wrap(~ true_prob + N,\n             labeller = label_both,\n             scales = \"free_y\",\n             ncol = 4) +\n  geom_vline(xintercept = 0.05, color = \"red\") +\n  labs(x = \"p-value\",\n       y = \"frequency\") +\n  scale_y_continuous(breaks = NULL)\nsimulation %>%\n  unnest(c(wins, p)) %>%\n  group_by(true_prob, N) %>%\n  summarise(signif = mean(p <= 0.05)) %>% \n  ggplot(aes(true_prob, signif, fill = true_prob == 0.5)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(label = signif), vjust = -0.2) +\n  facet_wrap(~N,\n             labeller = label_both) +\n  scale_y_continuous(expand = expansion(c(0, 0.1))) +\n  scale_fill_viridis_d() +\n  labs(y = \"Proportion of significant results\")"},{"path":"hypothesis-testing.html","id":"p-value-pitfalls","chapter":"Lesson 5 Hypothesis Testing","heading":"5.4 P-Value Pitfalls","text":"Let us look pitfalls p-values.\nRemember definition p-values, get\nsignificant result even true difference \n5% cases (assuming use alpha)?\nWell, test bunch things?\ncalled Multiple Testing problem\nassociated :test 20 different things, statistical\ntest produce significant result chance alone\n5% cases, expected number significant results 1.\nsurprised.\nSpeaking surprised: book, available free online,\n“Statistics done wrong”, Alex Reinhart\ndescribes p-values “measure surprise”:»p value measure right ,\nsignificant difference ;\n’s measure surprised actual difference\ngroups, got data suggesting .\nbigger difference, one backed data,\nsuggests surprise smaller p value.«\n— Alex Reinhart34So, surprised, focus hard \none significant result, trouble ensues.\n“publish perish” mentality, can easily happen,\nnegative findings published nearly enough,\npublished findings likely exaggerated.\nJohn Bohannon showcased beautifully \nrunning study chocolate consumption \ngetting published:\nFooled Millions Thinking Chocolate Helps Weight Loss. ’s .can ?","code":""},{"path":"hypothesis-testing.html","id":"multiple-testing-correction","chapter":"Lesson 5 Hypothesis Testing","heading":"5.4.1 Multiple Testing Correction","text":"simplest approach take p-values calculate\nrunning large number comparisons \ndividing number tests performed.\ncalled Bonferroni correctionOf course, looses statistical power\n(remember, free lunch).\nslightly sophisticated approach \ncontrolling false discovery rate (FDR)\nBenjamini-Hochberg procedure. retains\nbit power. happens:Sort p-values ascending order.Choose FDR \\(q\\) willing accept\ncall number tests done \\(m\\).Find largest p-value :\n\\(p \\leq iq/m\\) index \\(\\).new threshold significanceScale p-values accordinglyAnd R:","code":"\np_values <- c(0.5, 0.05, 0.3, 0.001, 0.003)\np.adjust(p_values, method = \"bonferroni\")## [1] 1.000 0.250 1.000 0.005 0.015\np.adjust(p_values, method = \"BH\")## [1] 0.50000000 0.08333333 0.37500000 0.00500000 0.00750000"},{"path":"hypothesis-testing.html","id":"other-forms-of-p-hacking","chapter":"Lesson 5 Hypothesis Testing","heading":"5.4.2 Other forms of p-hacking","text":"sort multiple testing fairly obvious.\nnotice , end large\nnumber p-values, example genetic\nscreening testing thousands genes.\nrelated problems harder spot.\nsingle research question often different\nstatistical tests run, trying\nchoosing one best\nagrees hypothesis option!\nLikewise, simply looking data form \ncomparison influences choice statistical test.\nIdeally, first run exploratory experiments\nmeant test hypothesis, \ndecide tests need, sample size \nwant particular power run actual\nexperiments designed test hypothesis.point, another shout-\nAlex Reinharts book.35\npleasant read\nalso shines light \nforms p-hacking.","code":""},{"path":"hypothesis-testing.html","id":"bayesian-statistics-and-the-base-rate-fallacy","chapter":"Lesson 5 Hypothesis Testing","heading":"5.5 Bayesian Statistics and the Base Rate Fallacy","text":"another subtle problem called \nbase rate fallacy. example, assume\nmedical test, testing certain condition.\nmedical testing, different words used\nconcepts defined above36., :Sensitivity = Power = true positive rate = \\(1-\\beta\\)Specificity = true negative rate = \\(1-\\alpha\\)Let us assume test \nsensitivity 90% specificity\n92%. visit doctor \nget test, get positive result,\nprobability, \nfact positive (.e. true positive)?\nWell, test specificity 92%,\nnegative, detected\n92% cases, mean, can\n92% certain, actually positive?Well, . ignoring \nbase rate, diseases called\nprevalence. proportion \ndisease exists general population., let us say, picking 1000 people\nrandom population testing\n. dealing hypothetical\ncondition affects 1% people,\nassume 10 people sample positive.\n10 people, 9 tested positive\n(due sensitivity),\ntrue positives.\nremaining 1 false negative.\nHowever, course also testing\nnegatives (knew ahead time\npoint testing) \ndue specificity, 8% also \ntested positive, 0.08 * 990, \nget 79 false positives.\nmany negatives \nsample, even relatively high specificity\nproduce lot false positives.\nactual probability \npositive positive test result \\[\\frac{true~positives}{true~positives + false~positives}=10\\%\\]Formally, described Bayes’s Formula\\[P(|B)=\\frac{P(B|)*P()}{P(B)}\\]Read: probability given B probability\nB given times probability divided \nprobability B.bayesian statistics, prevalences known\npriors.","code":""},{"path":"hypothesis-testing.html","id":"concepts-discussed-today","chapter":"Lesson 5 Hypothesis Testing","heading":"5.6 Concepts discussed today","text":"today familiar following concepts:Null alternative hypothesisP-values statistical significanceBinomial distributionProbability density, probability quantile functionsEffect size statistical powerFalse positives, false negativesMultiple testing p-hackingBayes’s Theorem","code":""},{"path":"hypothesis-testing.html","id":"exercises-4","chapter":"Lesson 5 Hypothesis Testing","heading":"5.7 Exercises","text":"Just quite bit material take ,\ntry limit exercises today.","code":""},{"path":"hypothesis-testing.html","id":"discovering-a-new-distribution","chapter":"Lesson 5 Hypothesis Testing","heading":"5.8 Discovering a new Distribution","text":"binomial distribution concerned sampling\nreplacement (can get head tails number\ntimes without using coin). exercise\nexplore sampling without replacement.\ncommon model urn two\ndifferent colored balls .\nresulting distribution called \nhypergeometric distribution \ncorresponding R functions <r/d/p/q>hyper\nUnfortunately, couldn’t think game \nmodeled sampling without replacement,\nlet know, find one!Imagine zoo manager.\nGet mood watching cute\nanimal videos cup tee coffee.\n\ntwo red pandas fun snow.\ngot gift another zoo! consists\n8 red pandas 2 giant pandas.\nprobability end \nproperly separated, randomly take 8 animals,\nput one enclosure put rest another?\npenguin colony hatched eggs \nbunch newcomers. 15 males \n10 females. look random subset \n12 penguins, distribution \nnumber males look like? number \nlikely? likely , get least\n9 males sample?\nGet mood watching cute\nanimal videos cup tee coffee.\n\ntwo red pandas fun snow.got gift another zoo! consists\n8 red pandas 2 giant pandas.\nprobability end \nproperly separated, randomly take 8 animals,\nput one enclosure put rest another?penguin colony hatched eggs \nbunch newcomers. 15 males \n10 females. look random subset \n12 penguins, distribution \nnumber males look like? number \nlikely? likely , get least\n9 males sample?","code":""},{"path":"hypothesis-testing.html","id":"resources-3","chapter":"Lesson 5 Hypothesis Testing","heading":"5.9 Resources","text":"https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108https://jimgruman.netlify.app/post/education-r/P-Value histograms blogpost David Robinson“Statistics done wrong”","code":""},{"path":"distributions.html","id":"distributions","chapter":"Lesson 6 Distributions","heading":"Lesson 6 Distributions","text":"… explore continuous distributions spotify data,\nfind central limit theorem related statistical tests\nbecome N-dimensional whale sharks.Disclaimer: wrote lecture, scope gradually\nincreased always next thing wanted\ninclude. kind hard stop.\nexercises today unrestrictive, can\nexplore tempo time\nquestions Friday.","code":""},{"path":"distributions.html","id":"some-preparation","chapter":"Lesson 6 Distributions","heading":"6.1 Some Preparation","text":"Today, explore process modeling\nlook different types models.\npart, using tidymodels framework.\ntidymodels framework extends tidyverse\nspecialized tools kinds modeling tasks\nfit neatly tools\nalready know. Go ahead install :","code":"\nlibrary(tidyverse)\ninstall.packages(\"tidymodels\")"},{"path":"distributions.html","id":"sidenote-on-reproducible-environments-with-renv","chapter":"Lesson 6 Distributions","heading":"6.1.1 Sidenote on Reproducible Environments with renv","text":"point, installed quite lot packages.\none hand, great fun extend \ncan make tedious tasks fun.\nhand, every package add introduces\ncalled dependency.\nuser doesn’t package installed,\nanalysis run.\nfeeling experimental use functions\npackages active development might\nchange future, run trouble\nupdate package.\nnever updating anything ever fun!\nshow , get best worlds:\npackages functions heart desires\nmaintaining complete reproducibility.\nmake sure can come back old\nprojects 2 years now still just run\ntime.solution package called renv.\nidea follows:\nInstead installing packages\none place, can one version\npackage time, renv installs packages\nlocally project folder.\nalso meticulously writes version numbers\npackages installed keeps cache,\ncopy version twice.R package like , first,\ninstall :, RStudio project R console,\ninitialize project use renv :couple things.\ncreates file named .Rprofile, \nwrites source(\"renv/activate.R\").\nR-profile file run automatically every\ntime start R session folder,\nmakes sure renv active every time\nopen project.\nalso creates folder called renv.\ninstall packages\nwant use project.\nimportant file renv.lock file.\ncan look , just text file\npackages exact versions.notice, initializing renv,\npackages, example \ncan’t load tidyverse usual.\ninstall !\nHowever, case fairly\nfast, renv knows \nalready installed globally \nsimply copies files,\nfast.\ninstalled new package,\ncall:Renv tells us, changed environment\nconfirm, notes changes.also really easy collaborate \npeople. send \nproject folder, run:install packages noted lockfile.\ncan also use installed\nmany packages update \nregret want go back\nwritten lockfile.Finally, renv also provides functions update\ninstall new packages. work like install.packages,\nbit versatile.\nexample, let show different\nlocations can install packages.main location CRAN\n(Comprehensive R Archive Network).\nalso installed R .\nR packages subject certain standards\nusually stable tested.can also install packages directly source\ncode people uploaded.\nGitHub platform \ncan upload code track changes .\nlot times, can find current developement\nversion R package, packages \nyet CRAN GitHub.renv can install packages GitHub well,\nexample let us say, want test \nlatest version purrr package give feedback\ndevelopers.https://github.com/tidyverse/purrr says:Well, don’t need devtools , renv can\nregular install function:Giving just package name installs package CRAN,\npattern \"username/packgename\" installs GitHub.\nNow, back actual topic today!initialized renv need install\npackages need project even\nalready global package cache,\njust renv knows .","code":"\ninstall.packages(\"renv\")\nrenv::init()\nrenv::snapshot()\nrenv::restore()\n# ...\n# Or the the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"tidyverse/purrr\")\nrenv::install(\"tidyverse/purrr\")"},{"path":"distributions.html","id":"all-models-are-wrong-but-some-are-useful","chapter":"Lesson 6 Distributions","heading":"6.2 All models are wrong, but some are useful","text":"goes quote statistician George Box.»models wrong, useful«\n— George BoxWhat means model simplification\nreality must always omit details.\nmodel can depict complete underlying\nreality. However, models useful, \nunderstand useful , must first look \ndifferent types models .","code":""},{"path":"distributions.html","id":"types-of-models","chapter":"Lesson 6 Distributions","heading":"6.2.1 Types of Models","text":"tidymodels book\nnames three types models,\nparticular model can fall multiple\ncategories :Descriptive Models \npurely used describe underlying\ndata make patters easier see.\nadd smooth line ggplot\ngeom_smooth, default\nmethod called LOESS curve, \nstands Locally Estimated Scatterplot\nSmoothing. produce insights\nrevealing patterns us,\ncan used\ne.g. make predictions. \njust pretty looking smooth line.Inferential Models \ndesigned test hypothesis \nmake decisions. rely heavily \nassumptions data\n(e.g. probability distribution\npopulations follows) \nlikely encountered \nanswer research questions.\nmodels typically\nproduce p-value, compare\nthreshold like last weekInferential Models \ndesigned test hypothesis \nmake decisions. rely heavily \nassumptions data\n(e.g. probability distribution\npopulations follows) \nlikely encountered \nanswer research questions.\nmodels typically\nproduce p-value, compare\nthreshold like last weekPredictive Models \ndesigned process data \nmake predictions \nresponse variable upon receiving\nnew data. done correctly,\nalso hold data\nmodel never gets see,\ntime evaluate \ntest performs unseen data.\nDepending much know\n(want know) \nunderlying processes, differentiate\nmechanistic models like\nfitting physically meaningful\nfunction data empirically driven models, mainly\nconcerned creating good\npredictions, matter underlying\nmechanism.Predictive Models \ndesigned process data \nmake predictions \nresponse variable upon receiving\nnew data. done correctly,\nalso hold data\nmodel never gets see,\ntime evaluate \ntest performs unseen data.\nDepending much know\n(want know) \nunderlying processes, differentiate\nmechanistic models like\nfitting physically meaningful\nfunction data empirically driven models, mainly\nconcerned creating good\npredictions, matter underlying\nmechanism.now explore different examples.\nFirst, let introduce dataset today:","code":""},{"path":"distributions.html","id":"say-hello-to-spotify-data","chapter":"Lesson 6 Distributions","heading":"6.3 Say Hello to Spotify Data","text":"created playlist spotify,\nquite diverse can\nlook range features. can\neven listen \nexercises want. , write\n. cool thing spotify ,\nAPI, Application Interface. APIs ways \ncomputer programs talk .\nuse spotify app look songs, computers\nuse API talk spotify server.\nR rich ecosystem packages,\nsomeone already wrote package allows R talk \nAPI: spotifyr.check R folder lecture,\ncan see downloaded processed data \nplaylist. Note script work \nright away, first need register\nspotify developer get called token,\nlike username password one long text, allowed\nsend bots way.\nprobably just want download data \ngithub repository usual way.Let’s look, shall ?can get quick overview columns :Finally decent numbers!\njust measly discrete values\nlast week.\nsong playlist, get artist,\nyear arrived number features like\ndanceable, loud fast song .\ncan easily imagine spotify using \nnumbers suggest new songs based features\nlistened .\nfact, going lay foundations\nalgorithm today.","code":"\nlibrary(tidyverse)\nsongs <- read_csv(\"data/06/spotify_playlist.csv\")\nglimpse(songs, width = 60)## Rows: 347\n## Columns: 17\n## $ track_name        <chr> \"Africa\", \"Take on Me\", \"Wake Me…\n## $ track_artists     <chr> \"TOTO\", \"a-ha\", \"Wham!\", \"Elton …\n## $ danceability      <dbl> 0.671, 0.573, 0.620, 0.504, 0.37…\n## $ energy            <dbl> 0.373, 0.902, 0.573, 0.904, 0.89…\n## $ key               <dbl> 9, 6, 0, 6, 8, 10, 2, 4, 6, 4, 1…\n## $ loudness          <dbl> -18.064, -7.638, -11.893, -6.863…\n## $ mode              <dbl> 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,…\n## $ speechiness       <dbl> 0.0323, 0.0540, 0.0423, 0.1790, …\n## $ acousticness      <dbl> 0.257000, 0.018000, 0.271000, 0.…\n## $ instrumentalness  <dbl> 8.01e-05, 1.25e-03, 0.00e+00, 1.…\n## $ liveness          <dbl> 0.0481, 0.0928, 0.0607, 0.1400, …\n## $ valence           <dbl> 0.732, 0.876, 0.897, 0.772, 0.66…\n## $ tempo             <dbl> 92.718, 84.412, 81.548, 176.808,…\n## $ time_signature    <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n## $ track_duration_ms <dbl> 295893, 225280, 231333, 183440, …\n## $ track_popularity  <dbl> 83, 83, 79, 79, 59, 61, 71, 83, …\n## $ track_year        <dbl> 1982, 1985, 1984, 1983, 2020, 20…"},{"path":"distributions.html","id":"visualising-continuous-distributions","chapter":"Lesson 6 Distributions","heading":"6.4 Visualising Continuous Distributions","text":"dealing continuous distribution,\nlike lot features spotify\nsongs dataset, always multiple\nways represent data.\nFirst, just look numbers. \nuse valence values songs:Notice anything interesting numbers?\ndon’t either. brain way better suited\nlooking graphical representations,\n: ggplot cave!kind hard see, points overlap.\ncan get better picture distribution\nusing transparency bit jitter:Using histogram, can put points bins\nget plot similar got discrete values.\nNote plot flipped ’s side now.might want play around bin size \nget better feel distribution.\nAnother way apply smoothing function\nestimate density points along continuous\nrange, even places originally points:plots can misleading, original\nnumber points quite small, cases,\nbetter , showing actual individual\npoints well. reason, first plots\nvertical, cool way\nshowing points distribution,\nstill space show multiple\ndistributions next .\nImagine taking density plot, turning 90 degrees\nmirroring middle.\nget called violin plot.\noverlay points top, use something\nlittle predictable jitter time:\nggbeeswarm package present: geom_quasirandom.cool, now can easily\ncompare two different distributions\nnext still see individual\npoints.\nexample, might ask:“songs major cord higher valence \nsongs minor cord dataset?”Note: jittering works, \nfeature x-axis discrete. continuous,\nchanging data jittering x-axis.might also want add summaries like mean \ngroup plot additional marker.\nleads us general concept summary statistics.\nnumber , can quite useful\n, well, summarise complex distribution.\ncan also misleading, can simplification .","code":"\nhead(songs$valence)## [1] 0.732 0.876 0.897 0.772 0.668 0.324\nsongs %>% \n  ggplot(aes(x = factor(0), valence)) +\n  geom_point()\nsongs %>% \n  ggplot(aes(x = factor(0), valence)) +\n  geom_jitter(width = 0.05)\nsongs %>% \n  ggplot(aes(valence)) +\n  geom_histogram()\nsongs %>% \n  ggplot(aes(valence)) +\n  geom_density(fill = \"darkblue\", alpha = 0.3)\nsongs %>% \n  ggplot(aes(factor(0), valence)) +\n  geom_violin(fill = \"darkblue\", alpha = 0.3) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6)\nsongs %>% \n  ggplot(aes(factor(mode), valence)) +\n  geom_violin(fill = \"darkblue\", alpha = 0.3) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6) +\n  labs(x = \"Mode (Minor/Major)\")"},{"path":"distributions.html","id":"summary-statistics","chapter":"Lesson 6 Distributions","heading":"6.4.1 Summary Statistics…","text":"Let us start considering different things \ncan say distribution one number.\nFirst, might look range numbers,\nmaximum minimum.\nper mode, can compare values.appears valence can assume values 0 1.\nshortcut range function:Next, want know centers points.\ndifferent notions center\ndistribution.\nmean average sum values\ndivided number values.\nmedian call quantile, point \ndivides distribution equally sized parts,\nspecifically 50% values 50%\nmedian.median just one many percentiles\ncan think . display 50th \nwell 25th 75th percentile one plot,\nget called boxplot:“whiskers” box extend 1.5 times box size \nlast data point, whichever makes smaller whiskers.\nPoints extreme whiskers \nlabeled outliers boxplot usually displayed \npoints. Like violin plot,\nalso option plot original un-summarized\npoints top. case, need make sure\nchange outlier color boxplot \nNA, otherwise plotting twice:hints one downside boxplots:\nbox prominent focus point\nplot, definition,\ncontains 50% datapoints.\nrest delegated thin whiskers.Finally, want know, far values\nscatter around means potential\npopulation mean. encompassed\ntwo closely related measures: variance\nstandard deviation.illustrative purposes, can plot datapoints\ne.g valence order appear data\nadd line mean.variance expected value squared deviation\nrandom variable mean.words: Take distance points\nmean sum (add red lines plot\ntogether) divide \\(n-1\\).\\[var(X) = \\frac{\\sum_{=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]“Hang !” hear saying: “\\(n-1\\)?”\nexcellent question. first\nstatement talked expected value.\n(One example expected value mean,\nexpected value … well, values).\nindeed, expected value often \nterm \\(1/n\\). statement talking\nexpected value (squared deviation)\nwhole population.\ncan use uncorrected version \nwhole population (e.g. songs ever existed)\nwant talk population.\nusually, sample, \nwant draw conclusions population.\nusing sample estimate\nvariance population, biased.\ncan correct bias using \\(n-1\\) instead\n\\(n\\).\nknown Bessel’s correction.\nyet come really intuitive explanation,\none idea: thing dividing\nnecessarily sample size time\nwant try calculate expected\nvalue estimator, just happens \nsample size bunch cases.\nterm really represents \ndegrees freedom (DF) deviations.\nDFs can thought number independent things.\ndegrees freedom \\(n\\) reduced \\(1\\), \nknow mean sample (use \ncalculation), know \\(1\\)\nindividual values, last value automatically\nknown thus doesn’t count towards degrees freedom.Next : Standard Deviation (SD) square root\nvariance. commonly used error\nbars, square root inverts squaring \ndone get variance. back\ndimensions data.\\[\\sigma_X=\\sqrt{var(X)}\\]Finally, Standard Error Mean,\nsometimes called Standard Error (SEM, SE).\nalso used commonly error bars.\nreason lot people favor \nSD might just , smaller,\ndistinct use-cases.\\[SEM=\\sigma / \\sqrt{n}\\]take standard deviation divide \nsquare-root \\(n\\). Imagine :\nactually whole population available.\nLike example penguins earth.\nrepeatedly take samples \nsize \\(n\\). means individual samples\nvary, ’s mean,\nstandard deviation variance. \nstandard error standard deviation means.\nmeasure far means repeated samples\nscatter around true population mean.\nHowever, don’t usually whole population!\nMeasuring property penguins \nworld takes long time, running\nexperiment lab cells exist\never exist takes infinite amount \ntime. probably research grant\nmoney can finance.\n, instead, Standard Error Mean\nused standard deviation sample \nformula . best estimate\nstandard deviation whole population.\n, trying make inferences\nmean whole population based sample,\nmakes sense also give SEM way \nquantifying uncertainty.R functions sd, mean var,\nbuilt function sem,\ncan easily write one :","code":"\nsongs %>% \n  group_by(mode) %>% \n  summarise(\n    max = max(valence),\n    min = min(valence)\n  )## # A tibble: 2 x 3\n##    mode   max    min\n##   <dbl> <dbl>  <dbl>\n## 1     0 0.877 0.0271\n## 2     1 0.965 0.0176\nrange(songs$valence)## [1] 0.0176 0.9650\nsongs %>% \n  group_by(mode) %>% \n  summarise(\n    mean(valence),\n    median(valence)\n  )## # A tibble: 2 x 3\n##    mode `mean(valence)` `median(valence)`\n##   <dbl>           <dbl>             <dbl>\n## 1     0           0.399             0.395\n## 2     1           0.299             0.258\nsongs %>% \n  add_row(mode = 1, valence = 1.2) %>% \n  ggplot(aes(factor(mode), valence)) +\n  geom_boxplot(fill = \"darkblue\",\n               alpha = 0.3,\n               outlier.alpha = 1) +\n  annotate(geom = \"curve\", x = 1.8, y = 1.25, xend = 1.95, yend = 1.19,\n           curvature = .3, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(\"text\", x = 1.75, y = 1.25,\n           label = \"outlier\\n (that I added because there where none)\",\n           vjust = 0.5, hjust = 1) +\n  coord_cartesian(clip = \"off\")\nsongs %>% \n  add_row(mode = 1, valence = 1.2) %>% \n  ggplot(aes(factor(mode), valence)) +\n  geom_boxplot(fill = \"darkblue\", alpha = 0.3,\n               outlier.color =  NA) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6)\nsongs %>%\n  mutate(index = 1:n()) %>% \n  ggplot(aes(index, valence)) +\n  geom_segment(aes(y = mean(songs$valence),\n                   yend = mean(songs$valence),\n                   x = 0,\n                   xend = length(songs$valence))) +\n  geom_segment(aes(xend = index, yend = mean(songs$valence)),\n               color = \"darkred\", alpha = 0.6) +\n  annotate(\"text\", x = length(songs$valence) + 13, y = mean(songs$valence),\n           label = \"Mean\") +\n  geom_point()\nsem <- function(x) sd(x)/sqrt(length(x))"},{"path":"distributions.html","id":"or-how-to-lie-with-graphs","chapter":"Lesson 6 Distributions","heading":"6.4.2 … or: How to Lie with Graphs","text":"However, wary simple bar graphs error bars;\nlot can misleading .people say “y-axis include 0,”\nreason . always true,\nanother sensible baseline 0,\nespecially barplots y-axis start\n0 misleading thing can .\nmain reason humans perceive\nheight bars via area,\nlonger proportional bars\ndon’t start 0.\nplot also makes indication type \nerror-bars used sample size group.\nuses speechiness feature, hides \nactual distribution behind just 2 numbers\n(mean SEM) per group:next time see barplot ask question:37I hope can take inspiration \nchapter now vocabulary \nknow look comes data.","code":"\nsongs %>% \n  group_by(mode) %>%\n  summarise(across(speechiness, list(m = mean, sd = sd, sem = sem))) %>% \n  ggplot(aes(factor(mode), speechiness_m, fill = factor(mode))) +\n  geom_errorbar(aes(ymin = speechiness_m - speechiness_sem,\n                    ymax = speechiness_m + speechiness_sem,\n                    color = factor(mode)),\n                size = 1.3, width = 0.3, show.legend = FALSE) +\n  geom_col(size = 1.3, show.legend = FALSE) +\n  coord_cartesian(ylim = c(0.06, 0.08)) +\n  scale_fill_manual(values = c(\"#1f6293\", \"#323232\")) +\n  scale_color_manual(values = c(\"#1f6293\", \"#323232\")) +\n  labs(title = \"Don't Do This at Home!\",\n       y = \"Speechiness\",\n       x = \"Mode (Minor / Major)\") +\n  theme(\n    plot.title = element_text(size = 44, family = \"Daubmark\",\n                              color = \"darkred\")\n  )\nsongs %>% \n  ggplot(aes(speechiness, color = factor(mode),\n             fill = factor(mode))) +\n  geom_density(alpha = 0.3) "},{"path":"distributions.html","id":"graphic-devices-fonts-and-the-ggplot-book","chapter":"Lesson 6 Distributions","heading":"6.5 Graphic Devices, Fonts and the ggplot Book","text":"lot fun making graphs today’s session.\nNaturally, couple questions\ndone. two pointers\nwant give .","code":""},{"path":"distributions.html","id":"ggplot-book","chapter":"Lesson 6 Distributions","heading":"6.5.1 ggplot book","text":"Firstly, things ggplot, third edition\nggplot book currently worked \nthree absolute legends craft.38\nHadley Wickham author original ggplot \nggplot2 package, Danielle Navaro\nmakes amazing artwork\nteaches ggplot \nThomas Lin Pedersen\ncurrent maintainer ggplot2 constantly\nmakes cool features .\n-development book already available online\nfree: https://ggplot2-book.org/.","code":""},{"path":"distributions.html","id":"graphics-devics","chapter":"Lesson 6 Distributions","heading":"6.5.2 Graphics Devics","text":"Secondly, need briefly talk concept\nbrushed : graphics devices \nR printer computer.\ncreate plot R, starts mere numbers,\nsomething turn numbers \npixels (case raster-images) vectors\n(case vector images; might know svg pdf files.\nSorry, vectors R rather\ndescriptions lines).\njob ob graphics device.\nuse ggsave function example,\nfigures use based file extension,\ncan also specify manually.\nmentioning , \nplot just showed , used different font\ndefault. something can \nincredibly tricky graphics devices,\nfonts handled differently every operating\nsystem. Luckily, get way easier,\nThomas Lin Pedersen working another\npackage, graphics device, really\nfast works well fonts.\ncan check current development version :\nhttps://ragg.r-lib.org/","code":""},{"path":"distributions.html","id":"the-normal-distribution-and-the-central-limit-theorem","chapter":"Lesson 6 Distributions","heading":"6.6 The Normal Distribution and the Central Limit Theorem","text":"many different distributions .\nLuckily, one quite special can\nused multitude settings.\nharmlessly named Normal Distribution.\nR usual functions (density,\nprobability, quantile, random).Now, distribution special?Central Limit Theorem (CLT) states \nsample mean sufficiently large number\nindependent random variables approximately\nnormally distributed.\nlarger sample, better approximation.great visualization central limit\ntheorem, check interactive tutorial \nSeeing Theory.lot values measure actually \nsum many random processes, distributions things\nmeasure can often approximated normal distribution.can visually test values follow normal\ndistribution using quantile-quantile plot,\nplots quantiles sample \nquantiles normal distribution.\nstraight line means perfectly normal.values close mean pretty normal,\ntails distribution stray \nnormal distribution. way \nsmall large values \nexpected normal distribution.","code":"\ntibble(x = seq(-3, 3, 0.01)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = dnorm) +\n  stat_function(geom = \"area\", fun = dnorm,\n              fill = \"darkblue\", alpha = 0.3) +\n  labs(y = \"density\", title = \"Normal Distribution Density\")\n\ntibble(x = seq(-3, 3, 0.01)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = pnorm) +\n  labs(y = \"probability\", title = \"Cummulative Probability\")\nvalence <- songs %>% filter(mode == 1) %>% pull(valence)\nqqnorm(valence)\nqqline(valence)"},{"path":"distributions.html","id":"log-normality","chapter":"Lesson 6 Distributions","heading":"6.6.1 Log-normality","text":"one thing comes lot biological data:\nlot processes biology reliant \nsignal cascades, tend result many\nmultiplicative effects, rather additive effects,\nrequired Central Limit Theorem.\nresult, distributed normally,\nrather log-normally,\ntaking logarithm values\ntransforms multiplicative effects additive effects!","code":""},{"path":"distributions.html","id":"the-t-distribution","chapter":"Lesson 6 Distributions","heading":"6.7 The T-Distribution","text":"CLT valid large sample sizes.\nsmaller sample sizes, distribution \nmeans fatter tails normal distribution.\nstatistical tests,\nuse t-distribution instead \nnormal distribution.\ndegrees freedom get higher, \nt-distribution approaches normal distribution.\nFigure 6.1: t-distribution red, normal distribution black.\nRemember valence plot mode?demonstrative purposes going cheat little\npretend distributions approximatley normal\ncan look hypothesis tests:","code":"\nbase <- ggplot() + xlim(-5, 5)\n\nbase +\n  geom_function(aes(colour = \"normal\"), fun = dnorm, size = 1.2) +\n  geom_function(aes(colour = \"t, df = 1\"), fun = dt, args = list(df = 1), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 3\"), fun = dt, args = list(df = 3), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 30\"), fun = dt, args = list(df = 30), size = 1.2) +\n  guides(color = guide_legend(title = \"\"))\nsongs %>% \n  ggplot(aes(factor(mode), valence)) +\n  geom_violin(fill = \"darkblue\", alpha = 0.3) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6)"},{"path":"distributions.html","id":"students-t-test","chapter":"Lesson 6 Distributions","heading":"6.8 Student’s T-Test","text":"first test called student’s t-test. “Student”\npseudonym ’s inventor. “t” stands\nt-distribution. can use test\nnull hypothesis, two samples come \n(approximately normal) distributionWe receive p-value probability get difference\nmeans extreme extreme observed samples.\n, p-value small, difference\nlarge, lot values.Tests, rely assumption normality\ncalled parametric tests, \nassumption can met, need non-parametric tests.","code":"\nt.test(valence ~ mode, data = songs)## \n##  Welch Two Sample t-test\n## \n## data:  valence by mode\n## t = 3.9287, df = 328.06, p-value = 0.0001042\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  0.04999479 0.15027858\n## sample estimates:\n## mean in group 0 mean in group 1 \n##       0.3988897       0.2987530"},{"path":"distributions.html","id":"wilcoxon-rank-sum-test","chapter":"Lesson 6 Distributions","heading":"6.9 Wilcoxon rank-sum test","text":"Wilcoxon rank-sum test, \nMann–Whitney U test, one .\nget’s around assumption normality \ntransforming data ranks first.\n.e. points (independent group) \nordered values replaced \nposition ordering (rank).\nthink t-test testing \ndifference means, can think \nWilcoxon rank-sum test testing difference\nmedians.example, let us test difference \nspeechiness two modes (Minor/Major):","code":"\nwilcox.test(speechiness ~ mode, data = songs)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  speechiness by mode\n## W = 17918, p-value = 0.0003848\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"distributions.html","id":"direction-of-testing","chapter":"Lesson 6 Distributions","heading":"6.9.1 Direction of Testing","text":"tests argument alternative,\ncan c(\"two.sided\", \"less\", \"greater\").\ndirection alternative hypothesis.\ntesting, x greater less y?\ntesting difference direction (default)?\nhypothesis direction beforehand \nresult smaller p-values (half two-sided ones),\nneed hypothesis looking \ndata, especially running e.g. \ntwo sided test deciding, want \nsmaller p-value! p-values work.unsure tell functions,\ntwo groups supposed greater lesser,\ncan also supply data x y instead\nusing formula interface :save result test, can inspect object\nextract information :","code":"\nspeechiness_minor <- songs %>% filter(mode == 0) %>% pull(speechiness)\nspeechiness_major <- songs %>% filter(mode == 1) %>% pull(speechiness)\nwilcox.test(speechiness_minor, speechiness_major)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  speechiness_minor and speechiness_major\n## W = 17918, p-value = 0.0003848\n## alternative hypothesis: true location shift is not equal to 0\nw_test <- wilcox.test(speechiness_minor, speechiness_major)\nw_test$p.value## [1] 0.0003848449"},{"path":"distributions.html","id":"confidence-intervals","chapter":"Lesson 6 Distributions","heading":"6.9.2 Confidence Intervals","text":"t.test lonely sample can also used create confidence intervals\naround mean. short example 95% confidence\ninterval range expect mean\nsample fall 95% cases repeat\nexperiment infinite amount times.\nconfidence intervals also sometimes\nused error bars plots.Lastly today, going bit scope.\nleaving realm looking individual\nfeatures try condense information\nlittle space possible.","code":"\nvalence_minor <- songs %>% filter(mode == 0) %>% pull(valence)\ntest <- t.test(valence_minor)\ntest## \n##  One Sample t-test\n## \n## data:  valence_minor\n## t = 21.508, df = 144, p-value < 2.2e-16\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  0.3622315 0.4355478\n## sample estimates:\n## mean of x \n## 0.3988897\ntest$conf.int## [1] 0.3622315 0.4355478\n## attr(,\"conf.level\")\n## [1] 0.95"},{"path":"distributions.html","id":"chrunching-dimensions-with-dimensionality-reduction-pca","chapter":"Lesson 6 Distributions","heading":"6.10 Chrunching Dimensions with Dimensionality Reduction: PCA","text":"general notion Dimensionality Reduction \ntake features construct new\nfeatures , can represent data\nfewer features loosing little information.example, two features highly correlated\n.e. one changes ,\nmight better replacing single\nnew feature, goes along\naxis maximum variance two.\nnumber along line accounts \nvariance points, rest can\naccounted number describing distance\nline (perpendicular axis), \nless important first axis found.Imagine whale shark39And want orient mouth way\ncan eat greatest amount krill\none sweep40This first principal component. \nsecond perpendicular first.\nthrowback “Math Natural Scientists” liner\nalgebra, defining new coordinate system .whale sharks swim 3 dimensions, 2,\ndata even dimensions, one features\nrepresented one dimension.can quite hard humans imaging \nN-dimensional whale shark.R tidymodels us covered:PCA model , rather data preprocessing\nstep generates new features (principal components),\ncan later use models.\ntoday, just preprocessing .tidymodels, preprocessing done defining \nrecipe:take recipe prepare .prepared recipe, extract tidy form\nstep care (usually last one)\nsee, happened data.original features replace Principal Components\nexplain variance.\ncan see, features ended contributing\ncomponents:use 2 little helper functions \ntidytext package properly order bar.\nfirst component largely comprised high\nacousticness instrumentalness less energy positive direction.\nexpect e.g. classical music high axis.\nhigh value second component means high danceability\nlow tempo.can now explore, data looks like \nnew dimensions. , baking \nprepared recipe. set new_data NULL,\nwant use data already\nused prepare recipe (.e. calculate \nprincipal components).always, becomes clear plot:can now imagine, using simpler representation \nsongs principal component space, example\npropose new songs users based songs close \nsongs listened representation.Lastly, want stress, principal components \ncreated equal. first component always \nimportant.\n, see, almost 40% variance can explained\njust first component, exploring\n2 really makes little sense .","code":"\nsongs %>% \n  ggplot(aes(x = energy,\n             y = loudness,\n             label = track_name)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nlibrary(tidymodels)\nsongs_rec <- recipe( ~ ., data = songs) %>% \n  update_role(track_name, track_artists, new_role = \"id\") %>% \n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\nsongs_prep <- prep(songs_rec)\nsongs_prep## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         id          2\n##  predictor         15\n## \n## Training data contained 347 data points and no missing data.\n## \n## Operations:\n## \n## Centering and scaling for danceability, energy, key, loudness, ... [trained]\n## PCA extraction with danceability, energy, key, loudness, ... [trained]\nsongs_compontents <- tidy(songs_prep, 2)\nsongs_compontents## # A tibble: 225 x 4\n##    terms              value component id       \n##    <chr>              <dbl> <chr>     <chr>    \n##  1 danceability     -0.300  PC1       pca_hXKms\n##  2 energy           -0.376  PC1       pca_hXKms\n##  3 key              -0.0498 PC1       pca_hXKms\n##  4 loudness         -0.373  PC1       pca_hXKms\n##  5 mode              0.153  PC1       pca_hXKms\n##  6 speechiness      -0.146  PC1       pca_hXKms\n##  7 acousticness      0.364  PC1       pca_hXKms\n##  8 instrumentalness  0.359  PC1       pca_hXKms\n##  9 liveness         -0.123  PC1       pca_hXKms\n## 10 valence          -0.318  PC1       pca_hXKms\n## # … with 215 more rows\nsongs_compontents %>% \n  mutate(terms = tidytext::reorder_within(terms, by = value, within = component)) %>% \n  filter(component %in% paste0(\"PC\", 1:3)) %>% \n  ggplot(aes(value, terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free\") +\n  ggthemes::scale_fill_colorblind() +\n  guides(fill = \"none\") +\n  tidytext::scale_y_reordered()\nsongs_baked <- bake(songs_prep, new_data = NULL)\nsongs_baked %>% rmarkdown::paged_table()\nplt <- songs_baked %>% \n  mutate(song = paste(track_name, \",\", track_artists)) %>% \n  ggplot(aes(PC1, PC2, label = song)) +\n  geom_hline(yintercept = 0, alpha = 0.3) +\n  geom_vline(xintercept = 0, alpha = 0.3) +\n  geom_point()\n\nplotly::ggplotly(plt)\nsdev <- songs_prep$steps[[2]]$res$sdev\npercent_variation <- sdev^2 / sum(sdev^2)\n\ntibble(component = unique(songs_compontents$component),\n       percent_var = percent_variation) %>%\n  mutate(component = fct_inorder(component)) %>%\n  ggplot(aes(component, percent_var)) +\n  geom_col() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x = NULL, y = \"Percent variance explained by each PCA component\")"},{"path":"distributions.html","id":"exercises-5","chapter":"Lesson 6 Distributions","heading":"6.11 Exercises","text":"tidytuesday project also spotify\ndataset. one es even interesting,\nranges across different playlists\nvarious genres annotated said genres.\ndata (30000 songs)!\nDownload :https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md","code":""},{"path":"distributions.html","id":"the-plotty-horror-picture-show","chapter":"Lesson 6 Distributions","heading":"6.12 The Plotty Horror Picture Show","text":"Sometimes experience true horror\nsee light darkness. Take spotify data\nmake plot truly horrible!\nappreciate couple sentences thought process\nmakes plot particularly bad.\ncan strike terror reader’s heart multiple\nways. ideas, mix match suits :Make really ugly experimenting different theme options.Make really misleading defying viewer expectations\nbreaking norms. artist now,\nnorms don’t apply art.Try different geoms combinations aesthetics,\nmaybe find ones worst possible choice\nfeatures.","code":""},{"path":"distributions.html","id":"take-a-sad-plot-and-make-it-better","chapter":"Lesson 6 Distributions","heading":"6.13 Take a Sad Plot and Make it Better","text":"title exercise stolen \ntalk\nAlison Hill.Now use learned make great plot!\nPick features interested \nvisualize informative beautiful\npossible, still staying honest data.\nMaybe interested changes time,\nmaybe find favorite artist want situate\ncontext works.\nMaybe want explore different features relate\neven want attempt \nrecreate PCA see, can find clusters\ngenres. call.curious see, come !","code":""},{"path":"distributions.html","id":"stats-time","chapter":"Lesson 6 Distributions","heading":"6.14 Stats Time","text":"exercise statistics , please think \ntopics today’s lecture note questions\ncome , can talk Friday.","code":""},{"path":"distributions.html","id":"resources-4","chapter":"Lesson 6 Distributions","heading":"6.15 Resources","text":"Tidymodels websiteTidymodels bookggplot bookragg graphics device","code":""},{"path":"fallacies.html","id":"fallacies","chapter":"Lesson 7 Fallacies","heading":"Lesson 7 Fallacies","text":"… hear Stories Warplanes,\nCorrelation Regression explore Datasaurus Dozen.","code":"\nlibrary(tidyverse)"},{"path":"fallacies.html","id":"data-considerations","chapter":"Lesson 7 Fallacies","heading":"7.1 Data Considerations","text":"","code":""},{"path":"fallacies.html","id":"section","chapter":"Lesson 7 Fallacies","heading":"7.1.1 1943","text":"1943. second World War well underway, ravaging large parts Europe.\nMilitary aircraft first entered stage World War now\nreaching peak importance rain fire skies. Allied\nforces facing problem. warplanes get better, anti-aircraft\nsystems. effort improve survival fleet, US military\nstarts examining planes returning skirmishes opposing forces.\ncharacterize pattern bullet holes metal hull, meticulously\nnoting hit plane sustained. resulting picture better\nsummarized modern, redrawn version figure 7.1. \nlook just second, , let preface one \nstressing reach find.effect question invisible hand laboratory results \ncreate, advice people give reason don’t get rich \nreading books people telling get rich. pervasive\nmechanism powerful people often don’t want admit exists.\nFigure 7.1: Figure Wikipedia.41\ntaking look data gathered, military ready rush\naction. improve endurance aircraft, plan \nreinforce parts plane often hit bullets. \nstronger wings sturdier body plane, think, surely pilots\ncome back missions safely. wrong.","code":""},{"path":"fallacies.html","id":"the-story-of-abraham-wald","chapter":"Lesson 7 Fallacies","heading":"7.1.2 The Story of Abraham Wald","text":"pilots luck. military also consulted Statistics\nResearch Group Columbia University. man named Abraham Wald worked . \nnow unclassified report “method estimating plane vulnerability based \ndamage survivors,” argued generals.42 Instead -hit parts planes,\nleast-hit parts reinforced.Cover “method estimating plane vulnerability based damage \nsurvivors”43Instead -hit parts, least-hit parts reinforced.reason seemingly counterintuitive result now known \nsurvivorship bias. data collected contained survivors, \nplanes sustained damage severe enough hinder coming back\nmission. aircraft hit places simply didn’t\nmake back. Consequently, Wald advised reinforce engines fuel\ntanks.","code":""},{"path":"fallacies.html","id":"thinking-further","chapter":"Lesson 7 Fallacies","heading":"7.1.3 Thinking further","text":"one multitude biases, specifically selection bias, \ninfluence quality inferences can draw available data.\nKeep mind, data objective never exists vacuum. \nalways context consider. way data collected just one .\nlot ideas seem obvious hindsight, incidentally another\nbias social psychologists call hindsight bias, can sometimes \nhard spot.common saying music better back days, old\nmusic still holds new stuff radio just sounds .\nWell, quite. also survivorship bias work. bad \nforgettable songs past just faded oblivion, never mentioned\n, songs people generally agreed good survived ravages\ntime unscathed. similar happens success general, just songs. \nask CEO high corporate ladder, millionaire author \nbook reads “get rich,” sure witty anecdote \npersistence brilliance charisma got \nnow. seeing people just witty, just charismatic\neven just persistent simply lucky. people \ntell . takes whole lot courage admit ones\nsuccess based luck privilege.take back scientific context: planning \nexperiment lab, always ask whether data collection process can \nway biased towards trying show.leave :weird every time see image twitter ton retweets pic.twitter.com/VALAKdehePAnd cautionary tale jump straight back RStudio.","code":""},{"path":"fallacies.html","id":"miscelaneous","chapter":"Lesson 7 Fallacies","heading":"7.2 Miscelaneous","text":"ease back R programming, let’s look small helpful things\nmight otherwise missed sidelines.","code":""},{"path":"fallacies.html","id":"glue","chapter":"Lesson 7 Fallacies","heading":"7.2.1 Glue","text":"First glue package. Using paste create text \nvalues variables inserted can painful. glue package makes \nbreeze. Everything inside curly braces text inside glue\nfunction evaluated regular R code, enabling us write text quite\nnaturally:hope confused package ’s main function \nname.Another thing want briefly mention already hinted , talked\ngraphics devices.","code":"\nlibrary(glue)\nglue(\"1 + 1 is {1 + 1}\")## 1 + 1 is 2\nvalue <- 0.04\nglue(\"With a p-value of {value}!\")## With a p-value of 0.04!"},{"path":"fallacies.html","id":"svgs","chapter":"Lesson 7 Fallacies","heading":"7.2.2 SVGs","text":"Generally, devices produce pixel graphics \ninformation stored color values pixels.\ncalled raster devices. ragg, showed \nlast week, provides . zoom raster images,\ncan see texts shapes getting pixely blurry. Another way store\ninformation graphs write mathematical description lines \nshapes plot, interpreted program \nopen vector graphic. Let show popular one called\nScalable Vector Graphics (svg).get Error loadNamespace(name) : package called ‘svglite’\ninstall mentioned package first.can now open svg file e.g. browser preview . image\ndisplay software also preview . Notice, can zoom without\nloosing clarity. However, points shapes plot , \ncomputer work preview file gets bigger \ncomparable pixel graphics like png.showing svgs specifically, takes time master \nintricacies ggplot, example create annotation. don’t want \nleave impression everything done pure R. Sure, \nreproducibility standpoint, able replicate plots even\ncomplete research report via Rmarkdown press button great \nultimate goal. one ever started expert. \nconnecting tools already know important step., example, open svg file vector editor\ninkscape, also open source software, add different title \nannotate points.word Warning: , nothing preventing\nalso modify, move scale actual datapoints important\nparts vital convey correct information. extra careful \n.","code":"\nexample <- ggplot(mtcars, aes(disp, mpg)) +\n  geom_point() +\n  labs(title = \"Placeholder Title\")\n\nggsave(\"results/plots/example.svg\", example)"},{"path":"fallacies.html","id":"best-practices","chapter":"Lesson 7 Fallacies","heading":"7.2.3 Best Practices","text":"Speaking careful. one rule can give \nmake data analysis secure:raw data sacred! ever modify \nsave .even important ,\nexample, using excel preview csv file.\ncircumstances hit save button\nexcel looking raw data.\napproximately one-fifth genomic research papers containing\nerrors gene lists, excel converted genes\nSEPT2 (Septin 2) dates, can see .44\nBiologists since given renamed genes \ncommonly converted dates… point still stands.\ncaution course also necessary analyzing data\nR, just excel. read raw data \nsave processed version, create new file, even\nbetter, new folder . good convention example\ndivide data raw derived folder.","code":""},{"path":"fallacies.html","id":"covariance-correlation-and-regression","chapter":"Lesson 7 Fallacies","heading":"7.3 Covariance, Correlation and Regression","text":"Source: https://xkcd.com/552/Last week, talked measure spread \nrandom variable called variance.\\[var(X) = \\frac{\\sum_{=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]Today, extending idea 2 random variables.\nnormal distribution common, using\ntwo normally distributed variables.\nfun , different means \nstandard deviations (remember: SD square-root variance).also added lines \nmeans two random\nvariables. Maybe mentioned\nclearly earlier ,\ngeneral convention statistics random variables\nuppercase concrete values distribution \nletter lowercase.now get covariance X Y :\\[cov(X,Y)=\\text{E}\\left[(X-\\text{E}\\left[X\\right])(Y-\\text{E}\\left[Y\\right])\\right]\\]expected value \\(E[X]\\) just fancy way saying\nmean X.\nasses contribution individual points towards \ncovariance, can understand quite intuitively.\npoint higher x mean X higher\ny mean Y (top right quadrant) push covariance towards\npositive values. Likewise, point bottom left quadrant\nnegative differences X Y mean, cancel\nresult positive covariance.\nbottom right top left quadrants push towards negative\ncovariance. mix positive negative contributions \nresult covariance small absolute value.covariance one problem: weird units\n(X times Y) scale different depending random\nvariables.\nstandardize dividing standard\ndeviations get correlation coefficient:\\[corr(X,Y)=\\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}\\]can assume values -1 1. ’s full name \nPearson product-moment correlation coefficient, \npearsons R. can square get \\(R^2\\) (obviously),\nindicates strength correlation \nvalues 0 1 independent direction.\nmeet later.Let us apply knowledge new dataset.","code":"\nN <- 50\ndf <- tibble(\n  x = rnorm(N, 1, 0.8),\n  y = rnorm(N, 3, 1.2)\n)\n\nm_x <- mean(df$x)\nm_y <- mean(df$y)\n\nggplot(df, aes(x, y)) +\n  geom_vline(xintercept = m_x, alpha = 0.8, color = \"darkviolet\") +\n  geom_hline(yintercept = m_y, alpha = 0.8, color = \"darkviolet\") +\n  geom_point() "},{"path":"fallacies.html","id":"introducing-the-dataset","chapter":"Lesson 7 Fallacies","heading":"7.3.1 Introducing the Dataset","text":"dplyr package includes example dataset Star Wars\ncharacters. Unfortunately, created ago,\nbaby yoda, 87 characters present.guess baby yoda show now.Let’s look correlations:","code":"\nstarwars"},{"path":"fallacies.html","id":"pearson-vs.-spearman-not-a-boxing-match","chapter":"Lesson 7 Fallacies","heading":"7.3.2 Pearson vs. Spearman (not a Boxing Match)","text":"compute pearsons correlation, use cor function R.\nInstead filtering NA values beforehand,\nresult correlation NA, can use\nuse = \"complete.obs\" ignore NA computation.another way can specify features correlate.\ncorr also takes matrix data frame ’s x argument instead\nx y:known correlation matrix, can create \ntwo features, long features numeric\n(, correlation 1,4 “cat” “dog?”).\nneed analysis, handy know\nconvert matrix tibble:Looses information rownames, tibbles\nrownames. However, can tell as_tibble create\nnew column previously rownames.Now even turn tidy format make heatmap:working log correlations, certainly\nworth checking corrr package tidymodels framework:\nhttps://corrr.tidymodels.org/Apart cor, also cor.test, gives information.fancy, can use broom turn test output \ntidy format well:first surprised correlation \nlow. talking height mass, \nassumed highly correlated.\nLet us look data see going .culprit! massive outlier,\nsenses word “massive.” point\nfair say, Jabba Hutt use workout\nloose weight.\nLuckily, another method asses correlation.\nSpearman’s method resistant outliers,\ndata transformed ranks first,\nnegates massive effect outliers.\nVisually, points look like\nrank transformation:","code":"\npearson <- cor(starwars$mass, starwars$height, use = \"complete.obs\")\npearson## [1] 0.1338842\ncorr_matrix <- cor(starwars[c(\"mass\", \"height\")], use = \"complete.obs\")\ncorr_matrix##             mass    height\n## mass   1.0000000 0.1338842\n## height 0.1338842 1.0000000\nas_tibble(corr_matrix)## # A tibble: 2 x 2\n##    mass height\n##   <dbl>  <dbl>\n## 1 1      0.134\n## 2 0.134  1\nas_tibble(corr_matrix, rownames = \"feature1\")## # A tibble: 2 x 3\n##   feature1  mass height\n##   <chr>    <dbl>  <dbl>\n## 1 mass     1      0.134\n## 2 height   0.134  1\nas_tibble(corr_matrix, rownames = \"feature1\") %>% \n  pivot_longer(-feature1, names_to = \"feature2\", values_to = \"corr\") %>% \n  ggplot(aes(feature1, feature2, fill = corr, label = corr)) +\n  geom_raster() +\n  geom_text(color = \"white\")\ncor_test <- cor.test(starwars$mass, starwars$height, use = \"complete.obs\")\ncor_test## \n##  Pearson's product-moment correlation\n## \n## data:  starwars$mass and starwars$height\n## t = 1.02, df = 57, p-value = 0.312\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.1265364  0.3770395\n## sample estimates:\n##       cor \n## 0.1338842\nbroom::tidy(cor_test)\nlabel_text <- glue(\"Pearson correlation: {round(pearson, 2)}\")\n\njabba <- filter(starwars, str_detect(name, \"Jabba\"))\njabba_text <- list(x = 1100, y = 120)\n\nstarwars %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 500, y = 75, label = label_text,\n           hjust = 0) +\n  annotate(geom = \"curve\",\n           x = jabba_text$x, y = jabba_text$y,\n           xend = jabba$mass, yend = jabba$height,\n           curvature = .3,\n           arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\",\n           x = jabba_text$x,\n           y = jabba_text$y, label = \"Jabba the Hutt\",\n           hjust = 1.1) +\n  xlim(0, 1500) +\n  labs(x = \"mass [kg]\",\n       y = \"height [cm]\")\nspearman <- cor(starwars$mass, starwars$height,\n                method = \"spearman\",\n                use = \"complete.obs\")\n\nlabel_text <- glue(\"Spearman rank correlation: {round(spearman, 2)}\")\n\nstarwars %>% \n  mutate(mass = rank(mass),\n         height = rank(height)) %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 0, y = 75, label = label_text,\n           hjust = 0) +\n  labs(x = \"rank(mass)\",\n       y = \"rank(height)\")"},{"path":"fallacies.html","id":"linear-regression","chapter":"Lesson 7 Fallacies","heading":"7.3.3 Linear Regression","text":"Finally, linear regression \nrelated concept, correlation \nlinear regression quantify strength linear\nrelationship. However, key differences.\nfit linear model like y ~ + b * x,\nerror x. assume x something \nfixed, like temperature set experiment\ndosage used. Y hand random\nvariable. cov(X,Y) cor(X,Y), X Y random variables,\nusually things observed, set .correlation coefficient symmetrical translation-scale-invariant,\nmeaning \\(corr(X,Y)=corr(Y,X)\\) \\(corr(X,Y)=corr(X * +b,Y * c + d)\\),\nlinear models !data folder find IMDB ratings 10\nStar Wars movies (plus features).can fit linear model see production year\neffect rating.added gray segments called residuals.\nmakes linear regression work.\n’s full name Ordinary Least Squares squares \nquestion squares residuals, word least\nindicates squares minimized order find \nbest fit line.yet encounter Extraordinary Least Squares\nsure someone machine learning soon\nneed words fuel hype become thing.Looks like every year decreases estimated rating 0.03.One thing however correlation \nlinear regression, \\(R^2\\) value get\ncalculations:can interpret \\(R^2\\) fraction variance \nresponse variable y can explained \npredictor x.","code":"\nstarwars_movies <- read_rds(\"data/07/starwars_movies.rds\")\nstarwars_movies## # A tibble: 10 x 25\n##    Title  Rated Released   Runtime Genre  Director  Writer Actors Plot  Language\n##    <chr>  <chr> <date>     <chr>   <chr>  <chr>     <chr>  <chr>  <chr> <chr>   \n##  1 Star … PG    1977-05-25 121 min Actio… George L… Georg… Mark … Luke… English \n##  2 Star … PG    1980-06-20 124 min Actio… Irvin Ke… Leigh… Mark … Afte… English \n##  3 Star … PG    1983-05-25 131 min Actio… Richard … Lawre… Mark … Afte… English \n##  4 Star … PG-13 2015-12-18 138 min Actio… J.J. Abr… Lawre… Harri… As a… English \n##  5 Star … PG    1999-05-19 136 min Actio… George L… Georg… Liam … Two … English \n##  6 Star … PG-13 2005-05-19 140 min Actio… George L… Georg… Ewan … Thre… English \n##  7 Star … PG    2002-05-16 142 min Actio… George L… Georg… Ewan … Ten … English \n##  8 Star … PG-13 2017-12-15 152 min Actio… Rian Joh… Rian … Mark … Rey … English \n##  9 Rogue… PG-13 2016-12-16 133 min Actio… Gareth E… Chris… Felic… The … English \n## 10 Star … PG-13 2019-12-20 141 min Actio… J.J. Abr… Chris… Carri… The … English \n## # … with 15 more variables: Country <chr>, Awards <chr>, Poster <chr>,\n## #   Ratings <list>, Metascore <chr>, imdbRating <dbl>, imdbVotes <dbl>,\n## #   imdbID <chr>, Type <chr>, DVD <date>, BoxOffice <chr>, Production <chr>,\n## #   Website <chr>, Response <chr>, year <dbl>\nmodel <- lm(imdbRating ~ year, data = starwars_movies)\n\nbroom::augment(model) %>%\n  ggplot(aes(year, imdbRating)) +\n  geom_smooth(method = \"lm\", alpha = 0.3, color = \"darkviolet\") +\n  geom_point() +\n  geom_segment(aes(x = year, y = .fitted,\n                   xend = year, yend = imdbRating),\n               alpha = 0.4)\nbroom::tidy(model)## # A tibble: 2 x 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)  74.7      28.9         2.59  0.0322\n## 2 year         -0.0335    0.0144     -2.33  0.0484\nsummary(model)## \n## Call:\n## lm(formula = imdbRating ~ year, data = starwars_movies)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1171 -0.2631  0.1152  0.3955  0.8195 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 74.65951   28.85148   2.588   0.0322 *\n## year        -0.03354    0.01442  -2.326   0.0484 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7006 on 8 degrees of freedom\n## Multiple R-squared:  0.4035, Adjusted R-squared:  0.329 \n## F-statistic: 5.412 on 1 and 8 DF,  p-value: 0.04843"},{"path":"fallacies.html","id":"non-linear-least-squares","chapter":"Lesson 7 Fallacies","heading":"7.4 Non-linear Least Squares","text":"far, properly dealt linear relationships\nnow time get non-linear.\ncreating mechanistically driven\npredictive model, formula \nwant adjust parameters fits data.Let’s take classical Michaelis-Menten-Kinetics\ndataset enzyme reaction rates included R.\nconvert \ndataframe tibble prints\nnicer:initial rate \\(v_0\\) \nenzymatic reaction measured\ncontrol sample treated\npuromycin different substrate\nconcentrations.\nevery concentration \ntwo replicates except \none missing replicate.\ncan make explicit \ncan keep track replicates:Now can plot individual curves.Biochemistry studies, know\ncan express rate depending\nconcentration following\nformula:\\[rate=\\frac{(Vm * conc)}{(K + conc)}\\]make easier work , let’s\nturn function:Let’s pick arbitrary starting values.\nexample, see maximal velocity\naround 200.\nalso know K concentration half-maximal\nvelocity reached.geom_function expects function x anonymous function\nfirst argument values x-axis,\n.\nWell, bet can better guessing function!\nR can us linear least squares\nminimizing distance curve \ndatapoints.\njob nls function, stands \nNonlinear Least Squares.NlS needs starting values, use guess isn’t far .\ncompletely wrong, model doesn’t know direction \nmove parameters improve fit get error like :\nError nls(rate ~ rate(conc, Vm, K), data = puro, subset = state ==  : singular gradientFor special case, R also self-starting model. won’t go\nuseful general concept fitting\narbitry functions, can check SSmicmen model \nestimes starting values automatically.Additionally, nls takes argument subset, works\njust like dplyr verb filter can fit\nmodel subset data without create beforehand.now multiple ways displaying model.\nbroom::augments can add predictions \noriginal data (just treated state):obvious disadvantage adding\npoint (nick line) previously\ndatapoints. parameters complete\nfunction, can calculate arbitrary amount values\nmake smooth function.pull estimates K Vm broom \nuse function.\n, make use predict function.\ntakes model new data generates predicions.\ndon’t give new data, just get predictions \ndata used fit model.can also use combination geom_function,\nthink feel pretty natural comfortable\nwriting functions fly.Now, want fit model states?\ncan resort back trusty purrr package like\nearlier lecture.Now can inspect estimated parameters ease:check models performed. Unfortunately, can’t\nget performance metrics time short.Now, use estimated parameters \nmodels predict plot (well, ) model\nfit lines original data, easier way.\ngeom_smooth can take “nls” method well, \njust need make sure pass correct arguments.\ncan confusing, specifying\nformula geom_smooth, always needs \nformula y ~ x, whereas normal nls \nearlier, specified variables terms \nactual names (rate conc).also need se = FALSE, default R \ntry plot confidence interval around fit-line\nlike linear model, nls doesn’t return one,\nget error.unfortunate thing method end \nfitting model twice, get estimated parameters\nlikes second time ggplot\ndisplay fitted lines. cases \nproblem, model computationally expensive.","code":"\nas_tibble(Puromycin)## # A tibble: 23 x 3\n##     conc  rate state  \n##    <dbl> <dbl> <fct>  \n##  1  0.02    76 treated\n##  2  0.02    47 treated\n##  3  0.06    97 treated\n##  4  0.06   107 treated\n##  5  0.11   123 treated\n##  6  0.11   139 treated\n##  7  0.22   159 treated\n##  8  0.22   152 treated\n##  9  0.56   191 treated\n## 10  0.56   201 treated\n## # … with 13 more rows\npuro <- as_tibble(Puromycin) %>% \n  group_by(conc, state) %>% \n  mutate(rep = 1:n()) %>% \n  ungroup()\npuro %>% \n  ggplot(aes(conc, rate,\n             group = paste(rep, state),\n             color = state)) +\n  geom_line() +\n  geom_point()\nrate <- function(conc, Vm, K) {\n  (Vm * conc) / (K + conc)\n}\npuro %>% \n  ggplot(aes(conc, rate,\n             color = state)) +\n  geom_line(aes(group = paste(rep, state))) +\n  geom_point() +\n  geom_function(fun = ~ rate(conc = .x, Vm = 200, K = 0.2),\n                color = \"black\")\nmodel <- nls(rate ~ rate(conc, Vm, K),\n    data = puro,\n    subset = state == \"treated\",\n    start = list(K = 0.1, Vm = 200))\n\nmodel## Nonlinear regression model\n##   model: rate ~ rate(conc, Vm, K)\n##    data: puro\n##         K        Vm \n##   0.06412 212.68363 \n##  residual sum-of-squares: 1195\n## \n## Number of iterations to convergence: 6 \n## Achieved convergence tolerance: 6.108e-06\nbroom::augment(model) %>% \n  ggplot(aes(conc, rate)) +\n  geom_point() + \n  geom_line(aes(y = .fitted))\nbroom::tidy(model)## # A tibble: 2 x 5\n##   term  estimate std.error statistic  p.value\n##   <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n## 1 K       0.0641   0.00828      7.74 1.57e- 5\n## 2 Vm    213.       6.95        30.6  3.24e-11\npredict(model)##  [1]  50.56606  50.56606 102.81102 102.81102 134.36165 134.36165 164.68470\n##  [8] 164.68470 190.83289 190.83289 200.96878 200.96878\npredictions <- tibble(\n  conc = seq(0, 1, by = 0.01),\n  rate = predict(model, newdata = list(conc =  conc))\n)\n\npuro %>% \n  filter(state == \"treated\") %>% \n  ggplot(aes(conc, rate)) +\n  geom_point() +\n  geom_line(data = predictions, color = \"darkviolet\")\npuro %>% \n  filter(state == \"treated\") %>% \n  ggplot(aes(conc, rate)) +\n  geom_point() +\n  geom_function(fun = ~ predict(model,\n                                newdata = list(conc = .x)),\n                color = \"darkviolet\")\npuro_models <- puro %>% \n  nest(data = c(-state)) %>% # updated to reflect new `nest` syntax\n  mutate(\n    model = map(data, ~ nls(rate ~ rate(conc, Vm, K), data = .x,\n                            start = list(Vm = 200, K = 0.1))),\n    tidy = map(model, broom::tidy),\n    glance = map(model, broom::glance)\n  )\n\npuro_models## # A tibble: 2 x 5\n##   state     data              model  tidy             glance          \n##   <fct>     <list>            <list> <list>           <list>          \n## 1 treated   <tibble [12 × 3]> <nls>  <tibble [2 × 5]> <tibble [1 × 9]>\n## 2 untreated <tibble [11 × 3]> <nls>  <tibble [2 × 5]> <tibble [1 × 9]>\npuro_models %>% \n  unnest(tidy)## # A tibble: 4 x 9\n##   state   data       model term  estimate std.error statistic  p.value glance   \n##   <fct>   <list>     <lis> <chr>    <dbl>     <dbl>     <dbl>    <dbl> <list>   \n## 1 treated <tibble [… <nls> Vm    213.       6.95        30.6  3.24e-11 <tibble …\n## 2 treated <tibble [… <nls> K       0.0641   0.00828      7.74 1.57e- 5 <tibble …\n## 3 untrea… <tibble [… <nls> Vm    160.       6.48        24.7  1.38e- 9 <tibble …\n## 4 untrea… <tibble [… <nls> K       0.0477   0.00778      6.13 1.73e- 4 <tibble …\npuro_models %>% \n  unnest(glance)## # A tibble: 2 x 13\n##   state  data     model tidy    sigma isConv  finTol logLik   AIC   BIC deviance\n##   <fct>  <list>   <lis> <list>  <dbl> <lgl>    <dbl>  <dbl> <dbl> <dbl>    <dbl>\n## 1 treat… <tibble… <nls> <tibbl… 10.9  TRUE   6.09e-6  -44.6  95.3  96.7    1195.\n## 2 untre… <tibble… <nls> <tibbl…  9.77 TRUE   2.31e-6  -39.6  85.2  86.4     860.\n## # … with 2 more variables: df.residual <int>, nobs <int>\npuro %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_smooth(\n    method       = \"nls\",\n    formula      = y ~ rate(conc = x, Vm = Vm, K = K),\n    method.args  = list(start = list(Vm = 200, K = 0.1)),\n    se           = FALSE\n  )"},{"path":"fallacies.html","id":"exercises-6","chapter":"Lesson 7 Fallacies","heading":"7.5 Exercises","text":"","code":""},{"path":"fallacies.html","id":"the-datasaurus-dozen","chapter":"Lesson 7 Fallacies","heading":"7.6 The Datasaurus Dozen","text":"Datasaurus Dozen45 dataset\ncrafted illustrate certain concepts.\ncan accessed R via datasauRus package.Explore dataset looking publication\n(contains spoilers…):\nactually contains 13 different datasets,\ndenoted column dataset, one tibble.\nmeans x y different datasets?\nstandard deviations x y different datasets?\ncorrelations coefficients different datasets?\nbet notice pattern now.\nNow create one (multiple) scatterplots data.\nnotice? conclusions draw observation?\nactually contains 13 different datasets,\ndenoted column dataset, one tibble.\nmeans x y different datasets?\nstandard deviations x y different datasets?\ncorrelations coefficients different datasets?\nbet notice pattern now.Now create one (multiple) scatterplots data.\nnotice? conclusions draw observation?another dataset package illustrate different\npoint:First, turn tidy format, much like datasaurus_dozen\ntibble.Now, visualize distributions values \n5 different groups. Try different versions plot\nsatisfied, sure also include boxplot\ncompare approaches.","code":"\ndatasauRus::datasaurus_dozen\ndatasauRus::box_plots"},{"path":"fallacies.html","id":"preparing-for-the-christmas-special","chapter":"Lesson 7 Fallacies","heading":"7.7 Preparing for the Christmas Special","text":"datasets find interesting? Maybe \nalready data collected maybe just \nidea (second case might also episode\ngetting data internet via webscarping),\nmaybe dataset tidytuesday repository\nspeaks : https://github.com/rfordatascience/tidytuesday","code":""},{"path":"freestyle.html","id":"freestyle","chapter":"Lesson 8 Freestyle","heading":"Lesson 8 Freestyle","text":"… venture unknown show example data\nanalysis, gather data webscraping build small app.today’s lecture, script result experimenting\nunseen data seen video. can refer back\norder get code video goes fast.\n, script less useful script\nregular lectures. hope can learn something \napproach new task handle mistakes \nerrors. video largely unedited; removed \nportion attempts webscraping failed \ninternet connection gone…","code":"\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(glue)"},{"path":"freestyle.html","id":"christmas-theme","chapter":"Lesson 8 Freestyle","heading":"8.1 Christmas Theme!","text":"Christmas RStudio theme: https://github.com/gadenbuie/rsthemes","code":""},{"path":"freestyle.html","id":"advanced-rmarkdown","chapter":"Lesson 8 Freestyle","heading":"8.2 Advanced Rmarkdown","text":"Using RStudio plugin insert citations directly Zotero\npreview \nhttps://blog.rstudio.com/2020/11/09/rstudio-1-4-preview-citations/\npreview \nhttps://blog.rstudio.com/2020/11/09/rstudio-1-4-preview-citations/output formats:\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://github.com/rstudio/rticles\nnote: tinytex package pdf outputs\n\nhttps://bookdown.org/yihui/rmarkdown/https://github.com/rstudio/rticles\nnote: tinytex package pdf outputs\nnote: tinytex package pdf outputs","code":""},{"path":"freestyle.html","id":"finding-help-easier","chapter":"Lesson 8 Freestyle","heading":"8.3 Finding Help Easier","text":"https://reprex.tidyverse.org/","code":""},{"path":"freestyle.html","id":"into-the-unknown-an-example-analysis-of-unseen-data","chapter":"Lesson 8 Freestyle","heading":"8.4 Into the Unknown: An Example Analysis of Unseen Data","text":"Unknown, giphy\napi","code":""},{"path":"freestyle.html","id":"feedback","chapter":"Lesson 8 Freestyle","heading":"8.5 Feedback","text":"send round link feedback form.\nanonymous, way tracking \nsubmitted . just assume \ncount completed exercise.","code":""},{"path":"resources-5.html","id":"resources-5","chapter":"Resources","heading":"Resources","text":"Learning R can quite journey.\ncollecting useful links resources extra page.\nhelp understand topics covered, dive deeper interesting want discover cool things can R.","code":""},{"path":"resources-5.html","id":"learning-the-tidyverse","chapter":"Resources","heading":"8.6 Learning the tidyverse","text":"R Data Science46R4DS online CommunityRStudio Cheat Sheets!Modern Dive47RStudio Education","code":""},{"path":"resources-5.html","id":"learning-rmarkdown","chapter":"Resources","heading":"8.7 Learning Rmarkdown","text":"rmarkdown cheatsheetrmarkdown referencepandoc manual (advanced)rmarkdown reproducible analysisrmarkdown website","code":""},{"path":"resources-5.html","id":"learning-r-in-general","chapter":"Resources","heading":"8.8 Learning R in general","text":"Advanced R48Hands Programming R49R Packages50Data Visualization: Practical Introduction51Graph Cookbook52","code":""},{"path":"resources-5.html","id":"learning-statistics","chapter":"Resources","heading":"8.9 Learning Statistics","text":"Intuitive Biostatistics53Statistics Done Wrong54StatQuest!!! Josh StarnerModern Statistics Modern Biology","code":""},{"path":"resources-5.html","id":"talks-podcasts-blogs-videos-1","chapter":"Resources","heading":"8.10 Talks, Podcasts, Blogs, Videos","text":"Just people inspiring blogposts, videos likes.David Robinson\nYouTube\nwebsite\nYouTubewebsiteJulia Silge\nYouTube\nwebsite\nYouTubewebsiteAlison Hill\nwebsite\nwebsiteThomas Lin Pedersen\nwebsite\nwebsite","code":""},{"path":"resources-5.html","id":"misc-1","chapter":"Resources","heading":"8.11 Misc","text":"Cute insightful illustrations55Happy Git R","code":""},{"path":"resources-5.html","id":"package-documentation-1","chapter":"Resources","heading":"8.12 Package Documentation","text":"tidyversetidymodelsrmarkdownreadrdplyrggplottidyrstringrpurrrragg","code":""},{"path":"resources-5.html","id":"books-and-manuals","chapter":"Resources","heading":"8.13 Books and Manuals","text":"Tidymodels bookggplot bookRmarkdown CookbookRmarkdown Book","code":""},{"path":"resources-5.html","id":"getting-help-1","chapter":"Resources","heading":"8.14 Getting Help","text":"find helpR4DS online learning community","code":""},{"path":"resources-5.html","id":"lists-of-resources","chapter":"Resources","heading":"8.15 Lists of Resources","text":"meta section. list lists:big book Rr rest us","code":""},{"path":"resources-5.html","id":"packages-that-enable-this-lecture-format","chapter":"Resources","heading":"8.16 Packages that enable this lecture format","text":"R R Core Team56knitr Yihui Xie57rmarkdown JJ Allaire et al.58xaringan Yihui Xie59","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
