[{"path":"index.html","id":"welcome","chapter":"Hello and welcome!","heading":"Hello and welcome!","text":"… latest iteration introductory R course, learn analyse data style.Current course dates\nWS21/22\n21.10.21 – 09.12.21\nLecture online time\nSeminar weekly Fridays 9:15 – 10:45\nSign-(Heidelberg University): see discord\nLanguage: Lectures English seminar can German choose \n(case can always ask questions German well).course, handle different kinds data, create pretty insightful visualizations, compute different statistics data also explore statistical concepts mean.\npenguins p-values, got covered.\nFigure 0.1: One plots creating first lecture.\n","code":""},{"path":"index.html","id":"prerequisits","chapter":"Hello and welcome!","heading":"0.1 Prerequisits","text":"prior knowledge necessary.Software install:RRstudio","code":""},{"path":"index.html","id":"structure-of-the-course","chapter":"Hello and welcome!","heading":"0.2 Structure of the course","text":"current course WS21/22 Heidelberg University.\nparticipants biochemistry bachelor (master) students,\nmaterial open anyone!8 lectures total, accompanied :\nvideo lecture top page\nlecture script, consists code written lecture\n(plus code generate illustrative graphics) explanations\nExercises complete send \nseminar discuss exercises\ndiscord server ask questions share solutions\nvideo lecture top pageThe lecture script, consists code written lecture\n(plus code generate illustrative graphics) explanationsExercises complete send inA seminar discuss exercisesA discord server ask questions share solutionsI recommend watch lecture time, use lecture script afterwards look concepts code want revisit.\nCode chunks also copy-button, helpful quickly playing around , make sure actually walk lecture typing first,\nmuscle memory server well future.","code":""},{"path":"index.html","id":"exe","chapter":"Hello and welcome!","heading":"0.2.1 Exercises","text":"complete course, hand least 5 8 exercises.\nimportant part exercise perfect solution,\nencounter questions struggles attempt exercise, make sure include pain points well can cover Seminar.\nPlease hand solutions seminar via direct message discord.\nearlier week submit solutions,\ntime prepare answers seminar.","code":""},{"path":"index.html","id":"seminar","chapter":"Hello and welcome!","heading":"0.2.2 Seminar","text":"week, meet discuss exercises answer questions might popped .\nCurrently looks like possible person, case meet :Mathematikon (INF 205), IWR CIP-Pool 3. OGEven though technically computer rooms, great bring laptop can code along known able apply learned course well.\nAlso, might able install necessary software computer room.","code":""},{"path":"index.html","id":"discord","chapter":"Hello and welcome!","heading":"0.2.3 Discord and signup","text":"biochemistry student Heidelberg University,\nclick link: https://discord.gg/jVZcNPCrp7 join discord server sign course.\nlink doesn’t work, please send message via contact formOnce sure drop message name name matriculation number can put course onto official transcript records.\nserver, able ask questions can answered fellow learners, hand exercises receive feedback.Discord good choice , messages support code formatting can easily open voice call question get complicated.","code":""},{"path":"intro.html","id":"intro","chapter":"Lesson 1 Introduction","heading":"Lesson 1 Introduction","text":"Hi ! material first lecture appear.\nsign , check welcome page… get started R RStudio,\nlearn literate programming build first\nplot discovering Grammar Graphics.","code":""},{"path":"intro.html","id":"what-you-will-learn","chapter":"Lesson 1 Introduction","heading":"1.1 What You will Learn","text":"Throughout scientific career — potentially outside — encounter various forms data. Maybe experiment measured fluorescence molecular probe, simply count penguins local zoo. Everything data form another. raw numbers without context meaningless tables numbers boring look , often hide actual structure data.course learn handle different kinds data. learn create pretty insightful visualizations, compute different statistics data also statistical concepts mean. penguins p-values, got covered.course held English, concepts covered directly transfer research , working language English. said, feel free ask questions language understand, German also fine. Latin little rusty, thought.course, using programming language R. R language particularly well suited data analysis, initially designed statisticians interactive nature language, makes easier get started. don’t fret first encounter programming, take one step time.datasets chosen illustrate various concepts tools particularly centered around Biology. Rather, chose general datasets require less introduction enable us focus learning R statistics. talking penguins, racing games life expectancy instead intricate molecular measurements.","code":""},{"path":"intro.html","id":"execute-r-code","chapter":"Lesson 1 Introduction","heading":"1.2 Execute R Code","text":"consolescripta script like recipe, keeping important keeping e.g.\nplots come !can now execute commands R console bottom left. example can calculate mathematical expression:generate numbers one 10:rarely type directly console. want results reproducible, write code script first, next person1 can see replicate analysis. see reproducibility quite near dear , pop twice. scientists, sure understand importance.script like recipe. important part data analysis\nworkflow, long recipe, can recreate whatever\nproducts (e.g. plots, statistics, tables) ease.create new script, click little button top left corner. script can type regular R code, won’t get executed straight away. send line code console executed, hit Ctrl+Enter. Go ahead, try :","code":"\n1 + 1[1] 2\n1:10 [1]  1  2  3  4  5  6  7  8  9 10"},{"path":"intro.html","id":"get-to-know-rstudio","chapter":"Lesson 1 Introduction","heading":"1.3 Get to know RStudio","text":"projectspanes / layoutimportant settings:\nnever restore .Rdata startup\nnever restore .Rdata startuptheme (via packages,\ntalk later today)get deeper R,\nlet’s talk little bit Home working R:\nRStudio.one important setting like change:\nTools -> Global Options make sure “Restore .RData workspace startup”\nunchecked.\nworkspace RStudio save .RData contains objects created session, , can see Environment pane (default top right panel, bottom right setup). want load objects created last session current session automatically? reason reproducibility. want make sure everything analysis needs script. creates variables plots raw data sole source truth.Check lecture video customization RStudio\ne.g. themes make sure also use RStudio Projects structure work.","code":""},{"path":"intro.html","id":"expressions-tell-r-to-do-things","chapter":"Lesson 1 Introduction","heading":"1.4 Expressions: Tell R to do things","text":"R can lot’s things, let’s start basics, like calculating.\nEverything starts # comment ignored R.Create vectors : operator, e.g. numbers :toAnd mathematical operations automatically “vectorized”:fact, R scalars (individual values), just vectors length 1.","code":"\n1 + 1 # addition[1] 2\n32 / 11 # division[1] 2.909091\n3 * 4 # multiplication[1] 12\n13 %% 5 # modulo[1] 3\n13 %/% 5 # integer division[1] 2\n1:4[1] 1 2 3 4\n1:3 + 1:3[1] 2 4 6"},{"path":"intro.html","id":"variables-boxes-for-things","chapter":"Lesson 1 Introduction","heading":"1.5 Variables: Boxes for things","text":"Often, want store result computation reuse, give sensible name make code readable.\nvariables . can assign value variable using assignment operator <- (RStudio, shortcut : Alt+Minus):Executing code give output, use name variable, can see content.can operations variables:NOTE careful order execution! R enables work interactively execute code write script order Ctrl+Enter, execute (=“source”) whole script, executed top bottom.Furthermore, code executed automatically, change dependency expression later . second assignment x doesn’t change y.Variable names can contain letters (capitalization matters), numbers (first character) underscores _.2A depiction various naming styles Allison Horst3A good convention always use snake_case.","code":"\nmy_number <- 42\nmy_number[1] 42\nx <- 41\ny <- 1\nx + y[1] 42\nx <- 1\ny <- x + 1\nx <- 1000\ny[1] 2\n# snake_case\nmain_character_name <- \"Kvothe\"\n\n# or camelCase\nbookTitle <- \"The Name of the Wind\"\n\n# you can have numbers in the name\nx1 <- 12"},{"path":"intro.html","id":"atomic-datatype","chapter":"Lesson 1 Introduction","heading":"1.6 Atomic datatype","text":"First numbers (internally called numeric double), whole numbers (integer)well rarely used complex numbers (complex)Text data however used often (character, string).\nEverything enclosed quotation marks treated text.\nDouble single quotation marks fine.Logical values can contain yes , rather TRUE FALSE programming terms (boolean, logical).special types mix type. Like NULL value NA Assigned.NA contagious. computation involving NA return NA (R way knowing answer):functions can remove NAs giving us answer:can ask datatype object function typeof:also concept called factors (factor) categorical data, talk later, get deeper vectors.","code":"\n12\n12.5\n1L # denoted by L\n1 + 3i # denoted by the small i for the imaginary part\n\"It was night again.\"\n'This is also text'\nTRUE\nFALSE\nNULL\nNA\nNA + 1[1] NA\nmax(NA, 12, 1)[1] NA\nmax(NA, 12, 1, na.rm = TRUE)[1] 12\ntypeof(\"hello\")[1] \"character\""},{"path":"intro.html","id":"functions-calculate-run-and-automate-things","chapter":"Lesson 1 Introduction","heading":"1.7 Functions: Calculate, run and automate things","text":"R, everything exists object, everything something function.Functions main workhorse data analysis. example, mathematical functions, like sin, cos etc.Functions take arguments (sometimes called parameters) sometimes also return things. sin function takes just one argument x returns sine. returned value us. can use directly another computation store variable. don’t anything return value, R simply prints console.Note, = inside function parenthesis gives x = 0 function separate x defined outside function. example:learn function R, execute ? function name press F1 mouse function. actually one important things learn today, help pages can … well… incredibly helpful.can pass arguments name order appearance. following two expressions equivalent.notable functions start :Combine elements vector:Convert datatypes :Calculate summary value vectore:Create sequences numbers:just learned functions sin, seq max. wait, ! sense functions R (kind language two verbs?!), also powerful way:can define functions!syntax (\\(\\leftarrow\\) grammar programming languages) follows.function ends reaches return keyword. also ends reaches end function body implicitly returns last expression. written bit shorter fact often see people omitting explicit return end:can call freshly defined function:Got error like Error add(23, 19) : find function \"add\"? Check fact execute code defines function (.e. put cursor line function keyword hit Ctrl+Enter.).","code":"\nsin(x = 0)[1] 0\nx <- 10\ncos(x = 0)[1] 1\n# x outside of the function is still 10\nx[1] 10\n?sin\nsin(x = 12)\nsin(12)\nc(1, 3, 5, 31)[1]  1  3  5 31\nas.numeric(\"1\")[1] 1\nas.character(1)[1] \"1\"\nx <- c(1, 3, 5, 42)\nmax(x)[1] 42\nmin(x)[1] 1\nmean(x)[1] 12.75\nrange(x)[1]  1 42\nseq(1, 10, by = 2)[1] 1 3 5 7 9\nname_for_the_function <- function(parameter1, parameter2, ...) { # etc.\n  # body of the function\n  # things happen\n  result <- parameter1 + parameter2\n  # Something the function should return to the caller\n  return(result)\n}\nadd <- function(x, y) {\n  x + y\n}\nadd(23, 19)[1] 42"},{"path":"intro.html","id":"packages-sharing-functions","chapter":"Lesson 1 Introduction","heading":"1.8 Packages: Sharing functions","text":"one using R.\nwelcoming helpful community .\npeople also write bunch functions put together called package.\npeople even went step .\ntidyverse collection packages play well together also iron quirkier ways R works.4\nprovide consistent interface enable us learn less special cases.\nR function install.packages(\"<package_name_here>\") installs packages CRAN curated set R packages.R packages, especially ones using, often come\ngreat manuals help pages added link\npackage website \npackages hexagonal icons package script,\nmake sure click icons.don’t link hand can also always find\nhelp internet.\npackages publish source code site\ncalled GitHub, able find\nlinks, help documentation searching\nr  github.\nSometimes can helpful write R’s full name\nsearching (turns lot thing \nletter R): rstats.","code":""},{"path":"intro.html","id":"literate-programming-with-rmarkdown-code-is-communication","chapter":"Lesson 1 Introduction","heading":"1.9 Literate Programming with Rmarkdown: Code is communication","text":"first package like install called Rmarkdown.\nGo ahead install :one exception effort everything script just console. don’t want R trying install package every time run script, needs happen . can either turn comment, delete script, type console. can also use RStudio’s built-panel package installation.Rmarkdown enables us, combine text code produce range output formats like pdf, html, word documents, presentations etc. fact, whole website, including slides, created Rmarkdown. Sounds exciting? Let’s dive !Open new Rmarkdown document file extension .Rmd New File menu top left corner RStudio: File → New File → R Markdown choose html output format. particularly like html, don’t worry page breaks easily works screens different sizes, like phone.Rmarkdown document consists three things:Metadata:\nInformation document author date format called YAML. YAML header starts ends three minus signs ---.Text:\nRegular text interpreted markdown, meaning supports things like creating headings prefixing line #, text bold output surrounding **.Code chunks:\nStarting line 3 backticks {r} ending 3 backticks. interpreted R code. write code like .R script file. can insert new chunks button top right editor window use shortcut Ctrl+Alt+.Use document thoughts alongside code data analysis. Future (reviewer number 2) happy! run code inside chunks, use,little play button chunk, tried true Ctrl+Enter run one line, Ctrl+Shift+Enter run whole chunk. chunks can large small want, try maintain sensible structure.lecture video also demonstrates different output formats,\nexercises using html_document.knitshow output formats\nhtml_document\npdf_document\ndocx_document\nhtml_documentpdf_documentdocx_documentshow visual editorCute little monsters Rmarkdown Wizards Allison Horst","code":"\ninstall.packages(\"rmarkdown\")"},{"path":"intro.html","id":"the-tidyverse","chapter":"Lesson 1 Introduction","heading":"1.9.1 The Tidyverse","text":"Go ahead install tidyverse packages ","code":"\ninstall.packages(\"tidyverse\")"},{"path":"intro.html","id":"our-first-dataset-the-palmer-penguins","chapter":"Lesson 1 Introduction","heading":"1.10 Our First Dataset: The Palmer Penguins","text":"three penguin species Palmer Archipelago, Allison HorstSo let’s explore first dataset together fresh Rmarkdown document. setup chunk special. gets executed automatically chunk document run. makes good place load packages. dataset working today actually comes package, need install well (Yes, lot installing today, ):populate setup chunk withThis gives us penguins dataset:5","code":"\ninstall.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins"},{"path":"intro.html","id":"dataframes-rs-powerfull-tables","chapter":"Lesson 1 Introduction","heading":"1.10.1 Dataframes: R’s powerfull tables","text":"Let’s talk shape penguins object. str function reveals structure object us.penguins variable contains tibble, tidyverse\nversion dataframe.\nbehaves way prints nicer.\nlist columns, columns (usually) vectors.","code":"\nstr(penguins)tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."},{"path":"intro.html","id":"the-grammar-of-graphics-translate-data-into-visualizations","chapter":"Lesson 1 Introduction","heading":"1.11 The Grammar of Graphics: Translate data into visualizations","text":"probably took course want build cool visualizations data. order , let us talk can describe visualizations. Just like language grammar, smart people came grammar graphics,6 slightly modified turned R package can talk also create visualizations using grammar.7\npackage called ggplot2, already loaded included tidyverse. looking code, can describe need order create graphic.grammar means:\n- can build complex visualizations\nbasic building blocks\nfit together according \nrules (grammar)\n- just like lego bricks\n- just learn building blocks \ndifferent function \ndifferent types plots\n(e.g. barplot, scatterplot, lineplot,\npiechart)can build plot step step. data foundation plot, just gives us empty plotting canvas. assigning individual steps going variable, can sequentially add elements, can one step shown ., add aesthetic mapping plot. creates relation features dataset (like flipper length penguin) visual property, like position x-axis, color shape.Still, plot empty, coordinate system certain scale. geometric objects represent aesthetics. Elements plot added using + operator geometric elements ggplot knows start geom_. Let’s add points:Look help page geom_point find aesthetics understands. exact way features mapped aesthetics regulated scales starting scale_ name aesthetic:can add change labels (like x-axis-label) adding labs function.overall look plot regulated themes like pre-made theme_ functions finely regulated theme() function, uses element functions create look individual elements. Autocomplete helps us lot (Ctrl+Space).summary, plot needs:dataaesthetic mappinggeom(s)(stat(s))coordinate systemguidesscalesthemeWe can save plot ggsave function.\nalso arguments control dimentions resolution\nimage.Next week get rid annoying NA legend sex.","code":"\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %>%\n  ggplot(aes(flipper_length_mm, bill_length_mm,\n             color = species,\n             shape = sex)) +\n  geom_point(size = 2.5) +\n  geom_smooth(aes(group = species), method = \"lm\", se = FALSE,\n              show.legend = FALSE) +\n  labs(x = \"Flipper length [mm]\",\n       y = \"Bill length [mm]\",\n       title = \"Penguins!\",\n       subtitle = \"The 3 penguin species can be differentiated by their flipper- and bill-lengths.\",\n       caption = \"Datasource:\\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins:\\nPalmer Archipelago (Antarctica) penguin data.\\nR package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\",\n       color = \"Species\",\n       shape = \"Sex\") +\n  theme_minimal() +\n  scale_color_brewer(type = \"qual\") +\n  theme(plot.caption = element_text(hjust = 0))\nplt <- ggplot(penguins)\n\nplt\nplt <- ggplot(penguins,\n              aes(x = flipper_length_mm,\n                  y = bill_length_mm,\n                  color = species,\n                  shape = sex))\n\nplt\nplt <- plt +\n  geom_point()\n\nplt\nplt <- plt +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\"))\n\nplt\nplt <- plt +\n    labs(x = \"Flipper length [mm]\",\n         y = \"Bill length [mm]\",\n         title = \"Penguins!\",\n         subtitle = \"The 3 penguin species can differentiated by their flipper and bill lengths\")\nplt <- plt + \n  theme_minimal() +\n  theme(legend.text = element_text(face = \"bold\"))\n\nplt\nmy_plot <- ggplot(penguins,\n                  aes(x = flipper_length_mm,\n                      y = bill_length_mm,\n                      shape = sex,\n                      color = species)) +\n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\")) +\n  labs(title = \"Penguins\") +\n  theme(plot.title = element_text(colour = \"purple\"))\n\nmy_plot \nggsave(\"my_plot.png\", my_plot)"},{"path":"intro.html","id":"the-community-there-to-catch-you.","chapter":"Lesson 1 Introduction","heading":"1.12 The Community: There to catch You.","text":"Comunity Teamwork Allison HorstGoogling Error Message","code":""},{"path":"intro.html","id":"bonus-get-more-rstudio-themes","chapter":"Lesson 1 Introduction","heading":"1.13 Bonus: Get more RStudio themes","text":"talk packages come fromhttps://github.com/gadenbuie/rsthemes","code":""},{"path":"intro.html","id":"exercises","chapter":"Lesson 1 Introduction","heading":"1.14 Exercises","text":"course graded, need way confirming indeed take part course. order get confirmation, send solutions minimum 5 8 exercises Seminar Fridays. week like create fresh Rmarkdown document solutions code well questions arose lecture. help lot improving course.done solving exercises, hit knit button (top editor panel) send resulting html document via discord (confirm looks way expected beforehand).today’s tasks:","code":""},{"path":"intro.html","id":"put-your-flippers-in-the-air","chapter":"Lesson 1 Introduction","heading":"1.14.1 Put your flippers in the air!","text":"fresh Rmarkdown document (without example template content),\nload tidyverse palmerpenguins packages.Write section text previous experience data analysis /programming (optional, can use information customize course).Produce scatterplot (meaning plot points) bill length vs. bill depth, colorcoded species.\nImaginary bonus points manage use colors penguin-image (hint: look help page scale_color_manual() find . Note, R can work ’s built-color names, rgb() specifications hex-codes #1573c7). Even bonus points also look theme() function ’s arguments, theme_<...>() functions make plot prettier.\nImaginary bonus points manage use colors penguin-image (hint: look help page scale_color_manual() find . Note, R can work ’s built-color names, rgb() specifications hex-codes #1573c7). Even bonus points also look theme() function ’s arguments, theme_<...>() functions make plot prettier.Create vector odd numbers 1 99 store variable.\nCreate second variable contains squares first.\nStore variables named list turn list tibble (enhanced version data.frame\nDiscover shortcut three steps using function tibble. Specifically, look third bullet point description ?tibble::tibble (two colons :: specify package function coming . need tibble(...) code tibble package loaded automatically tidyverse. , specify directly send correct help page).\nCreate scatterplot two variables stored tibble using ggplot.\ngeom_ function need add plot add line connects points?\nCreate second variable contains squares first.Store variables named list turn list tibble (enhanced version data.frameDiscover shortcut three steps using function tibble. Specifically, look third bullet point description ?tibble::tibble (two colons :: specify package function coming . need tibble(...) code tibble package loaded automatically tidyverse. , specify directly send correct help page).Create scatterplot two variables stored tibble using ggplot.geom_ function need add plot add line connects points?Check metadata (YAML) Rmarkdown document make sure contains name author: .\ncouple YAML options can try feel adventurous.\ncouple YAML options can try feel adventurous.Knit ship !","code":""},{"path":"intro.html","id":"learn-more","chapter":"Lesson 1 Introduction","heading":"1.15 Learn more:","text":"Check dedicated Resources page.","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"Lesson 2 Data Wrangling","heading":"Lesson 2 Data Wrangling","text":"… explore typical data analysis workflow tidyverse, wrangle different kinds data learn factors.Note:\ntry vocal code plain English type learning “translations” symbols keywords can help , . , programming can feel lot like conversation digital assistant helpful friend. boundary human languages computer languages blurry might think.Right setup-chunk, specify common code execution\noptions like showing code (echo) hiding messages warnings,\nfirst thing usually top new analysis load \npackages going used.\nlater find need , come back add\nlist, people reading code can see straight away,\ninstall order run code .","code":"\nlibrary(tidyverse)"},{"path":"data-wrangling.html","id":"a-data-analysis-workflow","chapter":"Lesson 2 Data Wrangling","heading":"2.1 A Data Analysis Workflow","text":"getting close importing first dataset file R.\nGenerally, first thing needs happen data analysis cover today.\ndata provided already pretty tidy start build visualizations.\ncommunicate-part also covered, working Rmarkdown ,\ndesigned communicate findings.\nNext week also look less tidy data,\ndefined “tidy data” .Figure Hadley Wickham Garrett Grolemund.8","code":""},{"path":"data-wrangling.html","id":"reading-data-with-readr","chapter":"Lesson 2 Data Wrangling","heading":"2.2 Reading Data with readr","text":" package responsible loading data tidyverse called readr, start loading whole tidyverse.Note, general, also load just readr package library(readr), need rest tidyverse later anyways.\nalso option load package rather use one function package prefixing function package name two colons (::) Like :\nreadr::read_csv(\"...\").Without ado, let’s download data today.\nfact, multiple ways go .\ndownload whole course folder GitHub following link\nsays “View book source”\nleft sidebar (pancake-menu mobile view) using download button\nGitHub:data folder called data organized sub-folders\nlecture number.\neverything need today can found folder 02.GitHub, can also download individual files,\nplain text files need remember one extra step.\nalready clicked file GitHub can\nsee ’s content, tempting copy paste link\nbrowser bar use R’s download.file function.\nHowever, just link part website\nGitHub shows file, link actual file.\ncan get correct link clicking Raw button:can use download file:look source lecture find \nset chunk option eval=FALSE, meaning code run.\ndon’t want download file every time make change \ncourse script.common error see people download.file trying download\nfile e.g. folder called data without first creating said folder.\nget one file directory errors, \nlikely cause.read gapminder data (csv)read csv also works url\nprobably also want local copy\nwrite_csv friends\nprobably also want local copywrite_csv friendsView ctrl+clickWith data downloaded, can make use RStudio’s autocompletion\ncomplete file-path data inside quotation marks.\ncan trigger explicitly ctrl+space tab.readr also tell datatypes guessed columns.\nLet’s inspect dataset:gapminder dataset9 excerpt\ngapminder project contains life expectancy birth 142 countries 5 year intervals 1952 2007.\nalso contains population Gross Domestic Product (GDP) per Inhabitant.\nbuilt visualization later .cool trick data variable\nView function.\neffect can reached ctrl+clicking \nusing button next environment panel.noted read_csv function can also\nread data links download automatically.\nHowever, order data nice safe,\nmight want save somewhere, just case\n(links can change, especially someone else’s link).","code":"\ndownload.file(\"https://raw.githubusercontent.com/jmbuhr/dataintro/main/data/02/gapminder.csv\", \"example-download.csv\")\ngapminder <- read_csv(\"data/02/gapminder.csv\")\ngapminder\nour_data <- read_csv(\"https://raw.githubusercontent.com/jmbuhr/dataintro/main/data/02/gapminder.csv\")\n\nwrite_csv(our_data, \"our-data.csv\")"},{"path":"data-wrangling.html","id":"a-project-based-workflow","chapter":"Lesson 2 Data Wrangling","heading":"2.3 A Project-based Workflow","text":"Last week simply went ahead created script file Rmarkdown file folder computer.\nR known, script ?\nknow, look, tell read file save plot?\nmain folder R starts called working directory.\nfind , current working directory , execute function getwd() get working directory:RStudio projects set working directory automatically, convenient.\nmakes easier us share code projects, simply copying whole\nfolder.\nfollow one prerequisite.\nfile paths need relative, absolute.\nabsolute file path starts root operating system,\nwindows see something like C:\\\\User\\Jannik\\Documents\\...\nmac linux starts /home/jannik/Documents/....\nexample, read gapminder dataset :terrible idea!\never move analysis folder, file path longer\ncorrect someone else tries run code\ncertainly called Jannik exact \ndirectory structure.\nalso work.order work portable, robust shareable,\nneed file paths relative root\nproject (set RStudio project)., ./ refers current working directory,\nset RStudio project.\ncan also omitted (e.g. data/02/...),\npath can’t start / \nmark absolute path.also function set working directory\n(called setwd),\nask never use .\norder use , specify\nworking directory using absolute path,\nrendering script useless anyone .\nUse RStudio projects instead.one thing didn’t tell Rmarkdown documents, yet.\nworking directory always folder ,\neven subdirectory project.\nway also means don’t necessarily need project work Rmarkdown,\none anyway makes easier keep track files \nconsistent structure.","code":"\ngetwd()[1] \"/home/jannik/teaching/dataintro\"\nread_csv(\"/home/jannik/Documents/projects/teaching/dataintro/data/02/gapminder.csv\")\nread_csv(\"./data/02/gapminder.csv\")"},{"path":"data-wrangling.html","id":"common-hurdles-when-importing-data","chapter":"Lesson 2 Data Wrangling","heading":"2.3.1 Common Hurdles when Importing Data","text":", importing gapminder csv went smoothly.\nalways case.\nnow look common hurdles importing data.function just used called read_csv, reads file format consists comma separated values.\nLook raw file text editor (word) like notepad RStudio see .\nfile extension .csv can sometimes lying…German, comma used separate decimal numbers\n(vs. dot English),\nlot Software output different type csv-file configured German.\nstill call csv,\nactually separated semicolons!\nspecial function :looking autocompletion options pop typing function name, might noticed similar function read.csv read.csv2.\nfunctions come R, without packages like tidyverse.\ncan course use well,\ntidyverse functions provide consistent experience \nless surprising quirks.\nteaching tidyverse first allows \nlearn less edge cases.look yet another file gapminder_tsv.txt,\nnotice file extension doesn’t tell us much format, text (opposed binary format computers can read).\nlook file:notice values separated “, special sequence stands tab character. read_tsv function job.\nshowing output just\ngapminder dataset .separator (also called delimiter) even obscure,\ncan use general function read_delim.\nSay co-worker misunderstood us thought tsv stands “Tilde separated values,”\ncan still read file.ways raw data can messy hard read depending machine can’t show .\nOne common thing encounter though measurement machines writing additional information first couple lines actual data\n(like time measurement).\nexample:first 2 lines part data.\nReading file normally csv produce something weird:\nfirst line contain commata, assume file contains one column also report bunch parsing failures.\nParsing act turning data represented raw text useful format,\nlike table numbers.can fix telling R skip first 2 lines entirely:using n_max argument functions save space lecture script.can also read excel files using function readxl package.\npackage automatically installed tidyverse,\nloaded along packages via library(tidyverse).\ncan either load library(readxl) \nrefer single function package without loading whole thing\nusing double colons (::) like :Remember, read gapminder dataset first\ntime day, saved variable called gapminder,\ngoing use going forward.","code":"\nread_csv2(\"data/02/gapminder_csv2.csv\")\nread_lines(\"data/02/gapminder_tsv.txt\", n_max = 3)[1] \"country\\tcontinent\\tyear\\tlifeExp\\tpop\\tgdpPercap\"    \n[2] \"Afghanistan\\tAsia\\t1952\\t28.801\\t8425333\\t779.4453145\"\n[3] \"Afghanistan\\tAsia\\t1957\\t30.332\\t9240934\\t820.8530296\"\nread_tsv(\"data/02/gapminder_tsv.txt\")\nread_delim(\"data/02/obscure_file.tsv\", \"~\")\nread_lines(\"data/02/gapminder_messier.csv\", n_max = 5)[1] \"# Some comment about the data\"                   \n[2] \"And maybe a personal note\"                       \n[3] \"country,continent,year,lifeExp,pop,gdpPercap\"    \n[4] \"Afghanistan,Asia,1952,28.801,8425333,779.4453145\"\n[5] \"Afghanistan,Asia,1957,30.332,9240934,820.8530296\"\nread_csv(\"data/02/gapminder_messier.csv\", skip = 2, n_max = 3)\nreadxl::read_xlsx(\"data/02/gapminder.xlsx\")"},{"path":"data-wrangling.html","id":"wrangling-data-with-dplyr","chapter":"Lesson 2 Data Wrangling","heading":"2.4 Wrangling Data with dplyr","text":" number ways can manipulate data.\ncourse mean manipulate ’s original sense, malicious one.\nsometimes referred data wrangling within tidyverse,\njob dplyr package (short data plyer, tool see logo).dplyr provides functions various operations data.\nTheses functions sometimes also called dplyr verbs.\ntake tibble data.frame input (plus additional parameters) always return tibble.\nenough talk, let’s go wrangling!Let’s go data wrangling! Artwork Allison Horst","code":""},{"path":"data-wrangling.html","id":"select","chapter":"Lesson 2 Data Wrangling","heading":"2.4.1 select","text":"first verb introduce used select columns.\nhence, called select.\nfirst argument always data, followed arbitrary number column names.\ncan recognize functions take arbitrary number additional arguments ... autocompletion help page.might confusing don’t need quotation marks around column names like \nlanguages even parts R.concept known quasiquotation data masking.\nquite unique R, allows functions known content data passed use environment computations search variable names.\nvariable country doesn’t exist global environment,\nexist column gapminder tibble.dplyr functions always look data first search names.help page select tells us different ways can select columns.\ncouple examples without output,\nrun R session confirm think \n(look help pages , quite well written).","code":"\nselect(gapminder, country, year, gdpPercap)\nselect(gapminder, year:pop)\nselect(gapminder, starts_with(\"co\"))\nselect(gapminder, where(is.numeric))\nselect(gapminder, where(is.character))\nselect(gapminder, c(1, 3, 4))"},{"path":"data-wrangling.html","id":"filter","chapter":"Lesson 2 Data Wrangling","heading":"2.4.2 filter","text":"selecting columns natural ask select rows.\nachieved function filter.Filter data. Artwork Allison HorstHere, select rows, year greater 2000\ncountry New Zealand.text comparisons cases sensitive, missed\nNew Zealand written lowercase letters.\norder make sure find correct country,\ncan helpful simply convert country names\nlower case, fact can use functions columns\nstraight inside dplyr verb.\nFunctions deal text (strings character R’s language)\ntidyverse start str_, easy find\nautocompletion.Instead combining conditions , (works \n& ), can also use | meaning .\n, get rows country New Zealand \ncountry Afghanistan.particular comparison can written succinctly,\nasking (every row), particular country %% vector?","code":"\nfilter(gapminder, year > 2000, country == \"New Zealand\")\nfilter(gapminder, year > 2000, str_to_lower(country) == \"new zealand\")\nfilter(gapminder, country == \"New Zealand\" | country == \"Afghanistan\")\nfilter(gapminder, country %in% c(\"New Zealand\", \"Afghanistan\"))"},{"path":"data-wrangling.html","id":"mutate","chapter":"Lesson 2 Data Wrangling","heading":"2.4.3 mutate","text":"back manipulating columns, time creating new ones changing old ones.\ndplyr verb called mutate.\nexample, might want calculate total GDP GDP per Capita population:Notice, none functions changed original variable gapminder.\ntake input return output,\nmakes easier reason code later chain pieces code together.\nchange ?\nUse Force! … ahem, mean, assignment operator (<-)., power dplyr shines.\nknows pop gdpPercap columns tibble \ngdp refers new name freshly created column.","code":"\nmutate(gapminder, gdp = pop * gdpPercap)\ngapminder <- mutate(gapminder, gdp = pop * gdpPercap)"},{"path":"data-wrangling.html","id":"interlude-begind-the-magic-handling-data-with-base-r","chapter":"Lesson 2 Data Wrangling","heading":"2.4.4 Interlude: Begind the magic, handling data with base-R","text":"section meant show happens behind scenes.\nstrictly necessary understand details order work effectively tidyverse, helps especially things don’t go planned.Let’s create tibble play :Instead tidyverse functions, can also use\ncalled subsetting, getting subset datasctructure,\nsquare brackets:selected first third column.\nalso works lone vectors:want select columns names without \ntidyverse, pass names character vector\n(hence quotation marks).two things square brackets, separated comma,\nfirst refers rows second refers columns.\ne.g. “first row columns 1 2”:Internally, tibbles / dataframes lists columns.\nLists ways accessing elements.\n$ symbol gets us element list:want use numbers (=indices) get single\nelement list (column tibble),\not use double square brackets:reason : Single square brackets give us subset\nlist, still packed list.\nwant unpack work need \ncontent just one element [[ us.pull function tidyverse works like $.Subsetting works looking things,\nalso allows us replace part subsetting:Note:\nbase-R tidyverse way mutually exclusive.\nSometimes can mix match.","code":"\ntest_tibble <- tibble(\n  x = 1:5,\n  y = x ^ 2,\n  z = c(\"hello\", \"world\", \"test\", \"four\", \"five\")\n)\n\ntest_tibble\ntest_tibble[c(1, 3)]\nevens <- seq(0, 10, 2)\nevens[c(1, 3)][1] 0 4\ntest_tibble[c(\"x\", \"z\")]\ntest_tibble[1, 1:2]\ntest_tibble$x[1] 1 2 3 4 5\ntest_tibble[[1]][1] 1 2 3 4 5\npull(test_tibble, x)[1] 1 2 3 4 5\nx <- 1:10\nx[1] <- 42\nx [1] 42  2  3  4  5  6  7  8  9 10"},{"path":"data-wrangling.html","id":"the-pipe","chapter":"Lesson 2 Data Wrangling","heading":"2.4.5 The pipe %>%","text":"tidyverse functions easier compose (.e. chain together).\nfacilitate , introduce another operator, bit like + numbers + add ggplot components, specially functions.\npipe, can either type insert RStudio Ctrl+Shift+M,\ntakes ’s left side passes first argument function right sideWhy useful?\nImagine data processing involves bunch steps,\nsave output intermediate variables.However, don’t really need intermediate variables\njust clutter code.\npip allows us express data processing series \nsteps:can read pipe head “” “take … pass ….”main tidyverse functions take data first argument,\ncan chain together fluently\nAdditionally, enables autocompletion column names inside function\ngets data.Next tidyverse pipe %>%, might also see |> point.\nlatter pipe introduced base-R whole\npiping thing got popular making part core language.","code":"\nsubset_gapminder <- select(gapminder, country, year, pop)\nfiltered_gapminder <- filter(subset_gapminder, year > 200)\nfinal_gapminder <- mutate(filtered_gapminder, pop_thousands = pop / 1000)\nfinal_gapminder\nfinal_gapminder <- gapminder %>% \n  select(country, year, pop) %>% \n  filter(year > 2000) %>% \n  mutate(pop_thousands = pop / 1000)\n\nfinal_gapminder"},{"path":"data-wrangling.html","id":"arrange","chapter":"Lesson 2 Data Wrangling","heading":"2.4.6 arrange","text":"simple thing might want table sort based column. arrange :helper function desc marks column arranged descending order.\ncan arrange multiple columns, first important.","code":"\ngapminder %>% \n  arrange(year)\ngapminder %>% \n  arrange(desc(year), pop)"},{"path":"data-wrangling.html","id":"summarise","chapter":"Lesson 2 Data Wrangling","heading":"2.4.7 summarise","text":"condense one multiple columns summary values, use summarise.\nLike mutate, can calculate multiple things one step.condensing whole columns one value, flattening tibble style Super Mario jumping mushrooms, often need.\nrather know summaries within certain groups.\nexample maximal gdp per country. group_by .","code":"\ngapminder %>% \n  summarise(\n    max_year = min(year),\n    pop = max(pop),\n    mean_life_expectancy = mean(lifeExp)\n  )"},{"path":"data-wrangling.html","id":"group_by","chapter":"Lesson 2 Data Wrangling","heading":"2.4.8 group_by","text":"group_by considered adverb, doesn’t change data changes subsequent functions handle data. example, tibble groups, summaries calculated within groups:summarize removes one level grouping.\ndata grouped multiple features, means groups remain.\ncan make sure data longer grouped ungroup.Groups also work within mutate filter.\nexample, can get rows gdp per Person highest per country:","code":"\ngapminder %>% \n  group_by(year) %>% \n  summarise(\n    lifeExp = mean(lifeExp)\n  )\ngapminder %>% \n  group_by(year, continent) %>% \n  summarise(\n    lifeExp = mean(lifeExp)\n  ) %>%\n  ungroup()\ngapminder %>%\n  group_by(country) %>% \n  filter(gdpPercap == max(gdpPercap))\ngapminder %>% \n  group_by(year) %>%\n  mutate(pop = pop / sum(pop))"},{"path":"data-wrangling.html","id":"others","chapter":"Lesson 2 Data Wrangling","heading":"2.4.9 others:","text":"can rename columns rename:Sometimes want refer size current group inside mutate summarise. function just called n(). example, wonder many rows data per year.shortcut group_by summarise n() count function:general, might find solving particular problem couple steps elegant solution. discouraged ! simply means always learn, tools already know now get long way set right track.think learned enough dplyr verbs now. can treat little ggplot visualization.","code":"\ngapminder %>% \n  rename(population = pop)\ngapminder %>% \n  group_by(year) %>% \n  mutate(n = n())\ngapminder %>% \n  count(year, country) %>% \n  count(n)"},{"path":"data-wrangling.html","id":"visualization-and-our-first-encounter-with-factors","chapter":"Lesson 2 Data Wrangling","heading":"2.5 Visualization and our first encounter with factors","text":" facet_wrap function slices plot theses subplots, style plot sometimes referred small multiples. point might wonder: “control order facets?” answer : factor!time vector can thought representing discrete categories (ordered unordered), can express turning vector factor factor function. enables R’s functions handle appropriately. Let’s create little example. start character vector.Note new information R gives us, Levels, possible values can put factor. automatically ordered alphabetically creation. can also pass vector levels creation.factor can contain elements levels, omitted whale shark, turned NA. tidyverse contains forcats package help factors. functions package start fct_.example, fct_relevel function,\nkeeps levels let’s us change order:Using action, get:Note: fct_relevel might constructed example.\noften need cousin fct_reoder\nreorder factor values column.Let’s make plot bit prettier adding color!\ngapminder package provided dataset also included nice color palette. included .csv file data/ folder can practice importing data . also take shortcut getting straight package (gapminder::country_colors). , using head function look first couple rows tibble look first couple elements named vector package.named vector means can access individual elements\nnames, ggplot can use names\nmatch example colors countries\npass scale_ function.final plot also add guides(color = \"none\"),\nshow guide (discrete colors typically legend),\nfill whole plot.","code":"\ngapminder %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line() +\n  facet_wrap(~continent)\nanimals <- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals <- factor(animals)\nanimals[1] cat   dog   bear  shark\nLevels: bear cat dog shark\nanimals <- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals <- factor(animals, levels = c(\"cat\", \"dog\"), ordered = TRUE)\nanimals[1] cat  dog  <NA> <NA>\nLevels: cat < dog\nanimals <- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals <- factor(animals)\nfct_relevel(animals, c(\"shark\", \"dog\"))[1] cat   dog   bear  shark\nLevels: shark dog bear cat\ngapminder %>% \n  mutate(continent = fct_relevel(continent, \"Oceania\")) %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line(alpha = 0.3) +\n  facet_wrap(~ continent)\ncountry_colors <- read_csv(\"data/02/country_colors.csv\")\ncolor <- country_colors$color\nnames(color) <- country_colors$country\nhead(color)         Nigeria            Egypt         Ethiopia Congo, Dem. Rep. \n       \"#7F3B08\"        \"#833D07\"        \"#873F07\"        \"#8B4107\" \n    South Africa            Sudan \n       \"#8F4407\"        \"#934607\" \nx <- c(first = 1, second = 3, hello = 5)\nx[\"first\"]first \n    1 \ngapminder %>% \n  mutate(continent = fct_relevel(continent, c(\"Oceania\"))) %>% \n  ggplot(aes(year, lifeExp, color = country)) +\n  geom_line() +\n  facet_wrap(~continent) +\n  guides(color = \"none\") +\n  scale_color_manual(values = color)"},{"path":"data-wrangling.html","id":"exercises-1","chapter":"Lesson 2 Data Wrangling","heading":"2.6 Exercises","text":"Drink cup coffee tea, relax, \njust worked quite long video.Familiarize folders computer.\nMake sure understand, directories files live.Download data today one ways taught.\ncan refer script anytime.file ./data/02/exercise1.txt unfamiliar format.\nFind structured read readr.\nCreate scatterplot x y column ggplot.\nLook help page geom_point.\ndifference geom_point(aes(color = <something>)) geom_point(color = <something>)?\nrelevant hint section ...-argument.\nMake plot pretty coloring points,\nkeeping mind distinction.\nFind structured read readr.Create scatterplot x y column ggplot.Look help page geom_point.\ndifference geom_point(aes(color = <something>)) geom_point(color = <something>)?\nrelevant hint section ...-argument.Make plot pretty coloring points,\nkeeping mind distinction.Read gapminder dataset readr\nUsing combination dplyr verbs / \nvisualizations ggplot,\nanswer following questions:\ncontinent highest life expectancy average\ncurrent year? two options .\nFirst, calculate simple mean countries\ncontinent. , remember countries\ndifferent population sizes, really need\nweighted mean using R’s function weighted.mean().\nrelationship GDP per capita \nlife expectancy? visualization might helpful.\npopulation countries change time?\nMake plot informative adding color,\nfacets labels (geom_text). Can find ,\nadd country name label last year?\nHint: look data argument geom_-functions\n.\nUsing combination dplyr verbs / \nvisualizations ggplot,\nanswer following questions:continent highest life expectancy average\ncurrent year? two options .\nFirst, calculate simple mean countries\ncontinent. , remember countries\ndifferent population sizes, really need\nweighted mean using R’s function weighted.mean().relationship GDP per capita \nlife expectancy? visualization might helpful.population countries change time?\nMake plot informative adding color,\nfacets labels (geom_text). Can find ,\nadd country name label last year?\nHint: look data argument geom_-functions\n.","code":""},{"path":"data-wrangling.html","id":"resources","chapter":"Lesson 2 Data Wrangling","heading":"2.7 Resources","text":"Don’t miss dedicated Resources page.","code":""},{"path":"data-wrangling.html","id":"package-documentation","chapter":"Lesson 2 Data Wrangling","heading":"2.7.1 Package Documentation","text":"tidyverse websiteThe readr package website cheatsheetThe dplyr package website cheatsheet","code":""},{"path":"data-wrangling.html","id":"getting-help","chapter":"Lesson 2 Data Wrangling","heading":"2.7.2 Getting Help","text":"find helpR4DS online learning community","code":""},{"path":"tidy-data.html","id":"tidy-data","chapter":"Lesson 3 Tidy data","heading":"Lesson 3 Tidy data","text":"… explore concept Tidy Data learn \nadvanced data wrangling techniquesOnce , start loading tidyverse.\nlecture video can also see recap, speed run sorts,\nlectures 1 2.","code":"\nlibrary(tidyverse)"},{"path":"tidy-data.html","id":"tidy-data-1","chapter":"Lesson 3 Tidy data","heading":"3.1 Tidy data","text":"Let’s get started pivotal topic.","code":""},{"path":"tidy-data.html","id":"what-and-why-is-tidy-data","chapter":"Lesson 3 Tidy data","heading":"3.1.1 What and why is tidy data?","text":"one concept also lends ’s name \ntidyverse want talk .\nTidy Data way turning datasets uniform shape.\nmakes easier develop work tools get consistent interface.\nknow turn dataset tidy dataset,\nhome turf can express ideas fluently code.\nGetting can sometimes tricky,\ngive important tools.tidy data, variable (feature) forms ’s column.\nobservation forms row.\ncell single value (measurement).\nFurthermore, information things belongs one table.Figure https://r4ds..co.nz/tidy-data.html Wickham Grolemund10","code":""},{"path":"tidy-data.html","id":"make-data-tidy","chapter":"Lesson 3 Tidy data","heading":"3.1.2 Make data tidy","text":"tidyr package.“Happy families alike; every unhappy family unhappy way”\n— Leo Tolstoy (https://tidyr.tidyverse.org/articles/tidy-data.html)quote holds true messy datasets well.\ntidyr package contained tidyverse provides small example datasets \ndemonstrate means practice.\nHadley Wickham Garrett Grolemund use book well (https://r4ds..co.nz/tidy-data.html).11Let’s make data tidy!table1, table2, table3, table4a, table4b,\ntable5 display number TB cases documented \nWorld Health Organization Afghanistan, Brazil,\nChina 1999 2000.\nfirst tidy format, others :nicely qualifies tidy data.\nEvery row uniquely identified country year,\ncolumns properties specific country\nspecific year.","code":"\ntable1"},{"path":"tidy-data.html","id":"pivot_wider","chapter":"Lesson 3 Tidy data","heading":"3.1.3 pivot_wider","text":"Now gets interesting.\ntable2 still looks organized, tidy (definition).\nNote, doesn’t say format useless — ’s places —\nfit snugly tools.\ncolumn type feature country,\nrather actual features hidden column \nvalues count column.order make tidy, dataset needs become wider.","code":"\ntable2\ntable2 %>% \n  pivot_wider(names_from = type, values_from = count)"},{"path":"tidy-data.html","id":"separate","chapter":"Lesson 3 Tidy data","heading":"3.1.4 separate","text":"table3, two features jammed one column.\nannoying, can’t easily calculate\nvalues; stored text \nseparated slash like cases/population.Ideally, want separate column two:","code":"\ntable3\ntable3 %>% \n  separate(col = rate, into = c(\"cases\", \"population\"), sep = \"/\")"},{"path":"tidy-data.html","id":"pivot_longer","chapter":"Lesson 3 Tidy data","heading":"3.1.5 pivot_longer","text":"table4a table4b split data two different tables,\nmakes harder calculate .\ndata closely related, want one table.\nanother principle tidy data violated.\nNotice column names?\n1999 feature Afghanistan can .\nRather, value feature (namely year),\nvalues 1999 column fact\nvalues feature population (table4a)\ncases (table4b).another case similar\nthing twice.\ngeneral rule thumb says:“copy paste code 3 times,\nprobably write function.”advantage reducing code duplication\nenabling us potentially reuse code\nlater another project, also aids readability\nforced give stop name.can use function tables.","code":"\ntable4a\ntable4b\ntable4a %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntable4b %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\nclean_wide_data <- function(data, values_column) {\n  data %>% \n    pivot_longer(-country, names_to = \"year\", values_to = values_column)\n}\nclean4a <- table4a %>% \n  clean_wide_data(\"cases\")\nclean4b <- table4b %>% \n  clean_wide_data(\"population\")"},{"path":"tidy-data.html","id":"left_join","chapter":"Lesson 3 Tidy data","heading":"3.1.6 left_join","text":"Now time join clean4a clean4b together.\n, need operation known databases\njoin.\nfact, whole concept \ntidy data closely related databases \nsomething called Codd`s normal forms12\nthrowing references just case interested \ntheoretical foundations.\nwithout ado:","code":"\nleft_join(clean4a, clean4b, by = c(\"country\", \"year\"))"},{"path":"tidy-data.html","id":"unite","chapter":"Lesson 3 Tidy data","heading":"3.1.7 unite","text":"table5, problem table3 \nadditionally opposite problem!\ntime, feature one column (namely year)\nspread across two columns (century year).want unite one,\nalso deal problem.\nHowever, find newly\ncreated year, cases population columns\nactually stored text, numbers!\nnext step, convert numbers\nparse_number function.parse_number bit like less strict version .numeric.\n.numeric can deal text contains number\nnothing else, parse_number can help us extracting numbers\neven non-number text around :parse_number handles , questions asked:Notice, applied function parse_number \nmultiple columns data?\nnotice pattern, lot’s code repetition,\nchances elegant solution.\ndon’t find elegant solution first try,\nkeeping open mind improve code long run.\ncase, let tell across function.\ncan use inside dplyr verbs mutate summarise\napply function multiple columns:’s first argument takes vector column names\n(c(...) bit) tidy-select specification (see ?dplyr_tidy_select)\n’s second argument either one function even list functions (names).Another way specifying columns use \nsay “every column country”\n-country.","code":"\ntable5\ntable5 %>% \n  unite(\"year\", century, year, sep = \"\") %>% \n  separate(rate, c(\"cases\", \"population\")) %>% \n  mutate(\n    year = parse_number(year),\n    cases = parse_number(cases),\n    population = parse_number(population)\n  )\nas.numeric(\"we have 42 sheep\")[1] NA\nparse_number(\"we have 42 sheep\")[1] 42\ntable5 %>% \n  unite(\"year\", century, year, sep = \"\") %>% \n  separate(rate, c(\"cases\", \"population\")) %>% \n  mutate(\n    across(c(year, cases, population), parse_number)\n  )\ntable5 %>% \n  unite(\"year\", century, year, sep = \"\") %>% \n  separate(rate, c(\"cases\", \"population\")) %>% \n  mutate(\n    across(-country, parse_number)\n  )"},{"path":"tidy-data.html","id":"another-example","chapter":"Lesson 3 Tidy data","heading":"3.1.8 Another example","text":"Let us look one last example data needs tidying,\nalso provided tidyr package example:lot columns!\n76 weeks song entered top 100 (assume USA)\nposition recorded.\nmight format made data entry easier,\nprevious person wanted make plots excel,\nwide format used denote multiple traces.\nevent, style visualizations grammar \ngraphics, want column represent feature,\ndata needs get longer:Let’s save variable.\n, can save extra mutate-step\nperforming transformation text numbers\nright inside pivot_longer function.Yes, pivot functions really powerful!notable difference often happens long-\nwide-format data way missing data handled.every row needs number columns,\nwide format every column week,\nbound lot NA values wherever\nsong simply longer top 100 \nspecified week.\nmissing values explicit.long format option make missing values\nimplicit simply omitting row \nmeaningful information.\nfunction na.omitt example, can remove\nrows NA somewhere:Let’s reward little visualization.\n, also introducing plotly package,\nhandy function ggplotly turn \nregular ggplot interactive plot.\nPlotly also way building plots,\nmight want check advanced interactive\n3-dimensional plots: https://plotly.com/r/,\npart don’t need worry due amazingly\nsimple ggplotly translation function.whole tidy data idea might seem like just another\nway moving numbers around.\nbuild mental model ,\ntruly transform way able think data.\ndata wrangling dplyr, shown last week,\nalso data visualization ggplot,\njourney began first week \nstill well underway.","code":"\nhead(billboard)\nbillboard %>% \n  pivot_longer(starts_with(\"wk\"), names_to = \"week\", values_to = \"placement\") %>% \n  mutate(week = parse_number(week)) %>% \n  head()\ntidy_bilboard <- billboard %>% \n  pivot_longer(starts_with(\"wk\"),\n    names_to = \"week\",\n    values_to = \"placement\",\n    names_prefix = \"wk\",\n    names_transform = list(week = as.integer)\n  )\n\ntidy_bilboard %>% head(10)\ntidy_bilboard %>%\n  head(10) %>% \n  na.omit()\nplt <- tidy_bilboard %>% \n  ggplot(aes(week, placement)) +\n  geom_point(aes(label = paste(artist, track))) +\n  geom_line(aes(group = paste(artist, track)))\n\nplotly::ggplotly(plt)"},{"path":"tidy-data.html","id":"more-shapes-for-data","chapter":"Lesson 3 Tidy data","heading":"3.2 More shapes for data","text":"Data comes many shapes R just\nvectors dataframes / tibbles.\ncourse omitting matrices,\nstore data type 2 dimensions,\n’s multi-dimensional equivalent arrays.omitting, fact already teased\nnever properly defined lists.","code":""},{"path":"tidy-data.html","id":"lists","chapter":"Lesson 3 Tidy data","heading":"3.2.1 Lists","text":"first glance, lists similar atomic vectors,\none dimensional data structures \ncan names.sets apart atomic vectors can \ncontain data type (like numbers text),\nlist can contain anything, even lists!turns , dataframes internally also lists,\nnamely list columns.\njust properties\n(R calls attributes) tell R display \nfamiliar rectangular shape.","code":"\nc(first = 1, second = 2) first second \n     1      2 \nlist(first = 1, second = 2)$first\n[1] 1\n\n$second\n[1] 2\nx <- list(first = 1, second = 2, \"some text\", list(1, 2), 1:5)\nx$first\n[1] 1\n\n$second\n[1] 2\n\n[[3]]\n[1] \"some text\"\n\n[[4]]\n[[4]][[1]]\n[1] 1\n\n[[4]][[2]]\n[1] 2\n\n\n[[5]]\n[1] 1 2 3 4 5\npalmerpenguins::penguins %>% head()"},{"path":"tidy-data.html","id":"nested-data","chapter":"Lesson 3 Tidy data","heading":"3.2.2 Nested data","text":"tidyr package provides tools dealing\ndata various shapes.\njust discovered first set operations called pivots\njoins get feel tidy data obtain various formats.\ndata always rectangular like can show spreadsheet.\nSometimes data already comes nested\nform, sometimes create nested data serves\npurpose.\n, mean nested?Remember lists can contain elements type, even lists.\nlist contains lists, call nested e.g.nested list always fun work ,\nstraightforward way represent\ndata rectangular, flat format,\nlikely want .\ndeal data rectangling today well.\nfirst, another implication\nnested lists:dataframes (tibbles) built top\nlists, can nest !\ncan sometimes come really handy.\ndataframe contains column \natomic vector list (list list),\ncall list column:Use View function, click environment panel inspect\nnested data better overview.course unlikely build nested tibbles hand\ntibble function.\nInstead, data usually comes dataset working .\nLet’s take familiar penguins dataset nest .nest syntax similar mutate, first specify name\ncolumn create (call data ),\nfollowed specification columns nest list column.data column now list tibbles individual tibble \nlist contains data species row.\nLooking data column’s first element, can see indeed\nregular tibble didn’t take personal get stuffed \nlist column.unnest column use function unnest.\nSometimes need specific use unnest_wider\nunnest_longer, automatic unnest makes\nright choices already.","code":"\nlist(\n  c(1, 2),\n  list(\n    42, list(\"hi\", TRUE)\n  )\n)\nexample <- tibble(\n  x = 1:3,\n  y = list(\n    \"hello\",\n    TRUE,\n    1:4\n  )\n)\n\nexample\n# View(example)\nnested <- palmerpenguins::penguins %>% \n  nest(data = -island)\n\nnested\nnested$data[[1]]\nnested %>% \n  unnest(data)"},{"path":"tidy-data.html","id":"exercises-2","chapter":"Lesson 3 Tidy data","heading":"3.3 Exercises","text":"","code":""},{"path":"tidy-data.html","id":"tidy-data-2","chapter":"Lesson 3 Tidy data","heading":"3.3.1 Tidy data","text":"first set exercises cheating little\ntake (absolutely brilliant) book\nR Data Science13 \noriginal creator much tidyverse.\n, first part, solve / answer 4 questions\nfound : https://r4ds..co.nz/tidy-data.html#exercises-24I give another hint, haven’t mentioned\nfar: introduced variables told \ncan contain letters, underscores numbers\nallowed start number.\nHowever, can use “illegal” names variables \ncolumns surround backticks, e.g.:Hadley can refer columns named years\npivot_longer exercise 1.","code":"\n`illegal variable` <- 42\n`illegal variable`[1] 42"},{"path":"tidy-data.html","id":"a-new-dataset-airlines","chapter":"Lesson 3 Tidy data","heading":"3.3.2 A new dataset: airlines","text":"Imagine second whole pandemic thing \ngoing planning vacation.\ncourse, want choose safest airline possible.\ndownload data incident reports.\ncan find ./data/03/ folder.Instead type_of_event n_events columns\nlike one column per type event,\nvalues count event.airlines least fatal accidents?\nhappens standardized numbers\ndistance theses airlines covered two time ranges?airlines best record comes\nfatalities per fatal accident?Create informative visualizations / tables\ncommunicate discoveries.\nmight beneficial plot e.g. highest lowest scoring Airlines.\nOne slice_ functions help .\nmake plot organized, might want \nlook fct_reorder.","code":""},{"path":"tidy-data.html","id":"resources-1","chapter":"Lesson 3 Tidy data","heading":"3.4 Resources","text":"tidyr documentationpurrr documentationstringr documentation\nworking text helpful cheatsheet \nregular expressions mentioned video","code":""},{"path":"functional-programming.html","id":"functional-programming","chapter":"Lesson 4 Functional Programming","heading":"Lesson 4 Functional Programming","text":"… functions, bringing whole tidyverse together \nexploring advanced dplyr data wrangling techniques.","code":""},{"path":"functional-programming.html","id":"todays-goal","chapter":"Lesson 4 Functional Programming","heading":"4.1 Todays goal","text":"goal today bring together everything\nlearned far solidify\nunderstanding wrangling data tidyverse.\ngoes according plan,\nmental capacity\nfreed statistics starting next week.\nunderstanding data hopefully\nenable us experiment play statistical\nconcepts without getting stuck much\ndata wrangling.\nalso means today’s lecture might\nchallenging far, \neverything learned now – \none way another – relevant.first, load libraries today usual.\nNote, cheating bit loading\ngapminder package well.\nEven though reading actual gapminder dataset\nfiles today, access \nvector country colors nice .lecture also quickly go \nimportant resources far.\nmostly concerns pure R resources,\ncover resources statistics maths\ninvolved next couple lectures.might able tell,\nmental models one favorite topics.\nstarting today powerful mental model:\niteration.","code":"\nlibrary(tidyverse)\nlibrary(gapminder)"},{"path":"functional-programming.html","id":"iteration","chapter":"Lesson 4 Functional Programming","heading":"4.2 Iteration","text":"Iteration basic idea one thing multiple times.\narea computers shine,\nchapter learn fully utilize power fingertips.example, reading multiple similar files.\nRemember gapminder dataset?\nWell, working ,\ntime, collaborator sent us one csv-file continent.\ncan find data/04/ folder.already know read one csv-file:","code":"\nread_csv(\"./data/04/Africa.csv\")"},{"path":"functional-programming.html","id":"the-imperative-programming-approach","chapter":"Lesson 4 Functional Programming","heading":"4.2.1 The Imperative Programming Approach","text":"first solution problem favorite one,\nwant show anyway sake completeness.\ngeneral, Functional Programming, tell computer want, Imperative Programming, tell computer steps .\n, tell R steps perform.\nFirst, get vector file-path’s \nfs package, stands file system.\ndir_ls means directory list, get \ncontents directory.\ncreate list store dataframes \ngoing read .\nalready define length list \nmaking data structure longer R’s strong suit \ndoesn’t know much space reserve .\niterate numbers 1 length\npaths.\niteration get path, read store\nresults list position .\nFinally bind list one dataframe:lost information Continent,\nfile name,\ndwelling long, let’s leave convoluted manual way behind explore ,\ndare say, elegant approach.","code":"\npaths <- fs::dir_ls(\"./data/04/\")\n\nresult <- vector(mode = \"list\", length = length(paths))\nfor (i in 1:length(paths)) {\n  result[[i]] <- read_csv(paths[i])\n}\n\nbind_rows(result)"},{"path":"functional-programming.html","id":"the-functional-programming-approach","chapter":"Lesson 4 Functional Programming","heading":"4.2.2 The Functional Programming Approach","text":"“course someone write -loops.\ndoesn’t .”\n— Jenny BryanWe function (read_csv) takes file path returns\n(spits ) data.\nFunctional Programming style,\nnext idea now function, takes two things:\nvector (atomic list) function.\nfeeds individual elements vector function,\none another.\nmathematics, relation set inputs set outputs called map,\nname following family functions comes .\ntidyverse, functional programming concepts live purrr package.Iterating Explicitly maps:First, create vector things want iterate , things\nfed function one :map read_csv function vector\nbind resulting list dataframes one dataframe:operation mapping vector combining resulting\nlist one dataframe actually common \nvariant map step automatically:distills everything initial -loop just one line code.\nUsing .id argument can save name file path\ncolumn dataset.\nallows us extract continent :way extracting continent file path seems magical first,\nstill refer cheat sheet stringr package lot\ndeal text:Iterating implicitly vectorized functions:first encounter iteration implicit form.\nuse\nR’s basic math operators, computer iterating behind scenes.\nTake expression:operation vectorized.\nWithout us tell R , R add first element first vector first element second vector forth.Notice, looks like operation happens time.\nreality, happens.\ncomputer just really fast adding numbers, one .mathematical operations R call another programming language \nactual addition.\nprogramming language closer way computers think,\nmaking less fun write us humans,\nalso faster instructions easier translate actions computer processor.Remember, build iteration (e.g. map function),\nfind task want apply multiple things,\nalready vectorized.\nturns , fs readr packages play \nwell together, readr can also just take vector\nfile paths combining automatically!Whenever encounter new problem, ask questions:function already ?already vectorized?, function solves problem one instance?Can map many things?purrr package contains various variants map function.map always return list.map_chr always returns atomic character (=text) vector.map_dbl always returns numbers (dbl = double precision).map_lgl always returns logical (yes , TRUE / FALSE) vectors.map_dfr always returns dataframe.-loop-version lot code, especially boilerplate, code just make construct work doesn’t convey intentions \ncode.\nFurthermore, loop focuses object iterated (file\npaths), map-version focuses happening (function, read_csv).\nloop still works.\ncan’t think way solve problem map function, absolutely OK use -loops.","code":"\npaths <- fs::dir_ls(\"./data/04/\")\nresult <- map(paths, read_csv)\nbind_rows(result)\nmap_df(paths, read_csv, .id = \"continent\")\ngapminder <- map_df(paths, read_csv, .id = \"continent\") %>% \n  mutate(continent = str_extract(continent, \"(?<=/)\\\\w+(?=\\\\.csv)\"))\n\nhead(gapminder)\n1:3 + 1:3[1] 2 4 6\nread_csv(paths, id = \"continent\")\n# map_"},{"path":"functional-programming.html","id":"if-you-copy-and-paste-the-same-code-more-than-three-times-write-a-function.","chapter":"Lesson 4 Functional Programming","heading":"4.3 “If you copy and paste the same code more than three times, write a function.”","text":"Writing functions can helpful making \ncode readable.\nallows us separate certain steps analysis\nrest, look isolation test \nvalidate , also allows us give reasonable names.also allows us re-use function across projects!\nLet’s imagine experiment read-\nmachine always certain format needs \ncleaning .\nturn function, e.g. like :can put function file (case R/my_functions)\nsource , example multiple analysis Rmarkdown documents.\nsource function nothing special, just runs R\ncode file.\ndefine functions file, functions\navailable us:Note: example function make sense use \n4 input, data days\ncourse different, hope get gist.like store regular R files (opposed Rmd files)\nfolder project called R.\nmakes already look like R package,\ncase decide later functions\nhelpful others well\nwant share easily\ncolleagues.\ncan read creating \nR packages .14","code":"\nread_experiment_data <- function(day) {\n  day <- str_pad(day, pad = 0, width = 2)\n  paths <- fs::dir_ls(paste0(\"./data/\",day,\"/\"))\n\n  gapminder <- map_df(paths, read_csv, .id = \"continent\") %>% \n    mutate(continent = str_extract(continent, \"(?<=/)\\\\w+(?=\\\\.csv)\"))\n\n  gapminder\n}\nsource(\"R/my_functions.R\")\nread_experiment_data(4)"},{"path":"functional-programming.html","id":"implicit-iteration-with-dplyr-build-many-models","chapter":"Lesson 4 Functional Programming","heading":"4.4 Implicit iteration with dplyr: build many models","text":"dplyrs idea grouping allows us express many\nideas implicitly also iterations.Let us start looking just one country first:can plot life expectancy time ggplot\nadd linear model plot \ngeom_smooth using method = \"lm\".However, geom_smooth allows us easily add smoothing\nlines linear trends plot, \ngive us information actual model.\norder need fit \nlm function:~ symbol defines formula, can read :\n“lifeExp depending year.”\ndata argument tells R look variables\nlifeExp year, namely algeria tibble.lot information model!\nbroom package provides functions cleaner\nspecialized output:Every 1 year life expectancy Algeria went half year.\ncourse, valid limited linear regime \ndatapoints.\ncan’t extrapolate indefinitely.\n, intercept tells us negative\nlife expectancy year 0.given data , good line fit ?R2 takes values 0 1, 1 perfectly\nstraight line connecting points 0 points\nplace.\n0.985 pretty good fit!Using dollar syntax, get pull one column \ntibble, can write function , given model,\nreturns R2And now come dplyr magic!\ngroup country (continent good measuere\njust don’t loose column summarizing),\ncan calculate linear model every country!Note two things: Firstly, don’t use data argument\nlm tidyverse functions already know\nlook lifeExp year \nrespecting groups.\nwithin group (.e. country), lifeExp contain life expectancy country,\nlife expectancy countries.\nSecondly, wrap part list \ncreate list column (models don’t fit atomic vector).\nOtherwise, dplyr complain.second step calculate rsqured value\nmodel mapping function created \nmodels.\nuse _dbl variant want values\natomic vector numbers, list.Finally, can add calculated R2 values column \noriginal gapminder dataset can use \nfollowing visualization., highlight irregularities (less linear countries)\nmaking transparency (= alpha value) depend\nR2 value.can highlight one continent filtering use \nggrepel package allow flexible labels.\nFurthermore check source document lecture\nvideo lecture find can\nadd figure caption control width height\nplot via knitr chunk options.\nalso showcase new way writing chunk options\nlatest version knitr package especially\nsuited longer captions.\nFigure 4.1: Figure caption\ndownward slope highlighted countries\nstarting 1990s result ravaging\nAIDS pandemic.\nprominent dips two curves, orange \nRwanda Cambodia gray, direct\nconsequences genocides.\ndire realities can way summarized\njust couple colorful lines.\nalso way qualified lecture\ntopics.\ngood friend mine, Timothy Williams,\nhowever researcher teacher field conflict violence\nfocus genocides.\nfield work Cambodia Rwanda\nbook “Complexity Evil. Perpetration Genocide” published December 18 2020.15","code":"\nalgeria <- gapminder %>% \n  filter(country == \"Algeria\")\nalgeria %>% \n  ggplot(aes(year, lifeExp)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\nmodel <- lm(lifeExp ~ year, data = algeria)\nsummary(model)\nCall:\nlm(formula = lifeExp ~ year, data = algeria)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3844 -0.5935 -0.2703  0.5339  2.4992 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.068e+03  4.380e+01  -24.38 3.07e-10 ***\nyear         5.693e-01  2.213e-02   25.73 1.81e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.323 on 10 degrees of freedom\nMultiple R-squared:  0.9851,    Adjusted R-squared:  0.9836 \nF-statistic: 661.9 on 1 and 10 DF,  p-value: 1.808e-10\nbroom::tidy(model)\nbroom::glance(model)\nget_r_squared <- function(model) {\n  broom::glance(model)$r.squared\n}\nall_models <- gapminder %>% \n  group_by(country, continent) %>% \n  summarise(\n    model = list(lm(lifeExp ~ year)),\n    rsquared = map_dbl(model, get_r_squared)\n  )\n\nhead(all_models)\ngapminder <- gapminder %>% \n  left_join(select(all_models, -model))\ngapminder %>% \n  ggplot(aes(year, lifeExp, color = country, alpha = 1/rsquared)) +\n  geom_line() +\n  guides(color = \"none\", alpha = \"none\") +\n  scale_color_manual(values = country_colors) +\n  facet_wrap(~continent, scales = \"free\") +\n  theme_minimal()\ngapminder %>% \n  filter(continent == \"Africa\") %>% \n  ggplot(aes(year, lifeExp, color = country, group = country)) +\n  geom_line(color = \"black\", alpha = 0.3) +\n  geom_line(data = filter(gapminder, rsquared <= 0.4), size = 1.1) +\n  ggrepel::geom_text_repel(aes(label = country),\n                           data = filter(gapminder, rsquared <= 0.4,\n                                         year == max(year)),\n                           nudge_x = 20,\n                           direction = \"y\"\n                           ) +\n  guides(color = \"none\", alpha = \"none\") +\n  scale_color_manual(values = country_colors) +\n  facet_wrap(~continent, scales = \"free\") +\n  labs(x = \"Year\", y = \"Life Expectancy at Birth\") +\n  theme_minimal() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.2)))"},{"path":"functional-programming.html","id":"exercises-3","chapter":"Lesson 4 Functional Programming","heading":"4.5 Exercises","text":"want get playing around data,\nkeep mind solutions exercise\nset stone.\noften one viable way graphing\ndataset use \nOffice Hour talk advantages\ndisadvantages approaches \ncame .","code":""},{"path":"functional-programming.html","id":"roman-emperors","chapter":"Lesson 4 Functional Programming","heading":"4.5.1 Roman emperors","text":"first exercise uses dataset roman emperors\ntidytuesday project\n(link).\ncan import :slight error data dates actually BC time.\norder fix using lubridate package,\ninstalled tidyverse, automatically loaded.\nconvenience function can use fix dataset:questions answer.\nDecide , particular question best\nanswered using visualization, table, simple sentence\ncombination three.popular way rise power?common causes death among roman\nemperors? () killed ?dynasty successful?\nFirstly, often dynasty reign?\nSecondly, long reigns?\ndynasty rather part ,\ngoal live longest?\nFirstly, often dynasty reign?Secondly, long reigns?dynasty rather part ,\ngoal live longest?","code":"\nemperors <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv\")\nlibrary(tidyverse)\nlibrary(lubridate)\n\nfix_emperors <- function(data) {\n  data %>% \n    mutate(\n      birth = case_when(\n        index %in% c(1, 2, 4, 6) ~ update(birth, year = -year(birth)),\n        TRUE                     ~ birth\n      ),\n      reign_start = case_when(\n        index == 1 ~ update(reign_start, year = -year(reign_start)),\n        TRUE       ~ reign_start\n      )\n    )\n}"},{"path":"functional-programming.html","id":"dairy-products-in-the-us","chapter":"Lesson 4 Functional Programming","heading":"4.5.2 Dairy Products in the US","text":"Another dataset\n(link)\nconcerns dairy product consumption per person US across number years.\nLoad :masses given lbs (pounds),\ncan convert kg?products lost customer base time,\nones won? products greatest absolute\nchange production estimated straight line?, fun! make interesting\nfindings along way, go ahead produce plots\nhighlight .","code":"\ndairy <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv\")"},{"path":"functional-programming.html","id":"resources-2","chapter":"Lesson 4 Functional Programming","heading":"4.6 Resources","text":"purrr documentationstringr documentationdplyr documentation","code":""},{"path":"probability-and-hypothesis-testing.html","id":"probability-and-hypothesis-testing","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"Lesson 5 Probability and Hypothesis Testing","text":"… reason nature randomness \ndiscover various statistical tests.","code":""},{"path":"probability-and-hypothesis-testing.html","id":"motivation","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.1 Motivation","text":"first four lectures covered fundamentals handling data R.\nNow, shift focus away towards \ndata analysis. talk different statistical tests, common mistakes,\navoid spot research. course, \nusing R. still learn one useful function \ntechnique along way. instances clear use R\nsolely demonstrate idea statistics code just included \ncurious, whether code something likely also use \nanalysis. open questions things unclear two\ncases. purely aesthetic code might also speed typing edit.longer text parts might helpful look script watching video pause frequently take notes (Rmarkdown great lecture notes well!).","code":"\nlibrary(tidyverse)"},{"path":"probability-and-hypothesis-testing.html","id":"statistically-significant","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.2 Statistically Significant…","text":"…keep using word. don’t think means think means.statistically significantYou hear phrases “statistically significant,” “significant” even\n“significant” thrown around quite bit academic literature . \noften used carelessly, clearly defined meaning. meaning\nuncover today. meaning related concept called\np-values, equally bad reputation frequently misused. \np p-value stands probability, order understand p-values, \nneed understand probability learn deal randomness, chance, \nluck . …","code":""},{"path":"probability-and-hypothesis-testing.html","id":"getting-our-hands-dirty-with-probability","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.3 Getting our Hands dirty with Probability","text":"understand statistics means understanding nature randomness first.\nFigure 5.1: ggplot chessboard\nSay friend playing game chess, friend proudly\nproclaims:\n“definitely better player!”\n“Proof !” reply.\n“’s easy,” says: “won 7 8 rounds played today.”\n“Pah! ’s just luck.” less witty slightly stubborn response.expected, shall using R resolve vital conflict.R rainbow","code":""},{"path":"probability-and-hypothesis-testing.html","id":"definitions-hypothesis","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.3.1 Definitions: Hypothesis","text":"involuntarily uttered hypothesis, testable assumption. \nwant test hypothesis using statistics. first hypothesis (“\nbetter player.”) call alternative hypothesis (\\(H_1\\)). \nname can bit confusing, often, actual scientific\nhypothesis, thing interested . , alternative ? \nalternative called null hypothesis (\\(H_0\\)), second\nstatement (“just luck”). null hypothesis provides sort baseline\nfindings. usually goes along lines “\nobservations just based chance alone?” “chance” can source\nrandom variation system.tricky part way directly test alternative\nHypothesis, can test null hypothesis. null\nhypothesis discard, always multiple alternative hypothesis \nexplain data. example, even end discarding idea \nfriend’s chess success luck, prove \nalternative hypothesis better player (still \ncheating example). keep mind transfer \nscientific setting. Just show something unlikely \narisen chance mean favorite alternative hypothesis \nautomatically true., words warning, let’s test null hypothesis!","code":""},{"path":"probability-and-hypothesis-testing.html","id":"testing-the-null-hypothesis-with-a-simulation","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.3.2 Testing the Null Hypothesis with a Simulation","text":"start building little simulation. testing \nhypothesis, important defined \\(H_0\\) \\(H_1\\) properly, \nprevious section. need little specific. Winning \nchance entail completely random process, can model coin\nflip. R lovely function sample take number things \nvector, without replacement taking thing:giving number things draw just shuffles vector, \nfairly boring case just two tings. can’t sample 10 things \nvector two elementsBut can, put thing back every time:, let’s make little specific question:run script million times, resulting proportion\nrandom wins , close 50-50 \nused fair coin. However, don’t time play much Chess \nsure don’t money run million replicates experiment \nlab. , little simulated world, near infinite\nresources (simulation computationally costly).One trick used : calculate e.g. sum mean,\nR automatically converts TRUE 1 FALSE 0.Let’s create function returns random number wins friend \ngotten pure chance number rounds N.number different every time, change?histogram type plot shows often value occurs \nvector. Usually, values put bins first, grouping close values\ntogether continuous values, case makes sense just one\nvalue per bin dealing discrete values (e.g. half-wins).\nHistograms can either display raw counts frequency e.g. \npercentage. ggplot, use geom_bar don’t need binning, just\ncounting occurrences, geom_histogram need bin continuous\nvalues.expected, common number wins 8 4 (unless got really\nunlucky compiling script). Let us see, distribution\nchanges different values N. First, set grid numbers (\npossible combinations) can run bunch simulations:use trusty ggplot visualize distributions.fair coin, common number wins half number \ncoin flips. Note, still possible flip coin 15 times \nwin single time. just unlikely bars small \ncan’t see .Let us go back original debate. first statement: “better.” \nsomething can never definitively proven. always \npossibility, matter small, result arisen pure\nchance alone. Even wins 100 times don’t take single game \n, sort outcome still impossible appear just flipping \ncoin. can , calculate, likely certain event \nassumption null hypothesis (chance). can also decide \nthreshold \\(\\alpha\\) reject null hypothesis. called \nsignificance threshold. make observation calculate \nprobability observation like extreme smaller \nthreshold, deem result statistically significant. probability\nthus created called p-value.simulation, find probability win 7 8 rounds \nnull hypothesis :smaller commonly used significance threshold \\(\\alpha=0.05\\)\n(.e. \\(5\\%\\)). 7 8 wins, reject null hypothesis.\nnote threshold, matter commonly thoughtlessly used\nthroughout academic research, completely arbitrary.","code":"\ncoin <- c(\"heads\", \"tails\")\nsample(coin)[1] \"tails\" \"heads\"\nsample(coin, size = 10)Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\nsample(coin, 10, replace = TRUE) [1] \"heads\" \"heads\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\"\n[10] \"tails\"\nwinner <- c(\"you\", \"friend\")\nrandom_winners <- sample(winner, size = 8, replace = TRUE)\nrandom_winners[1] \"you\"    \"friend\" \"you\"    \"you\"    \"you\"    \"you\"    \"friend\" \"friend\"\nrandom_winners == \"friend\"[1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n1 + TRUE[1] 2\n1 + FALSE[1] 1\nsum(random_winners == \"friend\")[1] 3\nmean(random_winners == \"friend\")[1] 0.375\nget_n_win <- function(N) {\n  winner <- c(\"you\", \"friend\")\n  random_winners <- sample(winner, size = N, replace = TRUE)\n  sum(random_winners == \"friend\")\n}\n\nget_n_win(8)[1] 4\nresult <- map_dbl(rep(8, 1000), get_n_win)\nhead(result)[1] 8 6 3 1 4 1\ntibble(result) %>% \n  ggplot(aes(x = result)) +\n  geom_bar() +\n  labs(x = \"N wins for friend\",\n       title = \"Throwing a coin 8 times\") +\n  scale_x_continuous(breaks = 0:8)\nsimulation <- crossing(\n   N = 1:15,\n   rep = 1:1000\n) %>% \n  mutate(\n    wins = map_dbl(N, get_n_win)\n  )\nsimulation %>% \n  ggplot(aes(wins)) +\n  geom_bar() +\n  facet_wrap(~N, labeller = label_both) +\n  labs(title = \"Flipping a coin N times\")\nsimulation %>% \n  filter(N == 8) %>% \n  summarise(\n    mean(wins >= 7)\n  )"},{"path":"probability-and-hypothesis-testing.html","id":"getting-precise-with-the-binomial-distribution","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.3.3 Getting precise with the Binomial Distribution","text":"Now, just simulation 1000 trials, number can’t \narbitrarily precise, mathematical formula probability.\ncreated counting number successes series yes--trials\nbinomial distribution. common distributions, R provides \nset functions. functions starting d give us probability\ndensity function. case discrete values like counting wins, \nequivalent actual probability, continuous values obtain \nprobability taking integral. get integrals \ncorresponding functions starting p (probability).probability win exactly 7 8 games. wanted\nprobability 7 8! move integral. \npart can get bit confusing, default pbinom \nlower.tail = TRUE, according help page means probabilities\nreturns \\(P[X \\le x]\\).set lower.tail FALSE , get \\(P[X > x]\\), probability \nrandom variable X bigger number x. get probability \ninterested , need replace 7 6 well:simulation pretty close! exact values agrees reject \nnull hypothesis opponents equally good.\nfull graph probability density function binomial\ndistribution.integral, probability \\(P[X \\le x]\\).two functions want showcase family. third \ncalled quantile function. Quantiles divide probability distribution\npieces equal probability. One example quantile 50th\npercentile, also known median, divides values half \nvalues half . can keep dividing two halves\nwell, end quantiles. Eventually, arrive \nquantile function. inverse probability function, obtain\nswapping axis.Quantiles also useful \ndeciding random sample follows certain distribution\nquantile-quantile plots.Lastly, always also r variant function, gives us \nnumber random numbers distribution.","code":"\ndbinom(x = 7, size = 8, prob = 0.5) [1] 0.03125\npbinom(q = 7, size = 8, prob = 0.5)[1] 0.9960938\npbinom(q = 6, size = 8, prob = 0.5, lower.tail = FALSE)[1] 0.03515625\nggplot() +\n  stat_function(fun = function(x) dbinom(x = x, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\nggplot() +\n  stat_function(fun = function(q) pbinom(q = q, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\nggplot() +\n  stat_function(fun = function(p) qbinom(p = p, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 10, limits = c(0, 1))\nrbinom(10, 8, 0.5) [1] 4 3 3 7 4 4 3 5 5 4"},{"path":"probability-and-hypothesis-testing.html","id":"but-how-much-better-understanding-effect-size-and-power-false-positives-and-false-negatives","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.3.4 But how much better? Understanding Effect Size and Power, False Positives and False Negatives","text":"decided abandon null hypothesis players equally good,\nequates 50% win-chance player. determined\nmuch better . much better need us \nreliably discard null hypothesis just 8 games? generalization \nmuch better part, true difference, called effect\nsize.ability decide something statistically significant \nfact true difference called statistical power. depends \neffect size, significance threshold \\(\\alpha\\) sample size \\(n\\) (\nnumber games). can explore concept another simulation.also introduced new piece advanced dplyr syntax. rowwise similar \ngroup_by essentially puts row group. can useful\nworking list columns running function varying arguments \nallows us treat inside mutate bit like using one \nmap functions. information, see documentation\narticle.leaves us 10000 simulated numbers wins N games different\ntrue probabilities winning (.e. much better friend ). \ncalculate probability greater number wins \nnull hypothesis (equal probability win loss), words: \np-value.notice couple things plot. number games\nplayed approaches high numbers, p-values \ncase null hypothesis fact true (players \nchance winning), start following uniform distribution,\nmeaning true null hypothesis, p-values equally likely.\nseems counterintuitive first, direct consequence\ndefinition p-value.\nconsequence , apply regular significance\nthreshold 5%, definition say true\ndifference, even though none (.e. null hypothesis true\nfalsely reject favor alternative hypothesis).\ncalled false positive. definition, get \nleast \\(\\alpha\\) false positives experiments.\nLater, learn, real number false positives \neven higher.\nAnother name false positives Type errors.side coin, also cases\ntrue difference (used winning probabilities\n0.8 0.9), don’t reject null hypothesis \nget p-values larger \\(alpha\\).\nfalse negatives rate sometimes\nreferred \\(\\beta\\).\nAnother name false negatives Type II errors.\nPeople don’t particularly like talking negative things like errors,\ninstead often see inverse \\(\\beta\\), \nStatistical Power \\(1-\\beta\\).\nproportion correctly identified positives \nactual positives also shown plot .\nexample, say true win probability 90%\nplay 8 games. experiment runs infinite\nnumber parallel universes, conclude \nbetter chance 80% .\nset significance threshold higher detect\ntrue positives, \nalso increase false positives.also packages , function\ncompute power binomial test, think\nsimulation way approachable.\ncool thing simulations also, work\neven analytical solution, \ncan use play around planning experiment.","code":"\nreps <- 10000\nsimulation <- crossing(\n  N = c(8, 100, 1000, 10000),\n  true_prob = c(0.5, 0.8, 0.9)\n) %>% \n  rowwise() %>% \n  mutate(\n    wins = list(rbinom(n = reps, size = N,  prob = true_prob)),\n  ) %>% \n  unnest(wins) %>% \n  mutate(\n    p = pbinom(q = wins - 1, size = N, prob = 0.5, lower.tail = FALSE)\n  )\n\nhead(simulation)\nsimulation %>%\n  ggplot(aes(p)) +\n  geom_histogram() +\n  facet_wrap(~ true_prob + N,\n             labeller = label_both,\n             scales = \"free_y\",\n             ncol = 4) +\n  geom_vline(xintercept = 0.05, color = \"red\") +\n  labs(x = \"p-value\",\n       y = \"frequency\") +\n  scale_y_continuous(breaks = NULL)\nsimulation %>%\n  group_by(true_prob, N) %>%\n  summarise(signif = mean(p <= 0.05)) %>% \n  ggplot(aes(true_prob, signif, fill = true_prob == 0.5)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(label = signif), vjust = -0.2) +\n  facet_wrap(~N,\n             labeller = label_both) +\n  scale_y_continuous(expand = expansion(c(0, 0.1))) +\n  scale_fill_viridis_d() +\n  labs(y = \"Proportion of significant results\")"},{"path":"probability-and-hypothesis-testing.html","id":"p-value-pitfalls","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.4 P-Value Pitfalls","text":"Let us look pitfalls p-values.\nRemember definition p-values, get\nsignificant result even true difference \n5% cases (assuming use alpha)?\nWell, test bunch things?\ncalled Multiple Testing problem\nassociated :test 20 different things, statistical\ntest produce significant result chance alone\n5% cases, expected number significant results 1.\nsurprised.\nSpeaking surprised: book, available free online,\n“Statistics done wrong”, Alex Reinhart\ndescribes p-values “measure surprise”:»p value measure right ,\nsignificant difference ;\n’s measure surprised actual difference\ngroups, got data suggesting .\nbigger difference, one backed data,\nsuggests surprise smaller p value.«\n— Alex Reinhart16So, surprised, focus hard \none significant result, trouble ensues.\n“publish perish” mentality, can easily happen,\nnegative findings published nearly enough,\npublished findings likely exaggerated.\nJohn Bohannon showcased beautifully \nrunning study chocolate consumption \ngetting published:\nFooled Millions Thinking Chocolate Helps Weight Loss. ’s .can ?","code":""},{"path":"probability-and-hypothesis-testing.html","id":"multiple-testing-correction","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.4.1 Multiple Testing Correction","text":"simplest approach take p-values calculate\nrunning large number comparisons \ndividing number tests performed.\ncalled Bonferroni correctionOf course, looses statistical power\n(remember, free lunch).\nslightly sophisticated approach \ncontrolling false discovery rate (FDR)\nBenjamini-Hochberg procedure. retains\nbit power. happens:Sort p-values ascending order.Choose FDR \\(q\\) willing accept\ncall number tests done \\(m\\).Find largest p-value :\n\\(p \\leq iq/m\\) index \\(\\).new threshold significanceScale p-values accordinglyAnd R:","code":"\np_values <- c(0.5, 0.05, 0.3, 0.0001, 0.003)\np.adjust(p_values, method = \"bonferroni\")[1] 1.0000 0.2500 1.0000 0.0005 0.0150\np.adjust(p_values, method = \"fdr\")[1] 0.50000000 0.08333333 0.37500000 0.00050000 0.00750000"},{"path":"probability-and-hypothesis-testing.html","id":"other-forms-of-p-hacking","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.4.2 Other forms of p-hacking","text":"sort multiple testing fairly obvious.\nnotice , end large\nnumber p-values, example genetic\nscreening testing thousands genes.\nrelated problems harder spot.\nsingle research question often different\nstatistical tests run, trying\nchoosing one best\nagrees hypothesis option!\nLikewise, simply looking data form \ncomparison influences choice statistical test.\nIdeally, first run exploratory experiments\nmeant test hypothesis, \ndecide tests need, sample size \nwant particular power run actual\nexperiments designed test hypothesis.point, another shout-\nAlex Reinharts book.17\npleasant read\nalso shines light \nforms p-hacking.","code":""},{"path":"probability-and-hypothesis-testing.html","id":"bayesian-statistics-and-the-base-rate-fallacy","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.5 Bayesian Statistics and the Base Rate Fallacy","text":"another subtle problem called \nbase rate fallacy. example, assume\nmedical test, testing certain condition.\nmedical testing, different words used\nconcepts defined above18., :Sensitivity = Power = true positive rate = \\(1-\\beta\\)Specificity = true negative rate = \\(1-\\alpha\\)Let us assume test \nsensitivity 90% specificity\n92%. visit doctor \nget test, get positive result,\nprobability, \nfact positive (.e. true positive)?\nWell, test specificity 92%,\nnegative, detected\n92% cases, mean, can\n92% certain, actually positive?Well, . ignoring \nbase rate, diseases called\nprevalence. proportion \ndisease exists general population., let us say, picking 1000 people\nrandom population testing\n. dealing hypothetical\ncondition affects 1% people,\nassume 10 people sample positive.\n10 people, 9 tested positive\n(due sensitivity),\ntrue positives.\nremaining 1 false negative.\nHowever, course also testing\nnegatives (knew ahead time\npoint testing) \ndue specificity, 8% also \ntested positive, 0.08 * 990, \nget 79 false positives.\nmany negatives \nsample, even relatively high specificity\nproduce lot false positives.\nactual probability \npositive positive test result \\[\\frac{true~positives}{true~positives + false~positives}=10\\%\\]Formally, described Bayes’s Formula\\[P(|B)=\\frac{P(B|)*P()}{P(B)}\\]Read: probability given B probability\nB given times probability divided \nprobability B.bayesian statistics, prevalences known\npriors.","code":""},{"path":"probability-and-hypothesis-testing.html","id":"concepts-discussed-today","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.6 Concepts discussed today","text":"today familiar following concepts:Null alternative hypothesisP-values statistical significanceBinomial distributionProbability density, probability quantile functionsEffect size statistical powerFalse positives, false negativesMultiple testing p-hackingBayes’s Theorem","code":""},{"path":"probability-and-hypothesis-testing.html","id":"exercises-4","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.7 Exercises","text":"","code":""},{"path":"probability-and-hypothesis-testing.html","id":"a-fair-coin","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.8 A Fair Coin","text":"regular old coin flip 100 times.\nGiven significance threshold \\(\\alpha\\) 0.05,\nprobability (mistakenly) reject null hypothesis .e.\nconclude coin fair even though ?\nCan show simulation?\ntip can tell due vectorized nature\nfunctions involved won’t need map loop.\nshortest version think uses 3 functions.","code":""},{"path":"probability-and-hypothesis-testing.html","id":"an-unfair-game","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.9 An Unfair Game","text":"playing game roll sixes order win.\nSomeone trying fool us using loaded die.\ndie manipulated chance rolling\nsix 35% instead usual 1/6.\nsignificance threshold , \nchance us rejecting null hypothesis (= fair die)\nthus concluding correctly tricked\nrolling die 20 times?\nrun simulation .","code":""},{"path":"probability-and-hypothesis-testing.html","id":"discovering-a-new-distribution","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.10 Discovering a new Distribution","text":"binomial distribution concerned sampling\nreplacement (can get head tails number\ntimes without using coin). exercise\nexplore sampling without replacement.\ncommon model urn two\ndifferent colored balls .\nresulting distribution called \nhypergeometric distribution \ncorresponding R functions <r/d/p/q>hyperImagine zoo manager.\ngot gift another zoo! consists\n8 red pandas 2 giant pandas.\nprobability end \nproperly separated, randomly take 8 animals,\nput one enclosure put rest another?\npenguin colony hatched eggs \nbunch newcomers. 15 males \n10 females. look random subset \n12 penguins, distribution \nnumber males look like? number \nlikely? likely , get least\n9 males sample?\ngot gift another zoo! consists\n8 red pandas 2 giant pandas.\nprobability end \nproperly separated, randomly take 8 animals,\nput one enclosure put rest another?penguin colony hatched eggs \nbunch newcomers. 15 males \n10 females. look random subset \n12 penguins, distribution \nnumber males look like? number \nlikely? likely , get least\n9 males sample?","code":""},{"path":"probability-and-hypothesis-testing.html","id":"resources-3","chapter":"Lesson 5 Probability and Hypothesis Testing","heading":"5.11 Resources","text":"https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108https://jimgruman.netlify.app/post/education-r/P-Value histograms blogpost David Robinson“Statistics done wrong”","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"distributions-summaries-and-dimensionality-reduction","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","text":"… explore continuous distributions spotify data,\nfind central limit theorem related statistical tests\nbecome N-dimensional whale sharks.turns lecture got quite long.\nseparated two small interludes \ncrucial bigger picture bonus video.\ncan recognize corresponding parts \nscript prefix [Sidenote] section heading.\ninteresting watch read \nrequired exercises.main lecture video:bonus bits:","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"some-preparation","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.1 Some Preparation","text":"Today, explore process modeling\nlook different types models.\npart, using tidymodels framework.\ntidymodels framework extends tidyverse\nspecialized tools kinds modeling tasks\nfit neatly tools\nalready know. Go ahead install :Now ready get started","code":"\ninstall.packages(\"tidymodels\")\nlibrary(tidyverse)\nlibrary(tidymodels)"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"sidenote-on-reproducible-environments-with-renv","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.1.1 [Sidenote] on Reproducible Environments with renv","text":"point, installed quite lot packages.\none hand, great fun extend \ncan make tedious tasks fun.\nhand, every package add introduces\ncalled dependency.\nuser doesn’t package installed,\nanalysis run.\nfeeling experimental use functions\npackages active development might\nchange future, run trouble\nupdate package.\nnever updating anything ever fun!\nshow , get best worlds:\npackages functions heart desires\nmaintaining complete reproducibility.\nmake sure can come back old\nprojects 2 years now still just run\ntime.solution package called renv.\nidea follows:\nInstead installing packages\none place, can one version\npackage time, renv installs packages\nlocally project folder.\nalso meticulously writes version numbers\npackages installed keeps cache,\ncopy version twice.R package like , first,\ninstall :, RStudio project R console,\ninitialize project use renv :couple things.\ncreates file named .Rprofile, \nwrites source(\"renv/activate.R\").\nR-profile file run automatically every\ntime start R session folder,\nmakes sure renv active every time\nopen project.\nalso creates folder called renv.\ninstall packages\nwant use project.\nimportant file renv.lock file.\ncan look , just text file\npackages exact versions.notice, initializing renv,\npackages, example \ncan’t load tidyverse usual.\ninstall !\nHowever, case fairly\nfast, renv knows \nalready installed globally \nsimply copies files,\nfast.\ninstalled new package,\ncall:Renv tells us, changed environment\nconfirm, notes changes.also really easy collaborate \npeople. send \nproject folder, run:install packages noted lockfile.\ncan also use installed\nmany packages update \nregret want go back\nwritten lockfile.Finally, renv also provides functions update\ninstall new packages. work like install.packages,\nbit versatile.\nexample, let show different\nlocations can install packages.main location CRAN\n(Comprehensive R Archive Network).\nalso installed R .\nR packages subject certain standards\nusually stable tested.can also install packages directly source\ncode people uploaded.\nGitHub platform \ncan upload code track changes .\nlot times, can find current developement\nversion R package, packages \nyet CRAN GitHub.renv can install packages GitHub well,\nexample let us say, want test \nlatest version purrr package give feedback\ndevelopers.https://github.com/tidyverse/purrr says:Well, don’t need devtools , renv can\nregular install function:Giving just package name installs package CRAN,\npattern \"username/packgename\" installs GitHub.\nNow, back actual topic today!initialized renv need install\npackages need project even\nalready global package cache,\njust renv knows .","code":"\ninstall.packages(\"renv\")\nrenv::init()\nrenv::snapshot()\nrenv::restore()\n# ...\n# Or the the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"tidyverse/purrr\")\nrenv::install(\"tidyverse/purrr\")"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"all-models-are-wrong-but-some-are-useful","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.2 All models are wrong, but some are useful","text":"“models wrong, useful”\n— George BoxWhat means model simplification\nreality must always omit details.\nmodel can depict complete underlying\nreality. However, models useful, \nunderstand useful , must first look \ndifferent types models .","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"types-of-models","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.2.1 Types of Models","text":"tidymodels book\nnames three types models,\nparticular model can fall multiple\ncategories :Descriptive Models \npurely used describe underlying\ndata make patters easier see.\nadd smooth line ggplot\ngeom_smooth, default method called LOESS curve,\nstands Locally Estimated Scatterplot Smoothing.\nproduce insights revealing patterns us,\ncan used e.g. make predictions.\njust pretty looking smooth line.Inferential Models \ndesigned test hypothesis \nmake decisions. rely heavily \nassumptions data\n(e.g. probability distribution\npopulations follows) \nlikely encountered \nanswer research questions.\nmodels typically\nproduce p-value, compare\nthreshold like last week.Inferential Models \ndesigned test hypothesis \nmake decisions. rely heavily \nassumptions data\n(e.g. probability distribution\npopulations follows) \nlikely encountered \nanswer research questions.\nmodels typically\nproduce p-value, compare\nthreshold like last week.Predictive Models \ndesigned process data \nmake predictions \nresponse variable upon receiving\nnew data. done correctly,\nalso hold data\nmodel never gets see,\ntime evaluate \ntest performs unseen data.\nDepending much know\n(want know) \nunderlying processes, differentiate\nmechanistic models like\nfitting physically meaningful\nfunction data empirically driven models, mainly\nconcerned creating good\npredictions, matter underlying\nmechanism.Predictive Models \ndesigned process data \nmake predictions \nresponse variable upon receiving\nnew data. done correctly,\nalso hold data\nmodel never gets see,\ntime evaluate \ntest performs unseen data.\nDepending much know\n(want know) \nunderlying processes, differentiate\nmechanistic models like\nfitting physically meaningful\nfunction data empirically driven models, mainly\nconcerned creating good\npredictions, matter underlying\nmechanism.now explore different examples.\nFirst, let introduce dataset today:","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"say-hello-to-spotify-data","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.3 Say Hello to Spotify Data","text":"created playlist spotify,\nquite diverse can\nlook range features. can\neven listen \nexercises want. , write\n. cool thing spotify ,\nAPI, Application Interface. APIs ways \ncomputer programs talk .\nuse spotify app look songs, computers\nuse API talk spotify server.\nR rich ecosystem packages,\nsomeone already wrote package allows R talk \nAPI: spotifyr.check R folder lecture,\ncan see downloaded processed data \nplaylist. Note script work \nright away, first need register\nspotify developer get called token,\nlike username password one long text, allowed\nsend bots way.\nprobably just want download data \ngithub repository.Let’s look, shall ?can get quick overview columns :Finally decent numbers!\njust measly discrete values\nlast week.\nsong playlist, get artist,\nyear arrived number features like\ndanceable, loud fast song .\ncan easily imagine spotify using \nnumbers suggest new songs based features\nlistened .\nfact, going lay foundations\nalgorithm today.","code":"\nsongs <- read_csv(\"data/06/spotify_playlist.csv\")\nglimpse(songs)Rows: 393\nColumns: 18\n$ track_name        <chr> \"Africa\", \"Take on Me\", \"Wake Me Up Before You Go-Go…\n$ track_artists     <chr> \"TOTO\", \"a-ha\", \"Wham!\", \"Elton John\", \"The HU;Jacob…\n$ danceability      <dbl> 0.671, 0.573, 0.620, 0.504, 0.373, 0.624, 0.789, 0.7…\n$ energy            <dbl> 0.373, 0.902, 0.573, 0.904, 0.895, 0.857, 0.789, 0.6…\n$ key               <dbl> 9, 6, 0, 6, 8, 10, 2, 4, 4, 6, 6, 4, 4, 11, 10, 10, …\n$ loudness          <dbl> -18.064, -7.638, -11.893, -6.863, -3.846, -6.250, -4…\n$ mode              <dbl> 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1…\n$ speechiness       <dbl> 0.0323, 0.0540, 0.0423, 0.1790, 0.0610, 0.0542, 0.29…\n$ acousticness      <dbl> 0.257000, 0.018000, 0.271000, 0.356000, 0.022100, 0.…\n$ instrumentalness  <dbl> 8.01e-05, 1.25e-03, 0.00e+00, 1.21e-01, 2.45e-04, 2.…\n$ liveness          <dbl> 0.0481, 0.0928, 0.0607, 0.1400, 0.6610, 0.1100, 0.09…\n$ valence           <dbl> 0.7320, 0.8760, 0.8970, 0.7720, 0.6680, 0.3240, 0.37…\n$ tempo             <dbl> 92.718, 84.412, 81.548, 176.808, 172.392, 131.926, 1…\n$ time_signature    <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ track_duration_ms <dbl> 295893, 225280, 231333, 183440, 317077, 282920, 2480…\n$ track_popularity  <dbl> 82, 84, 79, 80, 57, 60, 74, 84, 84, 82, 82, 81, 81, …\n$ track_uri         <chr> \"spotify:track:2374M0fQpWi3dLnB54qaLX\", \"spotify:tra…\n$ track_year        <dbl> 1982, 1985, 1984, 1983, 2020, 2019, 2019, 2015, 2015…"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"visualising-continuous-distributions","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.4 Visualising Continuous Distributions","text":"dealing continuous distribution,\nlike lot features spotify\nsongs dataset, always multiple\nways represent data.\nFirst, just look numbers. \nuse valence values songs:Notice anything interesting numbers?\ndon’t either.\nbrain way better suited looking graphical representations,\n:\nggplot cave!kind hard see, points overlap.\ncan get better picture distribution\nusing transparency bit jitter:Using histogram, can put points bins\nget plot similar got discrete values.\nNote plot flipped ’s side now.might want play around bin size \nget better feel distribution.\nAnother way apply smoothing function\nestimate density points along continuous\nrange, even places originally points:plots can misleading, original\nnumber points quite small, cases,\nbetter , showing actual individual\npoints well. reason, first plots\nvertical, cool way\nshowing points distribution,\nstill space show multiple\ndistributions next .\nImagine taking density plot, turning 90 degrees\nmirroring middle.\nget called violin plot.\noverlay points top, use something\nlittle predictable jitter time:\nggbeeswarm package present: geom_quasirandom.cool, now can easily\ncompare two different distributions\nnext still see individual\npoints.\nexample, might ask:“songs major cord higher valence \nsongs minor cord dataset?”Note: jittering works, \nfeature x-axis discrete. continuous,\nchanging data jittering x-axis.might also want add summaries like mean \ngroup plot additional marker.\nleads us general concept summary statistics.\nnumber , can quite useful\n, well, summarise complex distribution.\ncan also misleading, can simplification .","code":"\nhead(songs$valence)[1] 0.732 0.876 0.897 0.772 0.668 0.324\nsongs %>% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_point()\nsongs %>% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_jitter(width = 0.1)\nsongs %>% \n  ggplot(aes(valence)) +\n  geom_histogram()\nsongs %>% \n  ggplot(aes(valence)) +\n  geom_density(fill = \"midnightblue\", alpha = 0.6)\nsongs %>% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_violin(fill = \"midnightblue\", alpha = 0.6) +\n  ggbeeswarm::geom_quasirandom(width = 0.35)\nsongs %>% \n  filter(!is.na(mode), !is.na(valence)) %>% \n  ggplot(aes(x = factor(mode), y = valence)) +\n  geom_violin(fill = \"midnightblue\", alpha = 0.6) +\n  ggbeeswarm::geom_quasirandom(width = 0.35) +\n  scale_x_discrete(labels = c(`0` = \"minor\", `1` = \"major\"))"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"summary-statistics","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.5 Summary Statistics…","text":"","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"mean-median-and-other-quartiles-range","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.6 Mean, Median (and other Quartiles), Range","text":"Let us start considering different things \ncan say distribution one number.\nFirst, might look range numbers,\nmaximum minimum.\nper mode, can compare values.\nNext, want know centers points.\ndifferent notions center\ndistribution.\nmean average sum values\ndivided number values.\nmedian call quantile, point \ndivides distribution equally sized parts,\nspecifically 50% values 50%\nmedian.appears valence can assume values 0 1.\nshortcut range function:median just one many percentiles\ncan think . display 50th \nwell 25th 75th percentile one plot,\nget called boxplot:“whiskers” box extend 1.5 times box size \nlast data point, whichever makes smaller whiskers.\nPoints extreme whiskers \nlabeled outliers boxplot usually displayed \npoints. Like violin plot,\nalso option plot original un-summarized\npoints top. case, need make sure\nchange outlier color boxplot \nNA, otherwise plotting twice.hints one downside boxplots\n(used without adding raw datapoints well):\nbox prominent focus point\nplot, definition,\ncontains 50% datapoints.\nrest delegated thin whiskers.","code":"\nsongs %>% \n  drop_na(valence, mode) %>% \n  group_by(mode) %>% \n  summarise(\n    min = min(valence),\n    max = max(valence),\n    mean = mean(valence),\n    median = median(valence)\n  )\nrange(songs$valence)[1] NA NA\n# in the lecture I used filter(!is.na(mode), !is.na(valence)),\n# but drop_na is more elegant.\n\nsongs %>% \n  drop_na(valence, mode) %>% \n  ggplot(aes(x = factor(mode), y = valence)) +\n  geom_boxplot(fill = \"midnightblue\", alpha = 0.6, outlier.color = NA) +\n  ggbeeswarm::geom_quasirandom(width = 0.35) +\n  scale_x_discrete(labels = c(`0` = \"minor\", `1` = \"major\"))"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"variance","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.6.1 Variance","text":"Finally, want know, far values\nscatter around means potential\npopulation mean. encompassed\ntwo closely related measures: variance\nstandard deviation.illustrative purposes, can plot datapoints\ne.g valence order appear data\nadd line mean.variance expected value squared deviation\nrandom variable mean.words: Take distance points\nmean sum (add red lines plot\ntogether) divide \\(n-1\\).\\[var(X) = \\frac{\\sum_{=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]“Hang !” hear saying: “\\(n-1\\)?”\nexcellent question. first\nstatement talked expected value.\n(One example expected value mean,\nexpected value … well, values).\nindeed, expected value often \nterm \\(1/n\\). statement talking\nexpected value (squared deviation)\nwhole population.\ncan use uncorrected version \nwhole population (e.g. songs ever existed)\nwant talk population.\nusually, sample, \nwant draw conclusions population.\nusing sample estimate\nvariance population, biased.\ncan correct bias using \\(n-1\\) instead\n\\(n\\).\nknown Bessel’s correction.\nyet come really intuitive explanation,\none idea: thing dividing\nnecessarily sample size time\nwant try calculate expected\nvalue estimator, just happens \nsample size bunch cases.\nterm really represents \ndegrees freedom (DF) deviations.\nDFs can thought number independent things.\ndegrees freedom \\(n\\) reduced \\(1\\), \nknow mean sample (use \ncalculation), know \\(1\\)\nindividual values, last value automatically\nknown thus doesn’t count towards degrees freedom.","code":"\nsongs %>%\n  filter(!is.na(valence)) %>% \n  mutate(index = 1:n()) %>% \n  ggplot(aes(index, valence)) +\n  geom_segment(aes(y = mean(valence),\n                   yend = mean(valence),\n                   x = 0,\n                   xend = length(valence))) +\n  geom_segment(aes(xend = index, yend = mean(valence)),\n               color = \"darkred\", alpha = 0.6) +\n  annotate(x = length(songs$valence),\n           y = mean(songs$valence, na.rm = TRUE),\n           label = \"Mean\",\n           geom = \"text\") +\n  geom_point()"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"standard-deviation","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.6.2 Standard deviation","text":"Next : Standard Deviation (SD) square root\nvariance. commonly used error\nbars, square root inverts squaring \ndone get variance. back\ndimensions data.\\[\\sigma_X=\\sqrt{var(X)}\\]","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"standard-error-of-the-mean","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.6.3 Standard Error of the Mean","text":"Finally, Standard Error Mean,\nsometimes called Standard Error (SEM, SE).\nalso used commonly error bars.\nreason lot people favor \nSD might just , smaller,\ndistinct use-cases.\\[SEM=\\sigma / \\sqrt{n}\\]take standard deviation divide \nsquare-root \\(n\\). Imagine :\nactually whole population available.\nLike example penguins earth.\nrepeatedly take samples \nsize \\(n\\). means individual samples\nvary, ’s mean,\nstandard deviation variance. \nstandard error standard deviation means.\nmeasure far means repeated samples\nscatter around true population mean.\nHowever, don’t usually whole population!\nMeasuring property penguins \nworld takes long time, running\nexperiment lab cells exist\never exist takes infinite amount \ntime. probably research grant\nmoney can finance.\n, instead, Standard Error Mean\nused standard deviation sample \nformula . best estimate\nstandard deviation whole population.\n, trying make inferences\nmean whole population based sample,\nmakes sense also give SEM way \nquantifying uncertainty.R functions sd, mean var,\nbuilt function sem,\ncan easily write one :","code":"\nsem <- function(x) sd(x) / sqrt(length(x))\nsongs %>% \n  drop_na(mode, valence) %>% \n  group_by(mode) %>% \n  summarise(\n    mean = mean(valence),\n    var = var(valence),\n    sd = sd(valence),\n    sem = sem(valence)\n  )"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"or-how-to-lie-with-graphs","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.7 … or: How to Lie with Graphs","text":"However, wary simple bar graphs error bars;\nlot can misleading .people say “y-axis include 0,”\nreason . always true,\nanother sensible baseline 0,\nespecially barplots y-axis start\n0 misleading thing can .\nmain reason humans perceive\nheight bars via area,\nlonger proportional bars\ndon’t start 0.\nplot also makes indication type \nerror-bars used sample size group.\nuses speechiness feature, hides \nactual distribution behind just 2 numbers\n(mean SEM) per group.next time see barplot ask question:Summary statistics Horst19I hope can take inspiration \nchapter now vocabulary \nknow look comes data.","code":"\nsongs %>% \n  drop_na(speechiness, mode) %>% \n  group_by(mode) %>%\n  summarise(across(speechiness, list(m = mean, sd = sd, sem = sem))) %>% \n  ggplot(aes(factor(mode), speechiness_m, fill = factor(mode))) +\n  geom_errorbar(aes(ymin = speechiness_m - speechiness_sem,\n                    ymax = speechiness_m + speechiness_sem,\n                    color = factor(mode)),\n                size = 1.3, width = 0.3, show.legend = FALSE) +\n  geom_col(size = 1.3, show.legend = FALSE) +\n  coord_cartesian(ylim = c(0.06, 0.08)) +\n  scale_fill_manual(values = c(\"#1f6293\", \"#323232\")) +\n  scale_color_manual(values = c(\"#1f6293\", \"#323232\")) +\n  labs(title = \"Don't Do This at Home!\",\n       y = \"Speechiness\",\n       x = \"Mode (Minor / Major)\") +\n  theme(\n    plot.title = element_text(size = 44, family = \"Daubmark\",\n                              color = \"darkred\")\n  )\nsongs %>% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"graphic-devices-fonts-and-the-ggplot-book","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.8 Graphic Devices, Fonts and the ggplot Book","text":"","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"ggplot-book","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.8.1 ggplot book","text":"Firstly, things ggplot, third edition\nggplot book currently worked \nthree absolute legends craft.20\nHadley Wickham author original ggplot \nggplot2 package, Danielle Navaro\nmakes amazing artwork\nteaches ggplot \nThomas Lin Pedersen\ncurrent maintainer ggplot2 constantly\nmakes cool features .\n-development book already available online\nfree: https://ggplot2-book.org/.","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"sidenote-graphics-devices","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.8.2 [Sidenote] Graphics Devices","text":"Secondly, need briefly talk concept\nbrushed : graphics devices \nR printer computer.\ncreate plot R, starts mere numbers,\nsomething turn numbers \npixels (case raster-images) vectors\n(case vector images; might know svg pdf files.\nSorry, vectors R rather\ndescriptions lines).\njob ob graphics device.\nuse ggsave function example,\nfigures use based file extension,\ncan also specify manually.\nmentioning , \nplot just showed , used different font\ndefault. something can \nincredibly tricky graphics devices,\nfonts handled differently every operating\nsystem. Luckily, get way easier,\nThomas Lin Pedersen working another\npackage, graphics device, really\nfast works well fonts.\ncan check current development version :\nhttps://ragg.r-lib.org/examples using graphics devices manually\nopening device first finalizing plot\nclosing device:manually specifying device ggsave.","code":"\npng(\"myplot.png\")\n\nsongs %>% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\ndev.off()\nsvg(\"myplot.svg\")\n\nsongs %>% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\ndev.off()\nplt <- songs %>% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\nggsave(\"newplot.png\", plt, device = ragg::agg_png)"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"the-normal-distribution-and-the-central-limit-theorem","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.9 The Normal Distribution and the Central Limit Theorem","text":"many different distributions .\nLuckily, one quite special can\nused multitude settings.\nharmlessly named Normal Distribution.\nR usual functions (density,\nprobability, quantile, random).Now, distribution special?Central Limit Theorem (CLT) states \nsample mean sufficiently large number\nindependent random variables approximately\nnormally distributed.\nlarger sample, better approximation.great visualization central limit\ntheorem, check interactive tutorial \nSeeing Theory.lot values measure actually \nsum many random processes, distributions things\nmeasure can often approximated normal distribution.can visually test values follow normal\ndistribution using quantile-quantile plot,\nplots quantiles sample \nquantiles normal distribution.\nstraight line means perfectly normal.values close mean pretty normal,\ntails distribution stray \nnormal distribution. way \nsmall large values \nexpected normal distribution.","code":"\ntibble(x = seq(-3, 3, 0.01)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = dnorm) +\n  stat_function(geom = \"area\", fun = dnorm,\n              fill = \"darkblue\", alpha = 0.3) +\n  labs(y = \"density\", title = \"Normal Distribution Density\")\n\ntibble(x = seq(-3, 3, 0.01)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = pnorm) +\n  labs(y = \"probability\", title = \"Cummulative Probability\")\nqqnorm(songs$valence)\nqqline(songs$valence, col = \"red\")"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"log-normality","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.9.1 Log-normality","text":"one thing comes lot biological data:\nlot processes biology reliant \nsignal cascades, tend result many\nmultiplicative effects, rather additive effects,\nrequired Central Limit Theorem.\nresult, distributed normally,\nrather log-normally,\ntaking logarithm values\ntransforms multiplicative effects additive effects!","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"the-t-distribution","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.10 The T-Distribution","text":"CLT valid large sample sizes.\nsmaller sample sizes, distribution \nmeans fatter tails normal distribution.\nstatistical tests,\nuse t-distribution instead \nnormal distribution.\ndegrees freedom get higher, \nt-distribution approaches normal distribution.\nFigure 6.1: t-distributions; normal distribution black.\nRemember valence plot mode?purposes going treat two distributions\nclose enough normal distribution first\ncan look hypothesis tests:","code":"\nbase <- ggplot() + xlim(-5, 5)\n\nbase +\n  geom_function(aes(colour = \"t, df = 1\"), fun = dt, args = list(df = 1), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 3\"), fun = dt, args = list(df = 3), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 30\"), fun = dt, args = list(df = 30), size = 1.2) +\n  geom_function(aes(colour = \"normal\"), fun = dnorm, size = 1.2) +\n  guides(color = guide_legend(title = \"\")) +\n  scale_color_viridis_d()\nsongs %>% \n  ggplot(aes(factor(mode), valence)) +\n  geom_violin(fill = \"darkblue\", alpha = 0.3) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6)"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"students-t-test","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.11 Student’s T-Test","text":"first test called student’s t-test. “Student”\npseudonym ’s inventor. “t” stands\nt-distribution. can use test\nnull hypothesis, two samples come \n(approximately normal) distribution.two samples similar quite likely \nvalues come form distribution, \nreject null hypothesis.Let us pretend moment fact \ndifference creating fake data (don’t lab…).Now end statistically significant p-value.Note, p-value says nothing effect size,\ndifference means sample.\ncan get significant p-value either showing tiny\ndifference lot’s data points showing larger\ndifference less data points.Tests, rely assumption normality\ncalled parametric tests, \nassumption can met, need non-parametric tests.","code":"\nt.test(valence ~ mode, data = songs)\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -1.0857, df = 365.99, p-value = 0.2783\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.07614235  0.02197185\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.4751015 \nfake_songs <- songs %>% \n  drop_na(mode, valence) %>% \n  mutate(valence = if_else(mode == 1, valence + 0.2, valence))\nfake_songs %>% \n  ggplot(aes(valence, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\nt.test(valence ~ mode, data = fake_songs)\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -9.1028, df = 365.99, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2761424 -0.1780282\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.6751015 "},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"wilcoxon-rank-sum-test","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.12 Wilcoxon rank-sum test","text":"Wilcoxon rank-sum test, \nMann–Whitney U test, one .\nget’s around assumption normality \ntransforming data ranks first.\n.e. points (independent group) \nordered values replaced \nposition ordering (rank).\nthink t-test testing \ndifference means, can think \nWilcoxon rank-sum test testing difference\nmedians.","code":"\nx <- c(1, 3, 2, 42, 5, 1000)\nx[1]    1    3    2   42    5 1000\nrank(x)[1] 1 3 2 5 4 6\nwilcox.test(valence ~ mode, data = songs)\n    Wilcoxon rank sum test with continuity correction\n\ndata:  valence by mode\nW = 15834, p-value = 0.3155\nalternative hypothesis: true location shift is not equal to 0"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"direction-of-testing","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.12.1 Direction of Testing","text":"tests argument alternative,\ncan c(\"two.sided\", \"less\", \"greater\").\ndirection alternative hypothesis.\ntesting, x greater less y?\ntesting difference direction (default)?\nhypothesis direction beforehand \nresult smaller p-values (half two-sided ones),\nneed hypothesis looking \ndata, especially running e.g. \ntwo sided test deciding, want \nsmaller p-value! p-values work.unsure tell functions,\ntwo groups supposed greater lesser,\ncan also supply data x y instead\nusing formula interface :save result test, can inspect object\nextract information .","code":"\nt.test(valence ~ mode, data = fake_songs, alternative = \"less\")\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -9.1028, df = 365.99, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is less than 0\n95 percent confidence interval:\n       -Inf -0.1859473\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.6751015 \nfake_songs %>% \n  ggplot(aes(valence, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\nvalence_major <- fake_songs %>% filter(mode == 1) %>% pull(valence)\nvalence_minor <- fake_songs %>% filter(mode == 0) %>% pull(valence)\n\nt.test(valence_major, valence_minor, alternative = \"greater\")\n    Welch Two Sample t-test\n\ndata:  valence_major and valence_minor\nt = 9.1028, df = 365.99, p-value < 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.1859473       Inf\nsample estimates:\nmean of x mean of y \n0.6751015 0.4480163 \ntest <- t.test(valence_major, valence_minor, alternative = \"greater\")\ntest$p.value[1] 2.893211e-18"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"confidence-intervals","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.12.2 Confidence Intervals","text":"t.test lonely sample can also used create confidence intervals\naround mean. short example 95% confidence\ninterval range expect mean\nsample fall 95% cases repeat\nexperiment infinite amount times.\nconfidence intervals also sometimes\nused error bars plots.","code":"\ntest <- t.test(songs$valence)\ntest$conf.int[1] 0.4377443 0.4871400\nattr(,\"conf.level\")\n[1] 0.95"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"chrunching-dimensions-with-dimensionality-reduction-pca","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.13 Chrunching Dimensions with Dimensionality Reduction: PCA","text":"Lastly today, going bit scope.\nleaving realm looking individual\nfeatures try condense information\nlittle space possible.general notion Dimensionality Reduction \ntake features construct new\nfeatures , can represent data\nfewer features loosing little information.example, two features highly correlated\n.e. one changes ,\nmight better replacing single\nnew feature, goes along\naxis maximum variance two.\nnumber along line accounts \nvariance points, rest can\naccounted number describing distance\nline (perpendicular axis), \nless important first axis found.Imagine whale shark!Whale shark Horst21And want orient mouth way\ncan eat greatest amount krill\none sweep.Krill Horst22This first principal component. \nsecond perpendicular first.\nthrowback “Math Natural Scientists” linear\nalgebra, defining new coordinate system .whale sharks swim 3 dimensions, 2,\ndata even dimensions, features\nrepresented dimensions.can quite hard humans imaging \nN-dimensional whale shark.R tidymodels us covered.PCA model , rather data preprocessing\nstep generates new features (principal components),\ncan later use models.\ntoday, just preprocessing .tidymodels, preprocessing done defining \nrecipe:take recipe prepare .can now explore, data looks like \nnew dimensions. , baking \nprepared recipe.original features replace Principal Components\nexplain variance.\nprepared recipe, extract tidy form\nstep care (usually last one)\nsee, happened data.\ncan see, features ended contributing\ncomponents getting\nresults pca step recipe.Let’s make plot!use 2 little helper functions \ntidytext package properly order bar.\nfirst component largely comprised high\nacousticness instrumentalness less energy positive direction.\nexpect e.g. classical music high axis.\nhigh value second component means high danceability\nlow tempo.songs end principle component space?can now imagine, using simpler representation \nsongs principal component space, example\npropose new songs users based songs close \nsongs listened representation.Lastly, want stress, principal components \ncreated equal. first component always \nimportant.\n, see, almost 27% variance can explained\njust first component, exploring\n2 really makes little sense .","code":"\nsongs %>% \n  ggplot(aes(x = energy,\n             y = loudness,\n             label = track_name)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nsongs_rec <- recipe( ~ ., data = songs) %>% \n  update_role(track_name, track_artists, track_uri, new_role = \"id variable\") %>% \n  step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nsongs_recData Recipe\n\nInputs:\n\n        role #variables\n id variable          3\n   predictor         15\n\nOperations:\n\nRemoving rows with NA values in all_predictors()\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\nsongs_prep <- prep(songs_rec)\nsongs_prepData Recipe\n\nInputs:\n\n        role #variables\n id variable          3\n   predictor         15\n\nTraining data contained 393 data points and 25 incomplete rows. \n\nOperations:\n\nRemoving rows with NA values in all_predictors()\nCentering and scaling for danceability, energy, key, loudness, ... [trained]\nPCA extraction with danceability, energy, key, loudness, ... [trained]\nsongs_baked <- bake(songs_prep, songs)\nsongs_baked\nterms <- tidy(songs_prep, id = \"pca\") %>% \n  mutate(component = parse_number(component))\n\nterms\ncolors <- fishualize::fish_pal(option = \"Centropyge_loricula\")(5)[3:4]\n\nterms %>% \n  filter(component <= 3) %>% \n  mutate(terms = tidytext::reorder_within(terms, by = value, within = component)) %>% \n  ggplot(aes(value, terms, fill = factor(sign(value)))) +\n  geom_col() +\n  scale_fill_manual(values = colors) +\n  facet_wrap(~component, labeller = label_both, scales = \"free\") +\n  tidytext::scale_y_reordered() +\n  guides(fill = \"none\")\nplt <- songs_baked %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(text = paste(track_name, \",\", track_artists)))\n\nplotly::ggplotly(plt)\ntidy(songs_prep)\ntibble(\n  sdev = songs_prep$steps[[3]]$res$sdev,\n  explained_variance = sdev^2 / sum(sdev^2),\n  pc = 1:length(sdev)\n) %>% \n  ggplot(aes(pc, explained_variance)) +\n  geom_col(fill = \"darkgreen\") +\n  geom_text(aes(label = scales::percent_format()(explained_variance)),\n            size = 3,\n            vjust = 1.1,\n            color = \"white\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x = NULL, y = \"Percent variance explained by each PCA component\")"},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"exercises-5","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.14 Exercises","text":"tidytuesday project also spotify\ndataset. one es even interesting,\nranges across different playlists\nvarious genres annotated said genres.\ndata (30000 songs)!\nDownload :https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"the-plotty-horror-picture-show","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.14.1 The Plotty Horror Picture Show","text":"Sometimes experience true horror\nsee light darkness. Take spotify data\nmake plot truly horrible!\nappreciate couple sentences thought process\nmakes plot particularly bad.\ncan strike terror reader’s heart multiple\nways. ideas, mix match suits :Make really ugly experimenting different theme options.Make really misleading defying viewer expectations\nbreaking norms. artist now,\nnorms don’t apply art.even axis labels?Experiment different (manual) color schemes!\nUsing Red Green excellent choice \nwant make sure plot unreadable every 12th man\n(due high prevalence red-green-blindness).Try different geoms combinations aesthetics,\nmaybe find ones worst possible choice\nfeatures.","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"take-a-sad-plot-and-make-it-better","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.14.2 Take a Sad Plot and Make it Better","text":"title exercise stolen \ntalk\nAlison Hill.Now use learned make great plot!\nPick features interested \nvisualize informative beautiful\npossible, still staying honest data.\nMaybe interested changes time,\nmaybe find favorite artist want situate\ncontext works.\nMaybe want explore different features relate\neven want attempt \nrecreate PCA see, can find clusters\ngenres. call.curious see, come !","code":""},{"path":"distributions-summaries-and-dimensionality-reduction.html","id":"resources-4","chapter":"Lesson 6 Distributions, Summaries and Dimensionality Reduction","heading":"6.15 Resources","text":"Tidymodels websiteTidymodels bookggplot bookragg graphics device","code":""},{"path":"fallacies-correlation-and-regression.html","id":"fallacies-correlation-and-regression","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"Lesson 7 Fallacies, Correlation and Regression","text":"… hear Stories Warplanes,\nCorrelation Regression explore Datasaurus Dozen.","code":""},{"path":"fallacies-correlation-and-regression.html","id":"setup","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.1 Setup","text":"","code":"\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(broom)"},{"path":"fallacies-correlation-and-regression.html","id":"data-considerations","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.2 Data Considerations","text":"","code":""},{"path":"fallacies-correlation-and-regression.html","id":"section","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.2.1 1943","text":"1943. second World War well underway, ravaging large parts Europe.\nMilitary aircraft first entered stage World War now\nreaching peak importance rain fire skies. Allied\nforces facing problem. warplanes get better, anti-aircraft\nsystems. effort improve survival fleet, US military\nstarts examining planes returning skirmishes opposing forces.\ncharacterize pattern bullet holes metal hull, meticulously\nnoting hit plane sustained. resulting picture better\nsummarized modern, redrawn version .Figure Wikipedia.23After taking look data gathered, military ready rush\naction. improve endurance aircraft, plan \nreinforce parts plane often hit bullets. \nstronger wings sturdier body plane, think, surely pilots\ncome back missions safely. wrong.pilots luck. military also consulted Statistics\nResearch Group Columbia University. man named Abraham Wald worked . \nnow unclassified report “method estimating plane vulnerability based \ndamage survivors,” argued generals’ conclusion.24 Instead -hit parts planes,\nleast-hit parts reinforced.Cover “method estimating plane vulnerability based damage survivors” Wald25Instead -hit parts, least-hit parts reinforced.reason seemingly counterintuitive result now known \nsurvivorship bias. data collected contained survivors, \nplanes sustained damage severe enough hinder coming back\nmission. aircraft hit places simply didn’t\nmake back. Consequently, Wald advised reinforce engines fuel\ntanks.","code":""},{"path":"fallacies-correlation-and-regression.html","id":"thinking-further","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.2.2 Thinking further","text":"one multitude biases, specifically selection bias, \ninfluence quality inferences can draw available data.\nKeep mind, data objective never exists vacuum. \nalways context consider. way data collected just one .\nlot ideas seem obvious hindsight, incidentally another\nbias social psychologists call hindsight bias, can sometimes \nhard spot.common saying music better back days, old\nmusic still holds new stuff radio just sounds .\nWell, quite. also survivorship bias work. bad \nforgettable songs past just faded oblivion, never mentioned\n, songs people generally agreed good survived ravages\ntime unscathed.\nsimilar thing happens success general, just songs.\nask CEO high corporate ladder, millionaire, author \nbook reads “get rich,” sure witty anecdote \npersistence, brilliance, charisma got \nnow. seeing people just witty, just charismatic\neven just persistent simply lucky. people \ntell . takes whole lot courage admit ones\nsuccess based luck privilege.take back scientific context: planning \nexperiment lab, always ask whether data collection process can \nway biased towards trying show.leave :weird every time see image twitter ton retweets pic.twitter.com/VALAKdehePAnd cautionary tale jump straight back RStudio.","code":""},{"path":"fallacies-correlation-and-regression.html","id":"sidenotes","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.3 Sidenotes","text":"","code":""},{"path":"fallacies-correlation-and-regression.html","id":"glue-and-inline-r-code","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.3.1 Glue and Inline R Code","text":"Using paste create text \nvalues variables inserted can painful.glue package makes breeze.\nEverything inside curly braces text inside glue\nfunction evaluated regular R code,\nenabling us write text quite naturally:hope confused package ’s main function \nname.","code":"\nname <- \"Jannik\"\nage <- 26\ntext <- paste(name, \"is\", age, \"years old.\")\ntext[1] \"Jannik is 26 years old.\"\ntext <- glue(\"{name} is {age} years old.\")\ntextJannik is 26 years old.\nglue(\"{name} is {age + 10} years old.\")Jannik is 36 years old."},{"path":"fallacies-correlation-and-regression.html","id":"inline-r-code","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.3.2 Inline R code","text":"Using backtick followed letter r \ncan add results code right text sections\nRmarkdown reports:1 + 1 = 2.Jannik 26 years old.","code":""},{"path":"fallacies-correlation-and-regression.html","id":"best-practices","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.3.3 Best Practices","text":"Speaking careful.\none rule can give make data analysis secure:raw data sacred!\never modify save .even important ,\nexample, using excel preview csv file.\ncircumstances hit save button\nexcel looking raw data.\napproximately one-fifth genomic research papers containing\nerrors gene lists, excel converted genes\nSEPT2 (Septin 2) dates, can see .26\nBiologists since given renamed genes \ncommonly converted dates… point still stands.\ncaution course also necessary analyzing data\nR, just excel. read raw data \nsave processed version, create new file, even\nbetter, new folder . good convention example\ndivide data raw derived folder.","code":""},{"path":"fallacies-correlation-and-regression.html","id":"covariance-correlation-and-regression","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.4 Covariance, Correlation and Regression","text":"Source: https://xkcd.com/552/Last week, talked measure spread \nrandom variable called variance.\\[var(X) = \\frac{\\sum_{=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]Today, extending idea 2 random variables.\nnormal distribution common, using\ntwo normally distributed variables.also added lines \nmeans two random\nvariables. Maybe mentioned\nclearly earlier ,\ngeneral convention statistics random variables\nuppercase concrete values distribution \nletter lowercase.now get covariance X Y :\\[cov(X,Y)=\\text{E}\\left[(X-\\text{E}\\left[X\\right])(Y-\\text{E}\\left[Y\\right])\\right]\\]expected value \\(E[X]\\) just fancy way saying\nmean X.\nasses contribution individual points towards \ncovariance, can understand quite intuitively.\npoint higher x mean X higher\ny mean Y (top right quadrant) push covariance towards\npositive values. Likewise, point bottom left quadrant\nnegative differences X Y mean, cancel\nresult positive covariance.\nbottom right top left quadrants push towards negative\ncovariance. mix positive negative contributions \nresult covariance small absolute value.covariance one problem: weird units\n(X times Y) scale different depending random\nvariables.\nstandardize dividing standard\ndeviations get correlation coefficient:\\[cor(X,Y)=\\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}\\]can assume values -1 1. ’s full name \nPearson product-moment correlation coefficient, \npearsons R. can square get \\(R^2\\) (obviously),\nindicates strength correlation \nvalues 0 1 independent direction.\nmeet later.Let us apply knowledge new dataset.","code":"\nN <- 50\ndf <- tibble(\n  x = rnorm(N),\n  y = rnorm(N)\n)\n\nm_x <- mean(df$x)\nm_y <- mean(df$y)\n\nggplot(df, aes(x, y)) +\n  geom_vline(xintercept = m_x, alpha = 0.8, color = \"midnightblue\") +\n  geom_hline(yintercept = m_y, alpha = 0.8, color = \"midnightblue\") +\n  geom_point(fill = \"white\", color = \"black\")"},{"path":"fallacies-correlation-and-regression.html","id":"introducing-the-dataset","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.4.1 Introducing the Dataset","text":"dplyr package includes example dataset Star Wars\ncharacters. Unfortunately, created ago,\nbaby yoda, 87 characters present.guess baby yoda show now.Let’s look correlations:","code":"\nstarwars"},{"path":"fallacies-correlation-and-regression.html","id":"pearson-vs.-spearman-not-a-boxing-match","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.4.2 Pearson vs. Spearman (not a Boxing Match)","text":"compute pearsons correlation, use cor function R.\nInstead filtering NA, can use\nuse = \"complete.obs\" ignore rows missing values computation.first surprised correlation \nlow. talking height mass, \nassumed highly correlated.\nLet us look data see going .culprit! massive outlier,\nsenses word “massive.”\nLuckily, another method asses correlation.\nSpearman’s method resistant outliers,\ndata transformed ranks first,\nnegates massive effect outliers.Visually, points look like\nrank transformation:Apart cor, also cor.test, gives information.\nfancy, can use broom turn test output \ntidy format well.another way can specify features correlate.\ncorr also takes matrix data frame ’s x argument instead\nx y.\nend pairwise correlation coefficients\ncolumns dataframe.known correlation matrix, can create \ntwo features, long features numeric\n(, correlation 1,4 “cat” “dog?”).\nUnfortunately three numeric columns\nstarwars dataset, makes pretty boring\ncorrelation matrix.let’s look another built-dataset instead.\nmtcars data cars, like \nengine displacement miles per gallon.makes much interesting correlation matrix:working lot correlations, certainly\nworth checking corrr package tidymodels framework:functions make steps easier.give use access two different types plots box.","code":"\npearson <- cor(starwars$height, starwars$mass, use = \"complete.obs\")\npearson[1] 0.1338842\nlabel_text <- glue(\"Pearson correlation: {round(pearson, 2)}\")\n\njabba <- filter(starwars, str_detect(name, \"Jabba\"))\njabba_text <- list(x = 1100, y = 120)\n\nstarwars %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 500, y = 75, label = label_text,\n           hjust = 0) +\n  annotate(geom = \"curve\",\n           x = jabba_text$x, y = jabba_text$y,\n           xend = jabba$mass, yend = jabba$height,\n           curvature = .3,\n           arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\",\n           x = jabba_text$x,\n           y = jabba_text$y, label = \"Jabba the Hutt\",\n           hjust = 1.1) +\n  xlim(0, 1500) +\n  labs(x = \"mass [kg]\",\n       y = \"height [cm]\")\nspearman <- cor(starwars$height, starwars$mass,\n                use = \"complete.obs\", method = \"spearman\")\nspearman[1] 0.7516794\nlabel_text <- glue(\"Spearman rank correlation: {round(spearman, 2)}\")\n\nstarwars %>% \n  mutate(mass = rank(mass),\n         height = rank(height)) %>% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 0, y = 75, label = label_text,\n           hjust = 0) +\n  labs(x = \"rank(mass)\",\n       y = \"rank(height)\")\ncortest <- cor.test(starwars$mass, starwars$height,\n                # method = \"spearman\",\n                use = \"complete.obs\")\n\ncortest\n    Pearson's product-moment correlation\n\ndata:  starwars$mass and starwars$height\nt = 1.02, df = 57, p-value = 0.312\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1265364  0.3770395\nsample estimates:\n      cor \n0.1338842 \ntidy(cortest)\nstarwars %>%\n  select(where(is.numeric)) %>% \n  head()\nmtcars %>% \n  ggplot(aes(disp, mpg)) +\n  geom_point()\ncor(mtcars) %>% \n  as_tibble(rownames = \"feature\") %>% \n  pivot_longer(-feature) %>% \n  ggplot(aes(feature, name, fill = value)) +\n  geom_raster() +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"blue\", high = \"red\",\n                       mid = \"white\", midpoint = 0)\ncorrr::correlate(mtcars) %>% \n  corrr::stretch()\ncorrr::correlate(mtcars) %>% \n  corrr::rplot()\ncorrr::correlate(mtcars) %>% \n  corrr::network_plot()"},{"path":"fallacies-correlation-and-regression.html","id":"difference-to-linear-regression","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.4.3 Difference to Linear Regression","text":"Finally, linear regression \nrelated concept, correlation \nlinear regression quantify strength linear\nrelationship. However, key differences.\nfit linear model like:\\[y \\sim + x * b\\]error x. assume x something \nfixed, like temperature set experiment\ndosage used. Y hand random\nvariable. cov(X,Y) cor(X,Y), X Y random variables,\nusually things observed, set .correlation coefficient symmetrical translation-scale-invariant:\\[cor(X,Y)=cor(Y,X)\\]\\[cor(X,Y)=cor(X * +b,Y * c + d)\\]true linear models!Let us look example linear regression\nappropriate correlation.\ndata folder find IMDB ratings 10\nStar Wars movies (plus features).can fit linear model see production year\neffect rating.added gray segments called residuals.\nmakes linear regression work.\n’s full name Ordinary Least Squares squares \nquestion squares residuals, word least\nindicates squares minimized order find \nbest fit line.Looks like every year decreases estimated rating 0.03.One thing however correlation \nlinear regression, \\(R^2\\) value get\ncalculations:can interpret \\(R^2\\) fraction variance \nresponse variable y can explained \npredictor x.","code":"\nratings <- read_rds(\"data/07/starwars_movies.rds\")\nratings\nmodel <- lm(imdbRating ~ year, data = ratings)\n\naugment(model) %>% \n  ggplot(aes(year, imdbRating)) +\n  geom_smooth(method = \"lm\", alpha = 0.3, color = \"midnightblue\") +\n  geom_segment(aes(x = year, y = .fitted,\n                   xend = year, yend = imdbRating),\n               alpha = 0.4) +\n  geom_point()\nbroom::tidy(model)\nsummary(model)\nCall:\nlm(formula = imdbRating ~ year, data = ratings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1000 -0.2467  0.1261  0.3880  0.7913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 77.13043   28.29937   2.726   0.0260 *\nyear        -0.03478    0.01414  -2.460   0.0393 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6872 on 8 degrees of freedom\nMultiple R-squared:  0.4306,    Adjusted R-squared:  0.3595 \nF-statistic: 6.051 on 1 and 8 DF,  p-value: 0.03933"},{"path":"fallacies-correlation-and-regression.html","id":"non-linear-least-squares","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.5 Non-linear Least Squares","text":"far, properly dealt linear relationships\nnow time get non-linear.\ncreating mechanistically driven\npredictive model, formula \nwant adjust parameters fits data.Let’s take classical Michaelis-Menten-Kinetics\ndataset enzyme reaction rates included R.\nconvert \ndataframe tibble prints\nnicer:initial rate \\(v_0\\) \nenzymatic reaction measured\ncontrol sample treated\npuromycin different substrate\nconcentrations.\nevery concentration \ntwo replicates except \none missing replicate.Biochemistry studies, know\ncan express rate depending\nconcentration following\nformula:\\[rate=\\frac{(Vm * conc)}{(K + conc)}\\]make easier work , let’s\nturn function.Let’s pick arbitrary starting values.\nexample, see maximal velocity\naround 200.\nalso know K concentration half-maximal\nvelocity reached.geom_function expects function x anonymous function\nfirst argument values x-axis,\n.\nWell, bet can better guessing function!\nR can us linear least squares\nminimizing distance curve \ndatapoints.\njob nls function, stands \nNonlinear Least Squares.","code":"\npuromycin <- as_tibble(Puromycin)\npuromycin\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point()\nrate <- function(conc, Vm, K) {\n  Vm * conc / (K + conc)\n}\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ rate(conc = .x, Vm = 200, K = 0.2),\n                color = \"black\")"},{"path":"fallacies-correlation-and-regression.html","id":"one-model","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.5.1 One model","text":"Let’s look just “treated” data first.NlS needs starting values, use guess isn’t far .\ncompletely wrong, model doesn’t know direction \nmove parameters improve fit get error like :\nError nls(rate ~ rate(conc, Vm, K), data = puro, subset = state ==  : singular gradientFor special case, R also self-starting model. won’t go\nuseful general concept fitting\narbitry functions, can check SSmicmen model \nestimes starting values automatically.Additionally, nls takes argument subset, works\nlike dplyr verb filter can fit\nmodel subset data without create beforehand.use broom package display model parameters tidy tibble.base-R function predict can make new predictions\nbased model new data:can use function inside geom_function:alternatively create new dataset predictions\nbeforehand use geom_line:","code":"\ntreated <- filter(puromycin, state == \"treated\")\nmodel <- nls(rate ~ rate(conc, Vm, K),\n             data = treated,\n             start = list(Vm = 200, K = 0.3)\n             )\n\nmodelNonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: treated\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06\ntidy(model)\nhead(predict(model, newdata = list(conc = seq(0, 1, 0.01))))[1]  0.00000 28.69405 50.56602 67.79038 81.70621 93.18326\ntreated %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ predict(model, newdata = list(conc = .x)),\n                color = \"black\")\npredictions <- tibble(\n  conc = seq(0, 1, 0.01),\n  rate = predict(model, newdata = list(conc = conc))\n)\n\ntreated %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions, color = \"black\")\naugment(model) %>% \n  ggplot(aes(conc, .resid)) +\n  geom_point()"},{"path":"fallacies-correlation-and-regression.html","id":"multiple-models","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.5.2 Multiple models","text":"Now, want fit model states?\ncan resort back trusty purrr package like\nearlier lecture.start creating function takes\ndataframe fits model:nesting data (grouped state)\nlist column can map\nfunction dataset.\nget fitted parameters\nmap tidy function broom\nfitted models.Let’s inspect fitted parameters.plot fitted models two options.\nFirstly, generate predicted values\nnumber concentrations beforehand\nplot :use geom_smooth, can take “nls” method well.\njust need make sure pass correct arguments.\ncan confusing, specifying\nformula geom_smooth, always needs \nformula y ~ x, whereas normal nls \nearlier, specified variables terms \nactual names (rate conc).also need se = FALSE, default R \ntry plot confidence interval around fit-line\nlike linear model, nls doesn’t return one,\nget error.unfortunate thing method end \nfitting model twice, get estimated parameters\nlikes second time ggplot\ndisplay fitted lines. cases \nproblem, model computationally expensive.","code":"\nfit_micmen <- function(data) {\n  nls(rate ~ rate(conc, Vm, K),\n      data = data,\n      start = list(Vm = 200, K = 0.3)\n  ) \n}\nmodels <- puromycin %>% \n  group_by(state) %>% \n  nest() %>% \n  mutate(\n    model = map(data, fit_micmen),\n    params = map(model, tidy)\n  )\nmodels %>% \n  select(state, params) %>% \n  unnest(params) %>% \n  select(state, term, estimate) %>% \n  pivot_wider(names_from = term, values_from = estimate)\nmake_predicions <- function(model) {\n  tibble(\n    conc = seq(0, 1.2, 0.01),\n    rate = predict(model, newdata = list(conc = conc))\n  )\n}\n\npredictions <- models %>% \n  mutate(\n    preds = map(model, make_predicions)\n  ) %>% \n  select(state, preds) %>% \n  unnest(preds)\n\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions)\npuromycin %>% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_smooth(method = \"nls\",\n              formula = y ~ rate(conc = x, Vm, K),\n              method.args = list(start = list(Vm = 200, K = 0.3)),\n              se = FALSE\n              )"},{"path":"fallacies-correlation-and-regression.html","id":"excursion-a-weird-error-message","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.5.3 Excursion: A weird error message","text":"Finally, want take minute mention another approach\ntook earlier series fitting\nmany linear models show , unfortunately\nwork .first looks like everything fine.\ninside dplyr verb nls know\nlook columns rate conc \nfit, specifying data argument.\nHowever, fails unexpected way later try\nmake predictions one models:reason follows:\nnls fit model didn’t remember actual values\nrate conc, just made note columns\navailable data.\ndata passed explicitly \njust wrote columns available \nenvironment called, \ntime inside summarise.\nCheck data argument :just says parent.frame, meaning “environment around .”\nleft context summarise,\nlonger available, can’t find rate column.\nalways safer pass data\nexplicitly like approach worked.","code":"\nnewmodels <- puromycin %>% \n  group_by(state) %>% \n  summarise(\n    model = list(nls(rate ~ rate(conc, Vm, K),\n                start = list(Vm = 200, K = 0.3))\n    )\n  )\nmake_predicions(newmodels$model[[1]])Error: Obsolete data mask.\nx Too late to resolve `rate` after the end of `dplyr::summarise()`.\nℹ Did you save an object that uses `rate` lazily in a column in the `dplyr::summarise()` expression ?\nnewmodels$model[[1]]Nonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: parent.frame()\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06"},{"path":"fallacies-correlation-and-regression.html","id":"exercises-6","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.6 Exercises","text":"","code":""},{"path":"fallacies-correlation-and-regression.html","id":"the-datasaurus-dozen","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.6.1 The Datasaurus Dozen","text":"Datasaurus Dozen27 dataset\ncrafted illustrate certain concepts.\ncan accessed R via datasauRus package.Explore dataset looking publication\n(contains spoilers…):\nactually contains 13 different datasets,\ndenoted column dataset, one tibble.\nmeans x y different datasets?\nstandard deviations x y different datasets?\ncorrelations coefficients different datasets?\nbet notice pattern now.\nNow create one (multiple) scatterplots data.\nnotice? conclusions draw observation?\nactually contains 13 different datasets,\ndenoted column dataset, one tibble.\nmeans x y different datasets?\nstandard deviations x y different datasets?\ncorrelations coefficients different datasets?\nbet notice pattern now.Now create one (multiple) scatterplots data.\nnotice? conclusions draw observation?another dataset package illustrate different\npoint:First, turn tidy format, much like datasaurus_dozen\ntibble.Now, visualize distributions values \n5 different groups. Try different versions plot\nsatisfied, sure also include boxplot\ncompare approaches.\nfind?","code":"\ndatasauRus::datasaurus_dozen\ndatasauRus::box_plots"},{"path":"fallacies-correlation-and-regression.html","id":"fit-a-non-linear-model","chapter":"Lesson 7 Fallacies, Correlation and Regression","heading":"7.6.2 Fit a non-linear model","text":"found gloriously 2000s website “Statistical Reference Datasets”:\nhttps://www.itl.nist.gov/div898/strd/index.html\nInformation Technology Laboratory.\nofficial website United Stats Government\namazing unapologetic Word-Art,\nalso features handy datasets \npractice fitting non-linear models (https://itl.nist.gov/div898/strd/nls/nls_main.shtml)!chose one explore:\nhttps://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Chwirut2.datBecause might come across challenges, leaving\ntips , hidden behind details panels,\ncan choose need :","code":""},{"path":"freestyle.html","id":"freestyle","chapter":"Lesson 8 Freestyle","heading":"Lesson 8 Freestyle","text":"… learn ANOVA, explore ggplot extensions \nbuild interactive web application Shiny.","code":""},{"path":"freestyle.html","id":"setup-1","chapter":"Lesson 8 Freestyle","heading":"8.1 Setup","text":"Today using quite bunch packages.\nlecture course introduced one one,\ngood habit imports\ndependencies near top analysis \nshow already.","code":"\nlibrary(multcomp)\nlibrary(patchwork)\nlibrary(gganimate)\nlibrary(palmerpenguins)\nlibrary(broom)\nlibrary(tidyverse)"},{"path":"freestyle.html","id":"anova","chapter":"Lesson 8 Freestyle","heading":"8.2 ANOVA","text":"Let’s get started.\nANOVA stands Analysis Variance.\nstart familiar penguins dataset ask\nquestion: “difference bill length species?”Optically, certainly looks like .\nrun bunch t-tests compare groups,\n3 total (Adelie–Chinstrap, Adelie–Gentoo, Gentoo–Chinstrap).\ncorrect multiple testing.\npoint, running one tests (like Adelie vs. Chinstrap)\nlooks promising option!\nLooking data visually form comparison,\ncheating formed hypothesis \nlooking data.ANOVA way comparing multiple groups tells us, \ndifference groups.\nhowever tell us, groups difference exists,\njust overall difference.\norder get p-values direct comparisons groups\nrun post-hoc (Latin “fact”) anova result.First sliding little extra code chunk, later \nline might want compare groups e.g. baseline \ncontrol group (see section Dunnet ) baseline\ndecided first factor-level grouping variable.\nfct_relevel can move lovel first ()\nposition.create anova fit aov.explore various functions.","code":"\npenguins %>% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2)\npenguins <- penguins %>% \n  mutate(species = fct_relevel(species, \"Gentoo\"))\nanova_fit <- aov(bill_length_mm ~ species, data = penguins)\nanova_fitCall:\n   aov(formula = bill_length_mm ~ species, data = penguins)\n\nTerms:\n                 species Residuals\nSum of Squares  7194.317  2969.888\nDeg. of Freedom        2       339\n\nResidual standard error: 2.959853\nEstimated effects may be unbalanced\n2 observations deleted due to missingness\nsummary(anova_fit)             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       2   7194    3597   410.6 <2e-16 ***\nResiduals   339   2970       9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\ntidy(anova_fit)"},{"path":"freestyle.html","id":"tukey-post-hoc-test","chapter":"Lesson 8 Freestyle","heading":"8.2.1 Tukey Post-Hoc Test","text":"straighforward post-hoc test, already built R,\n“Tukey’s Honest Significant Differences,”\ncompares groups groups.","code":"\nTukeyHSD(anova_fit) %>% \n  tidy()"},{"path":"freestyle.html","id":"dunnet-post-hoc-test","chapter":"Lesson 8 Freestyle","heading":"8.2.2 Dunnet Post-Hoc Test","text":"control group use Dunnet’s test compare \ngroups control group.\nmeans less comparisons end higher statistical power.Unfortunately, Dunnet built R, can get necessary\nfunctions multcomp package.\nloading multcomp package careful\nload tidyverse!\nmultcomp also loads bunch packages one\nbrings ’s select function, loading \ntidyverse overwrite tidyverse select make life hard.Note, package can also Tukey’s test changing “Dunnet”\n“Tukey.”","code":"\nglht(anova_fit, mcp(species = \"Dunnet\")) %>% \n  tidy()"},{"path":"freestyle.html","id":"ggplot-extensions","chapter":"Lesson 8 Freestyle","heading":"8.3 ggplot extensions","text":"order plot nice significance stars top \ncomparisons let first introduce ggplot extensions:https://exts.ggplot2.tidyverse.org/just collection packages extend ggplot shape form\nsimply work well .","code":""},{"path":"freestyle.html","id":"ggsignif-ggpubr","chapter":"Lesson 8 Freestyle","heading":"8.3.1 ggsignif, ggpubr","text":"One ggsignif, can use add comparison\nbrackets plot.ggsignif can also run ’s comparisons, \nwilcoxon rank sum test default, especially\nstatistical tests prefer run first\ntrusting packages.\nAdditionally, course different type test (ANOVA),\nalready values, use just manually\nadd text annotation:","code":"\npenguins %>% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2) +\n  ggsignif::geom_signif(\n    comparisons = list(c(\"Gentoo\", \"Adelie\")),\n    annotations = c(\"hello\")\n  )\nstars <- function(p) {\n  case_when(\n    p <= 0.001 ~ \"***\",\n    p <= 0.01  ~ \"**\",\n    p <= 0.05  ~ \"*\",\n    TRUE       ~ \"ns\"\n  )\n}\n\ndunnet <- glht(anova_fit, mcp(species = \"Dunnet\")) %>% \n  tidy() %>% \n  mutate(contrast = str_split(contrast, \" - \"),\n         stars    = stars(adj.p.value))\n\nplt <- penguins %>% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2) +\n  ggsignif::geom_signif(\n    comparisons = dunnet$contrast,\n    annotations = dunnet$stars,\n    y_position = c(60, 65)\n  )\n\nplt"},{"path":"freestyle.html","id":"patchwork","chapter":"Lesson 8 Freestyle","heading":"8.3.2 patchwork","text":"also saved plot variable now create second\none just show patchwork package,\ncan combine plots neat layouts:","code":"\nplt2 <- ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +\n  geom_point()\n\n((plt | plt2) / plt2 ) + plot_layout(guides = 'collect')"},{"path":"freestyle.html","id":"ggfortify","chapter":"Lesson 8 Freestyle","heading":"8.3.3 ggfortify","text":"ggfortiy might strictly necessary want mention \ntalk autoplot.\nautoplot ggplot2 already default.\ncreates automatic plot certain objects, like models example,\nanova result.\nggfortify makes objects ability generate autoplot.want mention important implication\ncommunication.\ntwo purposes plots.\nfirst purpose generate insight , data analyst.\nAutoplots often helpful .\nsecond purpose communicate findings reader!\ntakes time effort refine plot something generated\ninsight spent lot time data\nalso generating insights reader sees data first time.\nmany people stop first step publish plots\n, without thinking much reader.\nKeep mind advance scientific career.Communication key!\ncode plots means communicating.","code":"\nlibrary(ggfortify)\nautoplot(anova_fit)"},{"path":"freestyle.html","id":"esquisse","chapter":"Lesson 8 Freestyle","heading":"8.3.4 esquisse","text":"Two cool packages: esquisse package let’s \ncreate ggplots interactively gui!\nmake sure always save code created ensure\nreproducibility!","code":""},{"path":"freestyle.html","id":"gganimate","chapter":"Lesson 8 Freestyle","heading":"8.3.5 gganimate","text":"gganimate extends ggplot ability use time dimension:","code":"\ngapminder::gapminder %>% \n  ggplot(aes(gdpPercap, lifeExp, size = pop, color = country)) +\n  geom_point() +\n  scale_x_log10() +\n  # facet_wrap(~year) +\n  transition_time(year) +\n  scale_color_manual(values = gapminder::country_colors) +\n  guides(color = \"none\",\n         size = \"none\")"},{"path":"freestyle.html","id":"build-apps-with-shiny","chapter":"Lesson 8 Freestyle","heading":"8.4 Build Apps with shiny","text":"can find code written lecture : example-app/.","code":""},{"path":"freestyle.html","id":"exercises-7","chapter":"Lesson 8 Freestyle","heading":"8.5 Exercises","text":"","code":""},{"path":"freestyle.html","id":"the-whole-deal","chapter":"Lesson 8 Freestyle","heading":"8.5.1 The Whole Deal","text":"Data analysis takes practice.\nlast exercise get option show \nlearned fresh dataset.\npeople different interests, leaving\nchoice open.Choose dataset TidyTuesday project:https://github.com/rfordatascience/tidytuesday#datasetsand write data analysis report.\nAim write questions \ntopic answer available data.\nExplore plots tables try\nend 1 2 high quality plots\nclearly communicate findings.\n, fun!","code":""},{"path":"freestyle.html","id":"feedback","chapter":"Lesson 8 Freestyle","heading":"8.6 Feedback","text":"send round link feedback form.\nanonymous.","code":""},{"path":"resources-5.html","id":"resources-5","chapter":"Resources","heading":"Resources","text":"Learning R can quite journey.\ncollecting useful links resources extra page.\nhelp understand topics covered, dive deeper interesting want discover cool things can R.","code":""},{"path":"resources-5.html","id":"learning-the-tidyverse","chapter":"Resources","heading":"8.7 Learning the tidyverse","text":"R Data Science28The ggplot2 bookR4DS online CommunityRStudio Cheat Sheets!Modern Dive29RStudio Education","code":""},{"path":"resources-5.html","id":"learning-rmarkdown","chapter":"Resources","heading":"8.8 Learning Rmarkdown","text":"rmarkdown cheatsheetrmarkdown referencepandoc manual (advanced)rmarkdown reproducible analysisrmarkdown website","code":""},{"path":"resources-5.html","id":"learning-r-in-general","chapter":"Resources","heading":"8.9 Learning R in general","text":"Advanced R30Hands Programming R31R Packages32Data Visualization: Practical Introduction33Graph Cookbook34","code":""},{"path":"resources-5.html","id":"learning-statistics","chapter":"Resources","heading":"8.10 Learning Statistics","text":"Intuitive Biostatistics35Statistics Done Wrong36StatQuest!!! Josh StarnerModern Statistics Modern Biology","code":""},{"path":"resources-5.html","id":"helpful-tools","chapter":"Resources","heading":"8.11 Helpful tools","text":"Generate ggplots via graphical user interface esquisee","code":""},{"path":"resources-5.html","id":"talks-podcasts-blogs-videos","chapter":"Resources","heading":"8.12 Talks, Podcasts, Blogs, Videos","text":"Just people inspiring blogposts, videos likes.David Robinson\nYouTube\nwebsite\nYouTubewebsiteJulia Silge\nYouTube\nwebsite\nYouTubewebsiteAlison Hill\nwebsite\nwebsiteThomas Lin Pedersen\nwebsite\nwebsite","code":""},{"path":"resources-5.html","id":"misc","chapter":"Resources","heading":"8.13 Misc","text":"Cute insightful illustrations37Happy Git R","code":""},{"path":"resources-5.html","id":"package-documentation-1","chapter":"Resources","heading":"8.14 Package Documentation","text":"tidyversetidymodelsrmarkdownreadrdplyrggplottidyrstringrpurrrragg","code":""},{"path":"resources-5.html","id":"books-and-manuals","chapter":"Resources","heading":"8.15 Books and Manuals","text":"Tidymodels bookggplot bookRmarkdown CookbookRmarkdown Book","code":""},{"path":"resources-5.html","id":"getting-help-1","chapter":"Resources","heading":"8.16 Getting Help","text":"find helpR4DS online learning community","code":""},{"path":"resources-5.html","id":"lists-of-resources","chapter":"Resources","heading":"8.17 Lists of Resources","text":"meta section. list lists:big book Rr rest us","code":""},{"path":"resources-5.html","id":"packages-that-enable-this-lecture-format","chapter":"Resources","heading":"8.18 Packages that enable this lecture format","text":"R R Core Team38knitr Yihui Xie39rmarkdown JJ Allaire et al.40xaringan Yihui Xie41","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
