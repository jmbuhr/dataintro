[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Analysis with R",
    "section": "",
    "text": "Hello and welcome!\n… to the latest iteration of my introductory R course, where we will learn to analyse data in style.\nIn this course, we will handle different kinds of data, create pretty and insightful visualizations, compute different statistics on our data and also explore what these statistical concepts mean. From penguins to p-values, I got you covered.\nOne of the plots we will be creating in the first lecture.",
    "crumbs": [
      "Hello and welcome!"
    ]
  },
  {
    "objectID": "index.html#prerequisits",
    "href": "index.html#prerequisits",
    "title": "Introduction to Data Analysis with R",
    "section": "Prerequisits",
    "text": "Prerequisits\nNo prior knowledge necessary.\nSoftware to install:\n\nR\nRstudio\n\nwindows only: Rtools",
    "crumbs": [
      "Hello and welcome!"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Introduction to Data Analysis with R",
    "section": "Structure of the course",
    "text": "Structure of the course\nMost participants will be biochemistry bachelor (and master) students, but the material is open to anyone!\n\nThere are 8 lectures in total, each accompanied by:\n\nA video of the lecture at the top of each page\nThe lecture script, which consists of the code written during the lecture (plus some more code to generate illustrative graphics) and explanations\n\nExercises to complete and send in\nA seminar to discuss the exercises\nA discord server to ask questions and share solutions\n\n\n\nIn addition to the 8 regular lectures + follow up seminars, there is an introductory seminar in the first week, before the first lecture.\n\nOctober 17 (Tuesday, 13:00): First seminar\nOctober 24: Seminar for the first lecture\nDecember 12: Seminar for the last lecture\n\nThe first seminar is dedicated to helping you get your laptops set up for the course, show the course structure and explain core concepts.\nI do recommend to watch the lecture in your own time, and then use the lecture script afterwards to look up concepts and code you want to revisit. Code chunks also have a copy-button, which is helpful for quickly playing around with it, but make sure you actually walk through the lecture and do the typing first, because muscle memory will server you well in the future.\nExercises\nTo complete the course, hand in at least 6 out of 8 exercises. The important part here is not that each exercise is a perfect solution, but if you encounter questions and struggles during your attempt of the exercise, make sure to include those pain points as well so that we can cover those in the Seminar. Please hand in your solutions before the seminar via a direct message on discord. The earlier in the week you submit your solutions, the more time I have to prepare answers for the seminar.\nSeminar\nEach week, we will meet to discuss the exercises and answer any questions that might have have popped up. We will meet in:\n\nTODO\n\nPlease bring your own laptop, so that you can code along and know that you will be able to apply what you learned after the course as well.\nDiscord and signup\nIf you are a biochemistry student at Heidelberg University, click on this link: https://discord.gg/TYQhFAAfxu to join our discord server.\nTo sign up for the course, send a message to me (Jannik) with your full name and matriculation number so that we can put the course onto your official transcript of records. On the server, you will be able to ask questions that can be answered by me and your fellow learners, hand in the exercises and receive feedback.",
    "crumbs": [
      "Hello and welcome!"
    ]
  },
  {
    "objectID": "01-intro.html#what-you-will-learn",
    "href": "01-intro.html#what-you-will-learn",
    "title": "1  Introduction",
    "section": "\n1.1 What You will Learn",
    "text": "1.1 What You will Learn\nThroughout your scientific career — and potentially outside of it — you will encounter various forms of data. Maybe you do an experiment and measured the fluorescence of a molecular probe, or you simply count the penguins at your local zoo. Everything is data in some form or another. But raw numbers without context are meaningless and tables of numbers are not only boring to look at, but often hide the actual structure in the data.\nIn this course you will learn to handle different kinds of data. You will learn to create pretty and insightful visualizations, compute different statistics on your data and also what these statistical concepts mean. From penguins to p-values, I got you covered.\nThe course will be held in English, as the concepts covered will directly transfer to the research you do, where the working language is English. That being said, feel free to ask questions in any language that I understand, so German is also fine. My Latin is a little rusty, thought.\nIn this course, we will be using the programming language R. R is a language particularly well suited for data analysis, because it was initially designed by statisticians and because of the interactive nature of the language, which makes it easier to get started. So don’t fret if this is your first encounter with programming, we will take one step at a time.\nThe datasets chosen to illustrate the various concepts and tools are not particularly centered around Biology. Rather, I chose general datasets that require less introduction and enable us to focus on learning R and statistics. This is why we will be talking about penguins, racing games or life expectancy instead of intricate molecular measurements."
  },
  {
    "objectID": "01-intro.html#execute-r-code",
    "href": "01-intro.html#execute-r-code",
    "title": "1  Introduction",
    "section": "\n1.2 Execute R Code",
    "text": "1.2 Execute R Code\nYou can now execute commands in the R console in the bottom left. For example we can calculate a mathematical expression:\n\n1 + 1\n\n[1] 2\n\n\nOr generate the numbers from one to 10:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBut I rarely type directly into the console. Because we want our results to be reproducible, we write our code in a script first, so that the next person 1 can see what we did and replicate our analysis. You will see that reproducibility is quite near and dear to me, so it will pop up once or twice. And as scientists, I am sure you understand the importance.\n1 This will most likely be future You. And you will thank yourself later\nA script is like a recipe. It is the most important part of your data analysis workflow, because as long as you have the recipe, you can recreate whatever products (e.g. plots, statistics, tables) you have with ease.\n\nTo create a new script, click the little button in the top left corner. In a script you can type regular R code, but it won’t get executed straight away. To send a line of code to the console to be executed, hit Ctrl+Enter. Go ahead, try it with:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#get-to-know-rstudio",
    "href": "01-intro.html#get-to-know-rstudio",
    "title": "1  Introduction",
    "section": "\n1.3 Get to know RStudio",
    "text": "1.3 Get to know RStudio\nBefore we get deeper into R, let’s talk a little bit about our Home when working with R: RStudio.\nThere is one important setting that I would like you to change: Under Tools -&gt; Global Options make sure that “Restore .RData into workspace at startup” is unchecked. The workspace that RStudio would save as .RData contains all objects created in a session, which is, what we can see in the Environment pane (by default in the top right panel, bottom right in my setup). Why would we not want to load the objects we created in the last session into our current session automatically? The reason is reproducibility. We want to make sure that everything our analysis needs is in the script. It creates our variables and plots from the raw data and should be the sole source of truth.\nCheck out the lecture video for further customization of RStudio e.g. with themes and make sure to also use RStudio Projects to structure your work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#expressions-tell-r-to-do-things",
    "href": "01-intro.html#expressions-tell-r-to-do-things",
    "title": "1  Introduction",
    "section": "\n1.4 Expressions: Tell R to do things",
    "text": "1.4 Expressions: Tell R to do things\nR can do lot’s of things, but let’s start with some basics, like calculating. Everything that starts with # is a comment and will be ignored by R.\n\n1 + 1 # addition\n\n[1] 2\n\n32 / 11 # division\n\n[1] 2.909091\n\n3 * 4 # multiplication\n\n[1] 12\n\n13 %% 5 # modulo\n\n[1] 3\n\n13 %/% 5 # integer division\n\n[1] 2\n\n\nCreate vectors with the : operator, e.g. numbers from:to\n\n1:4\n\n[1] 1 2 3 4\n\n\nAnd mathematical operations are automatically “vectorized”:\n\n1:3 + 1:3\n\n[1] 2 4 6\n\n\nIn fact, R as no scalars (individual values), those are just vectors of length 1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#variables-boxes-for-things",
    "href": "01-intro.html#variables-boxes-for-things",
    "title": "1  Introduction",
    "section": "\n1.5 Variables: Boxes for things",
    "text": "1.5 Variables: Boxes for things\nOften, you will want to store the result of a computation for reuse, or to give it a sensible name and make your code more readable. This is what variables are for. We can assign a value to a variable using the assignment operator &lt;- (In RStudio, there is a shortcut for it: Alt+Minus):\n\nmy_number &lt;- 42\n\nExecuting the above code will not give you any output, but when you use the name of the variable, you can see its content.\n\nmy_number\n\n[1] 42\n\n\nAnd you can do operations with those variables:\n\nx &lt;- 41\ny &lt;- 1\nx + y\n\n[1] 42\n\n\n\nNOTE Be careful about the order of execution! R enables you to work interactively and to execute the code you write in your script in any order with Ctrl+Enter, but when you execute (=“source”) the whole script, it will be executed from top to bottom.\n\nFurthermore, code is not executed again automatically, if you change some dependency of the expression later on. So the second assignment to x doesn’t change y.\n\nx &lt;- 1\ny &lt;- x + 1\nx &lt;- 1000\ny\n\n[1] 2\n\n\nVariable names can contain letters (capitalization matters), numbers (but not as the first character) and underscores _. 2\n2 They can also contain dots (.), but it is considered bad practice, because it can lead to some confusing edge cases.\n# snake_case\nmain_character_name &lt;- \"Kvothe\"\n\n# or camelCase\nbookTitle &lt;- \"The Name of the Wind\"\n\n# you can have numbers in the name\nx1 &lt;- 12\n\n\n\nA depiction of various naming styles by Horst (2020)\n\nA good convention is to always use snake_case.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#atomic-datatype",
    "href": "01-intro.html#atomic-datatype",
    "title": "1  Introduction",
    "section": "\n1.6 Atomic datatype",
    "text": "1.6 Atomic datatype\nFirst we have numbers (which internally are called numeric or double)\n\n12\n12.5\n\nThen, there are whole numbers (integer)\n\n1L # denoted by L\n\nas well as the rarely used complex numbers (complex)\n\n1 + 3i # denoted by the small i for the imaginary part\n\nText data however will be used more often (character, string). Everything enclosed in quotation marks will be treated as text. Double or single quotation marks are both fine.\n\n\"It was night again.\"\n'This is also text'\n\nLogical values can only contain yes or no, or rather TRUE and FALSE in programming terms (boolean, logical).\n\nTRUE\nFALSE\n\nThere are some special types that mix with any other type. Like NULL for no value and NA for “Not Assigned”.\n\nNULL\nNA\n\nNA is contagious. Any computation involving NA will return NA (because R has no way of knowing the answer):\n\nNA + 1\n\n[1] NA\n\nmax(NA, 12, 1)\n\n[1] NA\n\n\nBut some functions can remove NAs before giving us an answer:\n\nmax(NA, 12, 1, na.rm = TRUE)\n\n[1] 12\n\n\nYou can ask for the datatype of an object with the function typeof:\n\ntypeof(\"hello\")\n\n[1] \"character\"\n\n\nThere is also a concept called factors (factor) for categorical data, but we will talk about that later, when we get deeper into vectors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#functions-calculate-run-and-automate-things",
    "href": "01-intro.html#functions-calculate-run-and-automate-things",
    "title": "1  Introduction",
    "section": "\n1.7 Functions: Calculate, run and automate things",
    "text": "1.7 Functions: Calculate, run and automate things\n\nIn R, everything that exists is an object, everything that does something is a function.\n\nFunctions are the main workhorse of our data analysis. For example, there are mathematical functions, like sin, cos etc.\n\nsin(x = 0)\n\n[1] 0\n\n\nFunctions take arguments (sometimes called parameters) and sometimes they also return things. The sin function takes just one argument x and returns its sine. What we do with the returned value is up to us. We can use it directly in another computation or store it in a variable. If we don’t do anything with the return value, R simply prints it to the console.\nNote, that the = inside the function parenthesis gives x = 0 to the function and is separate from any x defined outside of the function. For example:\n\nx &lt;- 10\ncos(x = 0)\n\n[1] 1\n\n# x outside of the function is still 10\nx\n\n[1] 10\n\n\nTo learn more about a function in R, execute ? with the function name or press F1 with your mouse over the function. This is actually one of the most important things to learn today, because the help pages can be… well… incredibly helpful.\n\n?sin\n\nWe can pass arguments by name or by order of appearance. The following two expressions are equivalent.\n\nsin(x = 12)\nsin(12)\n\nOther notable functions to start out with:\nCombine elements into a vector:\n\nc(1, 3, 5, 31)\n\n[1]  1  3  5 31\n\n\nConvert between datatypes with:\n\nas.numeric(\"1\")\n\n[1] 1\n\nas.character(1)\n\n[1] \"1\"\n\n\nCalculate summary values of a vectore:\n\nx &lt;- c(1, 3, 5, 42)\nmax(x)\n\n[1] 42\n\nmin(x)\n\n[1] 1\n\nmean(x)\n\n[1] 12.75\n\nrange(x)\n\n[1]  1 42\n\n\nCreate sequences of numbers:\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\nYou just learned about the functions sin, seq and max. But wait, there is more! Not only in the sense that there are more functions in R (what kind of language would that be with only two verbs?!), but also in a more powerful way:\n\nWe can define our own functions!\n\nThe syntax (\\(\\leftarrow\\) grammar for programming languages) is as follows.\n\nname_for_the_function &lt;- function(parameter1, parameter2, ...) { # etc.\n  # body of the function\n  # things happen\n  result &lt;- parameter1 + parameter2\n  # Something the function should return to the caller\n  return(result)\n}\n\nThe function ends when it reaches the return keyword. It also ends when it reaches the end of the function body and implicitly returns the last expression. So we could have written it a bit shorter and in fact you will often see people omitting the explicit return at the end:\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nAnd we can call our freshly defined function:\n\nadd(23, 19)\n\n[1] 42\n\n\nGot an error like Error in add(23, 19) : could not find function \"add\"? Check that you did in fact execute the code that defines the function (i.e. put your cursor on the line with the function keyword and hit Ctrl+Enter.).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#packages-sharing-functions",
    "href": "01-intro.html#packages-sharing-functions",
    "title": "1  Introduction",
    "section": "\n1.8 Packages: Sharing functions",
    "text": "1.8 Packages: Sharing functions\nYou are not the only one using R. There is a welcoming and helpful community out there. Some people also write a bunch of functions and put them together in a so called package. And some people even went a step further. The tidyverse is a collection of packages that play very well together and also iron out some of the quirkier ways in which R works (Wickham et al. 2019). They provide a consistent interface to enable us to do more while having to learn less special cases. The R function install.packages(\"&lt;package_name_here&gt;\") installs packages from CRAN a curated set of R packages.\nThis is one exception to our effort of having everything in our script and not just in the console. We don’t want R trying to install the package every time we run the script, as this needs to happen only once. So you can either turn it into a comment, delete it from the script, or only type it in the console. You can also use RStudio’s built-in panel for package installation.\nR packages, especially the ones we will be using, often come with great manuals and help pages and I added a link to the package website for each of the packages to the hexagonal icons for each package in the script, so make sure to click the icons.\nIf you don’t have the link at hand you can also always find help on the internet. Most of these packages publish their source code on a site called GitHub, so you will be able to find further links, help and documentation by searching for r  github. Sometimes it can be helpful to write our R’s full name when searching (turns out there are a lot of thing with the letter R): rstats.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#literate-programming-with-quarto-previously-rmarkdown-code-is-communication",
    "href": "01-intro.html#literate-programming-with-quarto-previously-rmarkdown-code-is-communication",
    "title": "1  Introduction",
    "section": "\n1.9 Literate Programming with Quarto (previously Rmarkdown): Code is communication",
    "text": "1.9 Literate Programming with Quarto (previously Rmarkdown): Code is communication\n\n\n\nQuarto enables us, to combine text with code and then produce a range of output formats like pdf, html, word documents, presentations etc. In fact, this whole website was created with Quarto. Sounds exciting? Let’s dive into it!\nOpen up a new Quarto document with the file extension .qmd from the New File menu in the top left corner of RStudio: File → New File → Quarto Document and choose html as the output format. I particularly like html, because you don’t have to worry about page breaks and it easily works on screens of different sizes, like your phone.\nA Quarto document consists of three things:\n\n\nMetadata:\nInformation about your document such as the author or the date in a format called YAML. This YAML header starts and ends with three minus signs ---.\n\nText:\nRegular text is interpreted as markdown, meaning it supports things like creating headings by prefixing a line with #, or text that will be bold in the output by surrounding it with **.\n\nCode chunks:\nStarting with a line with 3 backticks and {r} and ending with 3 backticks. They will be interpreted as R code. This is where you write the code like you did in the .R script file. You can insert new chunks with the button on the top right of the editor window or use the shortcut Ctrl+Alt+i.\n\nUse these to document your thoughts alongside your code when you are doing data analysis. Future you (and reviewer number 2) will be happy! To run code inside of chunks, use,the little play button on the chunk, the tried and true Ctrl+Enter to run one line, or Ctrl+Shift+Enter to run the whole chunk. Your chunks can be as large or small as you want, but try to maintain some sensible structure.\nThe lecture video also demonstrates the different output formats (though for the exercises we will only be using html) and the visual editor.\n\n\nCute little monsters as Rmarkdown Wizards by Allison Horst\n\n\n1.9.1 The Tidyverse\n \nGo ahead and install the tidyverse packages with\n\ninstall.packages(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#our-first-dataset-the-palmer-penguins",
    "href": "01-intro.html#our-first-dataset-the-palmer-penguins",
    "title": "1  Introduction",
    "section": "\n1.10 Our First Dataset: The Palmer Penguins",
    "text": "1.10 Our First Dataset: The Palmer Penguins\n\n\n\n\n\nFigure 1.1: The three penguin species of the Palmer Archipelago, by Allison Horst\n\n\nSo let’s explore our first dataset together in a fresh Quarto document. The setup chunk is special. It gets executed automatically before any other chunk in the document is run. This makes it a good place to load packages. The dataset we are working with today actually comes in its own package, so we need to install this as well (Yes, there is a lot of installing today, but you will have to do this only once):\n\ninstall.packages(\"palmerpenguins\")\n\nAnd then we populate our setup chunk with\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nThis gives us the penguins dataset (Horst, Hill, and Gorman 2020):\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n1.10.1 Dataframes: R’s powerfull tables\nLet’s talk about the shape of the penguins object. The str function reveals the structure of an object to us.\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nThe penguins variable contains a tibble, which is the tidyverse version of a dataframe. It behaves the same way but prints out nicer. Both are a list of columns, where columns are (usually) vectors. We will learn more about their underlying datastructure, lists, next week.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-grammar-of-graphics-translate-data-into-visualizations",
    "href": "01-intro.html#the-grammar-of-graphics-translate-data-into-visualizations",
    "title": "1  Introduction",
    "section": "\n1.11 The Grammar of Graphics: Translate data into visualizations",
    "text": "1.11 The Grammar of Graphics: Translate data into visualizations\n \nYou probably took this course because you want to build some cool visualizations for you data. In order to do that, let us talk about how we can describe visualizations. Just like language has grammar, some smart people came up with a grammar of graphics (Wilkinson et al. 2005), which was then slightly modified and turned into an R package so that we can not only talk about but also create visualizations using this grammar (Wickham 2010). The package is called ggplot2, and we already have it loaded because it is included in the tidyverse. Before looking at the code, we can describe what we need in order to create this graphic.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %&gt;%\n  ggplot(aes(flipper_length_mm, bill_length_mm,\n             color = species,\n             shape = sex)) +\n  geom_point(size = 2.5) +\n  geom_smooth(aes(group = species), method = \"lm\", se = FALSE,\n              show.legend = FALSE) +\n  labs(x = \"Flipper length [mm]\",\n       y = \"Bill length [mm]\",\n       title = \"Penguins!\",\n       subtitle = \"The 3 penguin species can be differentiated by their flipper- and bill-lengths.\",\n       caption = \"Datasource:\\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins:\\nPalmer Archipelago (Antarctica) penguin data.\\nR package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\",\n       color = \"Species\",\n       shape = \"Sex\") +\n  theme_minimal() +\n  scale_color_brewer(type = \"qual\") +\n  theme(plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\nHaving a grammar means: - we can build complex visualizations with basic building blocks that fit together according to some rules (the grammar) - just like lego bricks - we just have to learn the building blocks and not a different function for all the different types of plots (e.g. barplot, scatterplot, lineplot, piechart)\nWe can build this plot up step by step. The data is the foundation of our plot, but this just gives us an empty plotting canvas. I am assigning the individual steps we are going through to a variable, so that we can sequentially add elements, but you can do this in one step as shown above.\n\nplt &lt;- ggplot(penguins)\n\nplt\n\n\n\n\n\n\n\nThen, we add and aesthetic mapping to the plot. It creates a relation from the features of our dataset (like the flipper length of each penguin) to a visual property, like position of the x-axis, color or shape.\n\nplt &lt;- ggplot(penguins,\n              aes(x = flipper_length_mm,\n                  y = bill_length_mm,\n                  color = species,\n                  shape = sex))\n\nplt\n\n\n\n\n\n\n\nStill, the plot is empty, it only has a coordinate system with a certain scale. This is because we have no geometric objects to represent our aesthetics. Elements of the plot are added using the + operator and all geometric elements that ggplot knows start with geom_. Let’s add some points:\n\nplt &lt;- plt +\n  geom_point()\n\nplt\n\n\n\n\n\n\n\nLook at the help page for geom_point to find out what aesthetics it understands. The exact way that features are mapped to aesthetics is regulated by scales starting with scale_ and the name of an aesthetic:\n\nplt &lt;- plt +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\"))\n\nplt\n\n\n\n\n\n\n\nWe can add or change labels (like the x-axis-label) by adding the labs function.\n\nplt &lt;- plt +\n    labs(x = \"Flipper length [mm]\",\n         y = \"Bill length [mm]\",\n         title = \"Penguins!\",\n         subtitle = \"The 3 penguin species can differentiated by their flipper and bill lengths\")\n\nThe overall look of the plot is regulated by themes like the pre-made theme_ functions or more finely regulated with the theme() function, which uses element functions to create the look of individual elements. Autocomplete helps us out a lot here (Ctrl+Space).\n\nplt &lt;- plt + \n  theme_minimal() +\n  theme(legend.text = element_text(face = \"bold\"))\n\nplt\n\n\n\n\n\n\n\nIn summary, this is what our plot needs:\n\ndata\naesthetic mapping\ngeom(s)\n(stat(s))\ncoordinate system\nguides\nscales\ntheme\n\n\nmy_plot &lt;- ggplot(penguins,\n                  aes(x = flipper_length_mm,\n                      y = bill_length_mm,\n                      shape = sex,\n                      color = species)) +\n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"orange\")) +\n  labs(title = \"Penguins\") +\n  theme(plot.title = element_text(colour = \"purple\"))\n\nmy_plot \n\n\n\n\n\n\n\nWe can save our plot with the ggsave function. It also has more arguments to control the dimentions and resolution of the image.\n\nggsave(\"my_plot.png\", my_plot)\n\nNext week we will be get rid of the annoying NA in the legend for sex.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-community-there-to-catch-you.",
    "href": "01-intro.html#the-community-there-to-catch-you.",
    "title": "1  Introduction",
    "section": "\n1.12 The Community: There to catch You.",
    "text": "1.12 The Community: There to catch You.\n\n\nComunity Teamwork by Allison Horst\n\n\n\nGoogling the Error Message",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#bonus-get-more-rstudio-themes",
    "href": "01-intro.html#bonus-get-more-rstudio-themes",
    "title": "1  Introduction",
    "section": "\n1.13 Bonus: Get more RStudio themes",
    "text": "1.13 Bonus: Get more RStudio themes\n\nand talk about where packages come from\nhttps://github.com/gadenbuie/rsthemes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#exercises",
    "href": "01-intro.html#exercises",
    "title": "1  Introduction",
    "section": "\n1.14 Exercises",
    "text": "1.14 Exercises\nThis course is not graded, but I need some way of confirming that you did indeed take part in this course. In order to get the confirmation, you will send your solutions for a minimum of 6 out of the 8 exercises to me before the Seminar Tuesdays For each week I would like you to create a fresh quarto document with your solutions as code as well as any questions that arose during the lecture. This will help me a lot in improving this course.\nWhen you are done solving the exercises, hit the knit button (at the top of the editor panel) and send me the resulting html document via discord (confirm that it looks the way you expected beforehand).\nHere are today’s tasks:\n\n1.14.1 Put your flippers in the air!\nIn a fresh quarto document (without the example template content), load the tidyverse and the palmerpenguins packages.\n\nWrite a section of text about your previous experience with data analysis and/or programming (optional, but I can use this information to customize the seminars to your needs).\n\nCreate a vector of all odd numbers from 1 to 99 and store it in a variable.\n\nCreate a second variable that contains the squares of the first.\nHave a look at the tibble function. Remember that you can always access the help page for a function using the ? syntax, e.g. ?tibble::tibble (The two colons :: specify the package a function is coming from. You only need tibble(...) in the code because the tibble package is loaded automatically with the tidyverse. Here, I specify it directly to send you to the correct help page).\nCreate a tibble where the columns are the vectors x and y.\nCreate a scatterplot (points) of the two columns using ggplot.\nWhat geom_ function do you need to add to the plot to add a line that connects your points?\n\n\n\nLoad the penguins dataset from the palmerpenguins package. Produce a scatterplot of the bill length vs. the bill depth, colorcoded by species.\n\nImaginary bonus points if you manage to use the same colors as in the penguin-image (hint: look at the help page for scale_color_manual() to find out how. Note, that R can work with it’s built-in color names, rgb() specifications or as hex-codes such as #1573c7). Even more bonus points if you also look into the theme() function and it’s arguments, or the theme_&lt;...&gt;() functions to make the plot prettier.\n\n\nCheck the metadata (YAML) of your quarto document and make sure it contains your name as the author:.\nThe filename of your solution should also contain your name, e.g. lecture1-jannik.qmd, which gets rendered to lecture1-jannik.html when you knit it.\n\nMake the output document self contained by adding embed-resources: true to the yaml header.\n\n\nHere are a couple more YAML options you can try if you feel adventurous.\n\n\n\nThe top of your Quarto document should now look like this between the ---:\ntitle: \"Lecture 1\"\nauthor: &lt;Your Name&gt;\nformat:\n  html:\n    embed-resources: true\n    # more html options if you want\n    # like e.g. theme: &lt;...&gt;\nexecute:\n  warning: false\n\nKnit it and ship it! (=press the render button and send me the rendered html document via discord)\n\n\n\n\n\n\n\nCaution\n\n\n\n\nSome operating systems (looking at you, Windows), will not show you file extensions by default (e.g. .html, .png, docx). Here is a guide on how to turn them on in Windows 11 and 10. I recommend to to so, to make it easier for you to send the correct file (the rendered html file instead of the source qmd).\nYou can test that your file is truely self contained, by copying it to a different location on your computer and opening it (a simple double-click should suffice) with your default browser. If you can see your plots, you are good to go. If you don’t see them that means the plots are not embedded into the output (just linked to their relative location). Check the second-to-last point of the exercise list again.\n\n\n\n\n1.14.2 Exercise Tips",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learn-more",
    "href": "01-intro.html#learn-more",
    "title": "1  Introduction",
    "section": "\n1.15 Learn more:",
    "text": "1.15 Learn more:\nCheck out the dedicated Resources page.\n\n\n\n\nHorst, Allison. 2020. “Artwork by @Allison_horst.” https://github.com/allisonhorst/stats-illustrations. https://github.com/allisonhorst/stats-illustrations.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. Manual.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilkinson, Leland, D. Wills, D. Rope, A. Norton, and R. Dubbs. 2005. The Grammar of Graphics. 2nd edition. New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#a-data-analysis-workflow",
    "href": "02-data-wrangling.html#a-data-analysis-workflow",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.1 A Data Analysis Workflow",
    "text": "2.1 A Data Analysis Workflow\nWe are getting close to importing our very first dataset from a file into R. Generally, this is the first thing that needs to happen with any data analysis and we will cover it today. The data I provided is already pretty tidy so we will start with that and build some visualizations. The communicate-part is also covered, because we are working in Rmarkdown after all, which is designed to communicate our findings. Next week we will also have a look at some less tidy data, but not before having defined what “tidy data” is.\n\n\nFigure from Wickham and Grolemund (2017)."
  },
  {
    "objectID": "02-data-wrangling.html#reading-data-with-readr",
    "href": "02-data-wrangling.html#reading-data-with-readr",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.2 Reading Data with readr\n",
    "text": "2.2 Reading Data with readr\n\n \nThe package responsible for loading data in the tidyverse is called readr, so we start by loading the whole tidyverse.\nNote, that in general, we could also load just the readr package with library(readr), but we need the rest of the tidyverse later on anyways. There is also the option to not load a package at all but rather only use one function from a package by prefixing the function with the package name and two colons (::) Like so: readr::read_csv(\"...\").\nWithout further ado, let’s download the data for today. In fact, there are multiple ways to go about this. We could download the whole course folder from GitHub https://github.com/jmbuhr/dataintro and then using the “&lt;&gt; Code” download button:\n\nThe data is all in a folder called data and organized into sub-folders with the lecture number. So everything you need for today can be found in folder 02.\nOn GitHub, we can also download individual files, but for plain text files you need to remember one extra step. If you have already clicked on the file in GitHub and can see it’s content, it is tempting to copy and paste the link from the browser bar and use R’s download.file function. However, this is just the link to the part of the website where GitHub shows the file, not a link to the actual file. We can get the correct link by clicking on the Raw button:\n\nThen we can use this to download the file:\n\ndownload.file(\"https://raw.githubusercontent.com/jmbuhr/dataintro/main/data/02/gapminder.csv\", \"example-download.csv\")\n\nIf you look into the source for this lecture you will find that I set the chunk option eval=FALSE, meaning the code will not but run. I don’t want to download the file again every time I make a change to the course script.\nA common error I see people having with download.file is trying to download a file into e.g. a folder called data without first creating said folder. If you get one of those No such file or directory errors, this is the most likely cause.\n\nread gapminder data (csv)\nread csv also works with url\n\nbut you probably also want a local copy\nwrite_csv and friends\n\n\n\nView and ctrl+click\n\nWith our data downloaded, we can make use of RStudio’s autocompletion to complete the file-path to our data inside of quotation marks. We can trigger it explicitly with ctrl+space or tab.\n\ngapminder &lt;- read_csv(\"data/02/gapminder.csv\")\n\nreadr will also tell you the datatypes it guessed for the columns. Let’s inspect our dataset:\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe gapminder dataset (Bryan 2017) is an excerpt from the gapminder project and contains the life expectancy at birth for 142 countries at 5 year intervals between 1952 and 2007. It also contains the population and the Gross Domestic Product (GDP) per Inhabitant. We will built a visualization later on.\nA cool trick for when you have your data in a variable is the View function. The same effect can be reached by ctrl+clicking on it or by using the button next to it in the environment panel.\nIt should be noted that the read_csv function can also read data from links and will download it automatically. However, in order to have the data nice and safe, we might want to save if somewhere, just in case (links can change, especially when it is someone else’s link).\n\nour_data &lt;- read_csv(\"https://raw.githubusercontent.com/jmbuhr/dataintro/main/data/02/gapminder.csv\")\n\nwrite_csv(our_data, \"our-data.csv\")"
  },
  {
    "objectID": "02-data-wrangling.html#a-project-based-workflow",
    "href": "02-data-wrangling.html#a-project-based-workflow",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.3 A Project-based Workflow",
    "text": "2.3 A Project-based Workflow\nLast week we simply went ahead and created a script file and an Rmarkdown file in some folder on our computer. But how does R known, where the script is? How does it know, where to look, when we tell it to read in a file or save a plot? The main folder where R starts is called the working directory. To find out, what our current working directory is, we execute the function getwd() for get working directory:\n\ngetwd()\n\n[1] \"/home/jannik/teaching/dataintro\"\n\n\nRStudio projects set this working directory automatically, which is very convenient. It makes it easier for us to share code and projects, by simply copying the whole folder. But we have to follow one prerequisite. Our file paths need to be relative, not absolute. An absolute file path starts at the root of our operating system, so on windows you will see something like C:\\\\User\\Jannik\\Documents\\... while on mac and linux it starts with /home/jannik/Documents/.... For example, I could read the same gapminder dataset by:\n\nread_csv(\"/home/jannik/Documents/projects/teaching/dataintro/data/02/gapminder.csv\")\n\nBut this is a terrible idea! If I ever move my analysis folder, this file path will no longer be correct and if someone else tries to run my code they will most certainly not be called Jannik and have the exact same directory structure. So it will also not work.\nIn order for our work to be portable, robust and shareable, we need our file paths to be relative to the root of our project (which is set by the RStudio project).\n\nread_csv(\"./data/02/gapminder.csv\")\n\nHere, ./ refers to the current working directory, which is set by the RStudio project. It can also be omitted (e.g. data/02/...), but the path can’t start with / because this would mark it as an absolute path.\nThere is also a function to set the working directory ourselves (it is called setwd), but I ask you to never use it. Because in order to use it, you would have to specify the working directory using an absolute path, rendering the script useless for anyone that is not you. Use RStudio projects instead.\nThere is one thing I didn’t tell you about Rmarkdown documents, yet. Their working directory is always the folder they are in, even if they are in some subdirectory of a project. In a way this also means that you don’t necessarily need a project to work with Rmarkdown, but having one anyway makes it easier to keep track of your files and have a consistent structure.\n\n2.3.1 Common Hurdles when Importing Data\nSo, importing the gapminder csv went smoothly. But this will not always be the case. We will now look at common hurdles when importing data.\nThe function we just used was called read_csv, because it reads a file format that consists of comma separated values. Look at the raw file in a text editor (not word) like notepad or RStudio to see why. But the file extension .csv can sometimes be lying…\nBecause in German, the comma is used to separate decimal numbers (vs. the dot in English), a lot of Software will output a different type of csv-file when configured in German. It will still call it csv, but actually it is separated by semicolons! We have a special function for this:\n\nread_csv2(\"data/02/gapminder_csv2.csv\")\n\nWhen looking through the autocompletion options that pop up when you are typing the function name, you might have noticed a similar function read.csv and read.csv2. These are the functions that come with R, without any packages like the tidyverse. You can of course use those as well, but the tidyverse functions provide a more consistent experience and have less surprising quirks. I am teaching the tidyverse first because it allows you to do more while having to learn less edge cases.\nIf we look at yet another file gapminder_tsv.txt, we notice that the file extension doesn’t tell us much about the format, only that it is text (as opposed to a binary format only computers can read). If we look into the file:\n\nread_lines(\"data/02/gapminder_tsv.txt\", n_max = 3)\n\n[1] \"country\\tcontinent\\tyear\\tlifeExp\\tpop\\tgdpPercap\"    \n[2] \"Afghanistan\\tAsia\\t1952\\t28.801\\t8425333\\t779.4453145\"\n[3] \"Afghanistan\\tAsia\\t1957\\t30.332\\t9240934\\t820.8530296\"\n\n\nWe notice that the values are separated by “, a special sequence that stands for the tab character. The read_tsv function will do the job. I am not showing the output here because it is just the gapminder dataset once again.\n\nread_tsv(\"data/02/gapminder_tsv.txt\")\n\nIf the separator (also called delimiter) is even more obscure, we can use the general function read_delim. Say a co-worker misunderstood us and thought tsv stands for “Tilde separated values”, we can still read his file.\n\nread_delim(\"data/02/obscure_file.tsv\", \"~\")\n\nThere are more ways in which raw data can be messy or hard to read depending on the machine but I can’t show all of them. One common thing you will encounter though is measurement machines writing some additional information in the first couple of lines before the actual data (like the time of the measurement). In this example:\n\nread_lines(\"data/02/gapminder_messier.csv\", n_max = 5)\n\n[1] \"# Some comment about the data\"                   \n[2] \"And maybe a personal note\"                       \n[3] \"country,continent,year,lifeExp,pop,gdpPercap\"    \n[4] \"Afghanistan,Asia,1952,28.801,8425333,779.4453145\"\n[5] \"Afghanistan,Asia,1957,30.332,9240934,820.8530296\"\n\n\nThe first 2 lines are not part of the data. Reading the file normally as a csv would produce something weird: Because the first line does not contain any commata, it will assume that the file contains only one column and also report a bunch of parsing failures. Parsing is the act of turning data represented as raw text into a useful format, like a table of numbers.\nWe can fix this by telling R to skip the first 2 lines entirely:\n\nread_csv(\"data/02/gapminder_messier.csv\", skip = 2, n_max = 3)\n\n# A tibble: 3 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n\n\nI was using the n_max argument of the functions above to save space in this lecture script.\nWe can also read excel files it using a function from the readxl package. This package is automatically installed with the tidyverse, but it is not loaded along with the other packages via library(tidyverse). We can either load it with library(readxl) or refer to a single function from the package without loading the whole thing using double colons (::) like so:\n\nreadxl::read_xlsx(\"data/02/gapminder.xlsx\")\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nRemember, that when we read in the gapminder dataset for the first time to day, we saved it in a variable called gapminder, so we are going to use this going forward."
  },
  {
    "objectID": "02-data-wrangling.html#wrangling-data-with-dplyr",
    "href": "02-data-wrangling.html#wrangling-data-with-dplyr",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.4 Wrangling Data with dplyr\n",
    "text": "2.4 Wrangling Data with dplyr\n\n \nThere a are a number of ways in which we can manipulate data. Of course I mean manipulate in it’s original sense, not the malicious one. This is sometimes referred to as data wrangling and within the tidyverse, this is a job for the dplyr package (short for data plyer, the tool you see in the logo).\ndplyr provides functions for various operations on our data. Theses functions are sometimes also called dplyr verbs. All of them take a tibble or data.frame as input (plus additional parameters) and always return a tibble. But enough talk, let’s go wrangling!\n\n\nLet’s go data wrangling! Artwork by Allison Horst\n\n\n2.4.1 select\nThe first verb we introduce is used to select columns. And hence, it is called select. The first argument is always the data, followed by an arbitrary number of column names. We can recognize functions the take an arbitrary number of additional arguments by the ... in the autocompletion and help page.\n\nselect(gapminder, country, year, gdpPercap)\n\n# A tibble: 1,704 × 3\n   country      year gdpPercap\n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952      779.\n 2 Afghanistan  1957      821.\n 3 Afghanistan  1962      853.\n 4 Afghanistan  1967      836.\n 5 Afghanistan  1972      740.\n 6 Afghanistan  1977      786.\n 7 Afghanistan  1982      978.\n 8 Afghanistan  1987      852.\n 9 Afghanistan  1992      649.\n10 Afghanistan  1997      635.\n# ℹ 1,694 more rows\n\n\nIt might be confusing why we don’t need quotation marks around the column names like we do in other languages or even other parts of R.\nThis concept is known as quasiquotation or data masking. It is quite unique to R, but it allows functions to known about the content of the data that is passed to them and use this as the environment in which they do their computations and search for variable names. So while the variable country doesn’t exist in the global environment, it does exist as a column of the gapminder tibble.\n\ndplyr functions always look in the data first when they search for names.\n\nThe help page for select tells us more about the different ways in which we can select columns. Here are a couple of examples without the output, run them in your R session to confirm that they do what you think they do (but do have a look at the help pages yourselves, they are quite well written).\n\nselect(gapminder, year:pop)\nselect(gapminder, starts_with(\"co\"))\nselect(gapminder, where(is.numeric))\nselect(gapminder, where(is.character))\nselect(gapminder, c(1, 3, 4))\n\n\n2.4.2 filter\nAfter selecting columns it is only natural to ask how to select rows. This is achieved with the function filter.\n\n\nFilter data. Artwork by Allison Horst\n\nHere, we select all rows, where the year is greater than 2000 and the country is New Zealand.\n\nfilter(gapminder, year &gt; 2000, country == \"New Zealand\")\n\nBecause text comparisons are cases sensitive, we would have missed New Zealand had we written it with lowercase letters. In order to make sure we find the correct country, it can be helpful to simply convert all country names to lower case, and in fact we can use functions on our columns straight inside of any dplyr verb. Functions that deal with text (strings or character in R’s language) in the tidyverse start with str_, so they are easy to find with autocompletion.\n\nfilter(gapminder, year &gt; 2000, str_to_lower(country) == \"new zealand\")\n\nInstead of combining conditions with , (which works the same as & here), we can also use | meaning or. Here, we get all rows where the country is New Zealand or the country is Afghanistan.\n\nfilter(gapminder, country == \"New Zealand\" | country == \"Afghanistan\")\n\nThis particular comparison can be written more succinctly, by asking (for every row), is the particular country %in% this vector?\n\nfilter(gapminder, country %in% c(\"New Zealand\", \"Afghanistan\"))\n\n\n2.4.3 mutate\nWe are back at manipulating columns, this time by creating new ones or changing old ones. The dplyr verb that does that is called mutate. For example, we might want to calculate the total GDP from the GDP per Capita and the population:\n\nmutate(gapminder, gdp = pop * gdpPercap)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap          gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\nNotice, that none of the functions changed the original variable gapminder. They only take an input and return and output, which makes it easier to reason about our code and later chain pieces of code together. How do you change it then? Use the Force! … ahem, I mean, the assignment operator (&lt;-).\n\ngapminder &lt;- mutate(gapminder, gdp = pop * gdpPercap)\n\nHere, the power of dplyr shines. It knows that pop and gdpPercap are columns of the tibble and that gdp refers to the new name of the freshly created column.\n\n2.4.4 Interlude: Begind the magic, handling data with base-R\nThis section is meant to show you what happens behind the scenes. It is not strictly necessary to understand all the details of it in order to work effectively with the tidyverse, but it helps especially when things don’t go as planned.\nLet’s create a tibble to play with:\n\ntest_tibble &lt;- tibble(\n  x = 1:5,\n  y = x ^ 2,\n  z = c(\"hello\", \"world\", \"test\", \"four\", \"five\")\n)\n\ntest_tibble\n\n# A tibble: 5 × 3\n      x     y z    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     1 hello\n2     2     4 world\n3     3     9 test \n4     4    16 four \n5     5    25 five \n\n\nInstead of the tidyverse functions, we can also use what is called subsetting, getting a subset of our datasctructure, with square brackets:\n\ntest_tibble[c(1, 3)]\n\n# A tibble: 5 × 2\n      x z    \n  &lt;int&gt; &lt;chr&gt;\n1     1 hello\n2     2 world\n3     3 test \n4     4 four \n5     5 five \n\n\nThis selected the first and third column. This also works for lone vectors:\n\nevens &lt;- seq(0, 10, 2)\nevens[c(1, 3)]\n\n[1] 0 4\n\n\nIf we want to select columns by their names without the tidyverse, we have to pass these names as a character vector (hence the quotation marks).\n\ntest_tibble[c(\"x\", \"z\")]\n\n# A tibble: 5 × 2\n      x z    \n  &lt;int&gt; &lt;chr&gt;\n1     1 hello\n2     2 world\n3     3 test \n4     4 four \n5     5 five \n\n\nIf we have two things in the square brackets, separated by a comma, the first refers to the rows and the second refers to the columns. e.g. this would be “the first row and the columns from 1 to 2”:\n\ntest_tibble[1, 1:2]\n\n# A tibble: 1 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n\n\nInternally, tibbles / dataframes are lists of columns. Lists have more ways of accessing their elements. The $ symbol gets us an element from the list:\n\ntest_tibble$x\n\n[1] 1 2 3 4 5\n\n\nIf we want to use numbers (=indices) to get a single element from a list (or a column from a tibble), we have ot use double square brackets:\n\ntest_tibble[[1]]\n\n[1] 1 2 3 4 5\n\n\nThe reason for this: Single square brackets give us a subset of the list, which is still packed up in a list. If we want to unpack it to work with it we need the content of just one element [[ does that for us.\nThe pull function from the tidyverse works like $.\n\npull(test_tibble, x)\n\n[1] 1 2 3 4 5\n\n\nSubsetting not only works for looking at things, it also allows us to replace the part we are subsetting:\n\nx &lt;- 1:10\nx[1] &lt;- 42\nx\n\n [1] 42  2  3  4  5  6  7  8  9 10\n\n\n\nNote:\nThe base-R and the tidyverse way are not mutually exclusive. Sometimes you can mix and match.\n\n\n2.4.5 The pipe %&gt;%\n\nThe tidyverse functions are easier to compose (i.e. chain together). To facilitate this, we introduce another operator, a bit like + for numbers or the + to add ggplot components, but specially for functions. The pipe, which you can either type or insert in RStudio with Ctrl+Shift+M, takes it’s left side and passes it as the first argument to the function on the right side\nWhy is this useful? Imagine our data processing involves a bunch of steps, so we save the output to intermediate variables.\n\nsubset_gapminder &lt;- select(gapminder, country, year, pop)\nfiltered_gapminder &lt;- filter(subset_gapminder, year &gt; 200)\nfinal_gapminder &lt;- mutate(filtered_gapminder, pop_thousands = pop / 1000)\nfinal_gapminder\n\n# A tibble: 1,704 × 4\n   country      year      pop pop_thousands\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan  1952  8425333         8425.\n 2 Afghanistan  1957  9240934         9241.\n 3 Afghanistan  1962 10267083        10267.\n 4 Afghanistan  1967 11537966        11538.\n 5 Afghanistan  1972 13079460        13079.\n 6 Afghanistan  1977 14880372        14880.\n 7 Afghanistan  1982 12881816        12882.\n 8 Afghanistan  1987 13867957        13868.\n 9 Afghanistan  1992 16317921        16318.\n10 Afghanistan  1997 22227415        22227.\n# ℹ 1,694 more rows\n\n\nHowever, we don’t really need those intermediate variables and they just clutter our code. The pip allows us to express our data processing as a series of steps:\n\nfinal_gapminder &lt;- gapminder %&gt;% \n  select(country, year, pop) %&gt;% \n  filter(year &gt; 2000) %&gt;% \n  mutate(pop_thousands = pop / 1000)\n\nfinal_gapminder\n\n# A tibble: 284 × 4\n   country      year      pop pop_thousands\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan  2002 25268405        25268.\n 2 Afghanistan  2007 31889923        31890.\n 3 Albania      2002  3508512         3509.\n 4 Albania      2007  3600523         3601.\n 5 Algeria      2002 31287142        31287.\n 6 Algeria      2007 33333216        33333.\n 7 Angola       2002 10866106        10866.\n 8 Angola       2007 12420476        12420.\n 9 Argentina    2002 38331121        38331.\n10 Argentina    2007 40301927        40302.\n# ℹ 274 more rows\n\n\nYou can read the pipe in your head as “and then” or “take … pass it into …”.\nAnd because all main tidyverse functions take data as their first argument, we can chain them together fluently Additionally, it enables autocompletion of column names inside of the function that gets the data.\nNext to the tidyverse pipe %&gt;%, you might also see |&gt; at some point. The latter is a pipe that was introduced to base-R because this whole piping thing got so popular they are making it part of the core language.\n\n2.4.6 arrange\nA simple thing you might want from a table is to sort it based on some column. This is what arrange does:\n\ngapminder %&gt;% \n  arrange(year)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap           gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.   6567086330.\n 2 Albania     Europe     1952    55.2  1282697     1601.   2053669902.\n 3 Algeria     Africa     1952    43.1  9279525     2449.  22725632678.\n 4 Angola      Africa     1952    30.0  4232095     3521.  14899557133.\n 5 Argentina   Americas   1952    62.5 17876956     5911. 105676319105.\n 6 Australia   Oceania    1952    69.1  8691212    10040.  87256254102.\n 7 Austria     Europe     1952    66.8  6927772     6137.  42516266683.\n 8 Bahrain     Asia       1952    50.9   120447     9867.   1188460759.\n 9 Bangladesh  Asia       1952    37.5 46886859      684.  32082059995.\n10 Belgium     Europe     1952    68    8730405     8343.  72838686716.\n# ℹ 1,694 more rows\n\n\nThe helper function desc marks a column to be arranged in descending order. We can arrange by multiple columns, where the first will be most important.\n\ngapminder %&gt;% \n  arrange(desc(year), pop)\n\n# A tibble: 1,704 × 7\n   country               continent  year lifeExp     pop gdpPercap          gdp\n   &lt;chr&gt;                 &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Sao Tome and Principe Africa     2007    65.5  199579     1598.   319014077.\n 2 Iceland               Europe     2007    81.8  301931    36181. 10924101861.\n 3 Djibouti              Africa     2007    54.8  496374     2082.  1033689705.\n 4 Equatorial Guinea     Africa     2007    51.6  551201    12154.  6699346424.\n 5 Montenegro            Europe     2007    74.5  684736     9254.  6336475807.\n 6 Bahrain               Asia       2007    75.6  708573    29796. 21112675360.\n 7 Comoros               Africa     2007    65.2  710960      986.   701111696.\n 8 Reunion               Africa     2007    76.4  798094     7670.  6121478793.\n 9 Trinidad and Tobago   Americas   2007    69.8 1056608    18009. 19027934931.\n10 Swaziland             Africa     2007    39.6 1133066     4513.  5114071458.\n# ℹ 1,694 more rows\n\n\n\n2.4.7 summarise\nTo condense one or multiple columns into summary values, we use summarise. Like with mutate, we can calculate multiple things in one step.\n\ngapminder %&gt;% \n  summarise(\n    max_year = min(year),\n    pop = max(pop),\n    mean_life_expectancy = mean(lifeExp)\n  )\n\n# A tibble: 1 × 3\n  max_year        pop mean_life_expectancy\n     &lt;dbl&gt;      &lt;dbl&gt;                &lt;dbl&gt;\n1     1952 1318683096                 59.5\n\n\nBut condensing whole columns into one value, flattening the tibble in the style of Super Mario jumping on mushrooms, is often not what we need. We would rather know the summaries within certain groups. For example the maximal gdp per country. This is what group_by is for.\n\n2.4.8 group_by\ngroup_by is considered an adverb, because it doesn’t change the data itself but it changes how subsequent functions handle the data. For example, if a tibble has groups, all summaries are calculated within these groups:\n\ngapminder %&gt;% \n  group_by(year) %&gt;% \n  summarise(\n    lifeExp = mean(lifeExp)\n  )\n\n# A tibble: 12 × 2\n    year lifeExp\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1952    49.1\n 2  1957    51.5\n 3  1962    53.6\n 4  1967    55.7\n 5  1972    57.6\n 6  1977    59.6\n 7  1982    61.5\n 8  1987    63.2\n 9  1992    64.2\n10  1997    65.0\n11  2002    65.7\n12  2007    67.0\n\n\nsummarize removes one level of grouping. If the data was grouped by multiple features, this means that some groups remain. We can make sure that the data is no longer grouped with ungroup.\n\ngapminder %&gt;% \n  group_by(year, continent) %&gt;% \n  summarise(\n    lifeExp = mean(lifeExp)\n  ) %&gt;%\n  ungroup()\n\n# A tibble: 60 × 3\n    year continent lifeExp\n   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1  1952 Africa       39.1\n 2  1952 Americas     53.3\n 3  1952 Asia         46.3\n 4  1952 Europe       64.4\n 5  1952 Oceania      69.3\n 6  1957 Africa       41.3\n 7  1957 Americas     56.0\n 8  1957 Asia         49.3\n 9  1957 Europe       66.7\n10  1957 Oceania      70.3\n# ℹ 50 more rows\n\n\nGroups also work within mutate and filter. For example, we can get all rows where the gdp per Person was highest per country:\n\ngapminder %&gt;%\n  group_by(country) %&gt;% \n  filter(gdpPercap == max(gdpPercap))\n\n# A tibble: 142 × 7\n# Groups:   country [142]\n   country     continent  year lifeExp       pop gdpPercap           gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1982    39.9  12881816      978.  12598563401.\n 2 Albania     Europe     2007    76.4   3600523     5937.  21376411360.\n 3 Algeria     Africa     2007    72.3  33333216     6223. 207444851958.\n 4 Angola      Africa     1967    36.0   5247469     5523.  28980597822.\n 5 Argentina   Americas   2007    75.3  40301927    12779. 515033625357.\n 6 Australia   Oceania    2007    81.2  20434176    34435. 703658358894.\n 7 Austria     Europe     2007    79.8   8199783    36126. 296229400691.\n 8 Bahrain     Asia       2007    75.6    708573    29796.  21112675360.\n 9 Bangladesh  Asia       2007    64.1 150448339     1391. 209311822134.\n10 Belgium     Europe     2007    79.4  10392226    33693. 350141166520.\n# ℹ 132 more rows\n\n\n\ngapminder %&gt;% \n  group_by(year) %&gt;%\n  mutate(pop = pop / sum(pop))\n\n# A tibble: 1,704 × 7\n# Groups:   year [12]\n   country     continent  year lifeExp     pop gdpPercap          gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8 0.00350      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3 0.00347      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 0.00354      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 0.00359      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 0.00366      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 0.00379      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 0.00300      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 0.00296      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 0.00319      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 0.00403      635. 14121995875.\n# ℹ 1,694 more rows\n\n\n\n2.4.9 others:\nWe can rename columns with rename:\n\ngapminder %&gt;% \n  rename(population = pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp population gdpPercap          gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8    8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3    9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0   10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0   11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1   13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4   14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9   12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8   13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7   16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8   22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\nSometimes you want to refer to the size of the current group inside of mutate or summarise. The function to to just that is called n(). For example, I wonder how many rows of data we have per year.\n\ngapminder %&gt;% \n  group_by(year) %&gt;% \n  mutate(n = n())\n\n# A tibble: 1,704 × 8\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap          gdp     n\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.   142\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.   142\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.   142\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.   142\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.   142\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.   142\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.   142\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.   142\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.   142\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.   142\n# ℹ 1,694 more rows\n\n\nA shortcut for group_by and summarise with n() is the count function:\n\ngapminder %&gt;% \n  count(year, country) %&gt;% \n  count(n)\n\n# A tibble: 1 × 2\n      n    nn\n  &lt;int&gt; &lt;int&gt;\n1     1  1704\n\n\nIn general, you might find after solving a particular problem in a couple of steps that there is a more elegant solution. Do not be discouraged by that! It simply means that there is always more to learn, but the tools you already know by now will get you a very long way and set you on the right track.\nI think we learned enough dplyr verbs for now. We can treat ourselves to a little ggplot visualization."
  },
  {
    "objectID": "02-data-wrangling.html#visualization-and-our-first-encounter-with-factors",
    "href": "02-data-wrangling.html#visualization-and-our-first-encounter-with-factors",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.5 Visualization and our first encounter with factors",
    "text": "2.5 Visualization and our first encounter with factors\n \n\ngapminder %&gt;% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line() +\n  facet_wrap(~continent)\n\n\n\n\nThe facet_wrap function slices our plot into theses subplots, a style of plot sometimes referred to as small multiples. At this point you might wonder: “How do I control the order of these facets?” The answer is: With a factor!\nAny time we have a vector that can be thought of as representing discrete categories (ordered or unordered), we can express this by turning the vector into a factor with the factor function. This enables R’s functions to handle them appropriately. Let’s create a little example. We start out with a character vector.\n\nanimals &lt;- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals &lt;- factor(animals)\nanimals\n\n[1] cat   dog   bear  shark\nLevels: bear cat dog shark\n\n\nNote the new information R gives us, the Levels, which is all possible values we can put into the factor. They are automatically ordered alphabetically on creation. We can also pass a vector of levels on creation.\n\nanimals &lt;- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals &lt;- factor(animals, levels = c(\"cat\", \"dog\"), ordered = TRUE)\nanimals\n\n[1] cat  dog  &lt;NA&gt; &lt;NA&gt;\nLevels: cat &lt; dog\n\n\nA factor can only contain elements that are in the levels, so because I omitted the whale shark, it will be turned into NA. The tidyverse contains the forcats package to help with factors. Most functions from this package start with fct_.\nFor example, the fct_relevel function, which keeps all levels but let’s us change the order:\n\nanimals &lt;- c(\"cat\", \"dog\", \"bear\", \"shark\")\nanimals &lt;- factor(animals)\nfct_relevel(animals, c(\"shark\", \"dog\"))\n\n[1] cat   dog   bear  shark\nLevels: shark dog bear cat\n\n\nUsing this in action, we get:\n\ngapminder %&gt;% \n  mutate(continent = fct_relevel(continent, \"Oceania\")) %&gt;% \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line(alpha = 0.3) +\n  facet_wrap(~ continent)\n\n\n\n\nNote: fct_relevel might be a very constructed example. More often you will need its cousin fct_reoder to reorder a factor by the values of some other column.\nLet’s make this plot a bit prettier by adding color! The gapminder package that provided this dataset also included a nice color palette. I included it as a .csv file in the data/ folder so that we can practice importing data once more. But you could also take the shortcut of getting it straight from the package (gapminder::country_colors). Here, we are using the head function to look at the first couple of rows of the tibble and to look at the first couple of elements of the named vector from the package.\n\ncountry_colors &lt;- read_csv(\"data/02/country_colors.csv\")\ncolor &lt;- country_colors$color\nnames(color) &lt;- country_colors$country\nhead(color)\n\n         Nigeria            Egypt         Ethiopia Congo, Dem. Rep. \n       \"#7F3B08\"        \"#833D07\"        \"#873F07\"        \"#8B4107\" \n    South Africa            Sudan \n       \"#8F4407\"        \"#934607\" \n\n\nHaving a named vector means that we can access individual elements by their names, and ggplot can use those names to match up for example colors with countries when we pass it to a scale_ function.\n\nx &lt;- c(first = 1, second = 3, hello = 5)\nx[\"first\"]\n\nfirst \n    1 \n\n\nTo the final plot we also add guides(color = \"none\"), because if we were to show a guide (for discrete colors typically a legend), it would fill up the whole plot.\n\ngapminder %&gt;% \n  mutate(continent = fct_relevel(continent, c(\"Oceania\"))) %&gt;% \n  ggplot(aes(year, lifeExp, color = country)) +\n  geom_line() +\n  facet_wrap(~continent) +\n  guides(color = \"none\") +\n  scale_color_manual(values = color)"
  },
  {
    "objectID": "02-data-wrangling.html#exercises",
    "href": "02-data-wrangling.html#exercises",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.6 Exercises",
    "text": "2.6 Exercises\n\nDrink a cup of coffee or tea, relax, because you just worked through quite a long video.\nFamiliarize yourself with the folders on your computer. Make sure you understand, where your directories and files live.\nDownload the data for today in one of the ways taught. You can refer to the script anytime.\nThe file ./data/02/exercise1.txt is in an unfamiliar format.\n\nFind out how it is structured and read it in with readr.\nCreate a scatterplot of the x and y column with ggplot.\nLook at the help page for geom_point. What is the difference between geom_point(aes(color = &lt;something&gt;)) and geom_point(color = &lt;something&gt;)? A relevant hint is in the section about the ...-argument.\nMake the plot pretty by coloring the points, keeping in mind the above distinction.\n\n\nRead in the gapminder dataset with readr\n\nUsing a combination of dplyr verbs and / or visualizations with ggplot, answer the following questions:\nWhich continent had the highest life expectancy on average in the most current year? There are two options here. First, calculate a simple mean for the countries in each continent. Then, remember that the countries have different population sizes, so we really need a weighted mean using R’s function weighted.mean().\nIs there a relationship between the GDP per capita and the life expectancy? A visualization might be helpful.\nHow did the population of the countries change over time? Make the plot more informative by adding color, facets and labels (with geom_text). Can you find out, how to add the country name label only to the last year? Hint: Have a look at the data argument that all geom_-functions have."
  },
  {
    "objectID": "02-data-wrangling.html#resources",
    "href": "02-data-wrangling.html#resources",
    "title": "\n2  Data Wrangling\n",
    "section": "\n2.7 Resources",
    "text": "2.7 Resources\nDon’t miss the dedicated Resources page.\n\n2.7.1 Package Documentation\n\nThe tidyverse website\nThe readr package website with cheatsheet\nThe dplyr package website with cheatsheet\n\n2.7.2 Getting Help\n\nHow to find help\nR4DS online learning community\n\n\n\n\n\nBryan, Jennifer. 2017. Gapminder: Data from Gapminder. Manual.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1 edition. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "03-tidy-data.html#tidy-data",
    "href": "03-tidy-data.html#tidy-data",
    "title": "\n3  Tidy data\n",
    "section": "\n3.1 Tidy data",
    "text": "3.1 Tidy data\nLet’s get started with this pivotal topic.\n\n3.1.1 What and why is tidy data?\nThere is one concept which also lends it’s name to the tidyverse that I want to talk about. Tidy Data is a way of turning your datasets into a uniform shape. This makes it easier to develop and work with tools because we get a consistent interface. Once you know how to turn any dataset into a tidy dataset, you are on home turf and can express your ideas more fluently in code. Getting there can sometimes be tricky, but I will give you the most important tools.\nIn tidy data, each variable (feature) forms it’s own column. Each observation forms a row. And each cell is a single value (measurement). Furthermore, information about the same things belongs in one table.\n\n\nFigure from https://r4ds.had.co.nz/tidy-data.html Wickham and Grolemund (2017)\n\n\n3.1.2 Make data tidy\n \nwith the tidyr package.\n\n“Happy families are all alike; every unhappy family is unhappy in its own way”\n— Leo Tolstoy (https://tidyr.tidyverse.org/articles/tidy-data.html)\n\nAnd this quote holds true for messy datasets as well. The tidyr package contained in the tidyverse provides small example datasets to demonstrate what this means in practice. Hadley Wickham and Garrett Grolemund use these in their book as well (https://r4ds.had.co.nz/tidy-data.html)(Wickham and Grolemund 2017).\nLet’s make some data tidy!\ntable1, table2, table3, table4a, table4b, and table5 all display the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. The first of these is in the tidy format, the others are not:\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nThis nicely qualifies as tidy data. Every row is uniquely identified by the country and year, and all other columns are properties of the specific country in this specific year.\n\n3.1.3 pivot_wider\n\nNow it gets interesting. table2 still looks organized, but it is not tidy (by our definition). Note, this doesn’t say the format is useless — it has it’s places — but it will not fit in as snugly with our tools. The column type is not a feature of the country, rather the actual features are hidden in that column with their values in the count column.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nIn order to make it tidy, this dataset would needs become wider.\n\ntable2 %&gt;% \n  pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n3.1.4 separate\n\nIn table3, two features are jammed into one column. This is annoying, because we can’t easily calculate with the values; they are stored as text and separated by a slash like cases/population.\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nIdeally, we would want to separate this column into two:\n\ntable3 %&gt;% \n  separate(col = rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n3.1.5 pivot_longer\n\ntable4a and table4b split the data into two different tables, which again makes it harder to calculate with. This data is so closely related, we would want it in one table. And another principle of tidy data is violated. Notice the column names? 1999 is not a feature that Afghanistan can have. Rather, it is the value for a feature (namely the year), while the values in the 1999 column are in fact values for the feature population (in table4a) and cases (in table4b).\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n\ntable4a %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ntable4b %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\nWe have another case where we are doing a very similar thing twice. There is a general rule of thumb that says:\n\n“If you copy and paste the same code 3 times, you should probably write a function.”\n\nThis no only has the advantage of reducing code duplication and enabling us to potentially reuse our code later in another project, it also aids readability because we are forced to give this stop a name.\n\nclean_wide_data &lt;- function(data, values_column) {\n  data %&gt;% \n    pivot_longer(-country, names_to = \"year\", values_to = values_column)\n}\n\nWe can then use this function for both tables.\n\nclean4a &lt;- table4a %&gt;% \n  clean_wide_data(\"cases\")\n\n\nclean4b &lt;- table4b %&gt;% \n  clean_wide_data(\"population\")\n\n\n3.1.6 left_join\n\nNow is the time to join clean4a and clean4b together. For this, we need an operation known from databases as a join. In fact, this whole concept of tidy data is closely related to databases and something called Codd`s normal forms (Codd 1990; Wickham 2014) so I am throwing these references in here just in case you are interested in the theoretical foundations. But without further ado:\n\nleft_join(clean4a, clean4b, by = c(\"country\", \"year\"))\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n3.1.7 unite\n\nIn table5, we have the same problem as in table3 and additionally the opposite problem! This time, feature that should be one column (namely year) is spread across two columns (century and year).\n\ntable5\n\n# A tibble: 6 × 4\n  country     century year  rate             \n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 19      99    745/19987071     \n2 Afghanistan 20      00    2666/20595360    \n3 Brazil      19      99    37737/172006362  \n4 Brazil      20      00    80488/174504898  \n5 China       19      99    212258/1272915272\n6 China       20      00    213766/1280428583\n\n\nWhat we want to do is unite those into one, and also deal with the other problem. However, we when do this we find out the our newly created year, cases and population columns are actually stored as text, not numbers! So in the next step, we convert those into numbers with the parse_number function.\n\ntable5 %&gt;% \n  unite(\"year\", century, year, sep = \"\") %&gt;% \n  separate(rate, c(\"cases\", \"population\")) %&gt;% \n  mutate(\n    year = parse_number(year),\n    cases = parse_number(cases),\n    population = parse_number(population)\n  )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nparse_number is a bit like a less strict version of as.numeric. While as.numeric can only deal with text that contains only a number and nothing else, parse_number can help us by extracting numbers even if there is some non-number text around them:\n\nas.numeric(\"we have 42 sheep\")\n\n[1] NA\n\n\nparse_number handles this, no questions asked:\n\nparse_number(\"we have 42 sheep\")\n\n[1] 42\n\n\nNotice, how we applied the same function parse_number to multiple columns of our data? If we notice such a pattern, where there is lot’s of code repetition, chances are that there is a more elegant solution. You don’t have to find that elegant solution at first try, but keeping an open mind will improve your code in the long run. In this case, let me tell you about the across function. We can use it inside of dplyr verbs such as mutate and summarise to apply a function to multiple columns:\n\ntable5 %&gt;% \n  unite(\"year\", century, year, sep = \"\") %&gt;% \n  separate(rate, c(\"cases\", \"population\")) %&gt;% \n  mutate(\n    across(c(year, cases, population), parse_number)\n  )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nAs it’s first argument it takes a vector of column names (the c(...) bit) or a tidy-select specification (see ?dplyr_tidy_select) and as it’s second argument either one function or even a list of functions (with names).\nAnother way of specifying what columns to use here would be to say “every column but the country” with -country.\n\ntable5 %&gt;% \n  unite(\"year\", century, year, sep = \"\") %&gt;% \n  separate(rate, c(\"cases\", \"population\")) %&gt;% \n  mutate(\n    across(-country, parse_number)\n  )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n3.1.8 Another example\nLet us look at one last example of data that needs tidying, which is also provided by the tidyr package as an example:\n\nhead(billboard)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nThis is a lot of columns! For 76 weeks after a song entered the top 100 (I assume in the USA) its position is recorded. It might be in this format because it made data entry easier, or the previous person wanted to make plots in excel, where this wide format is used to denote multiple traces. In any event, for our style of visualizations with the grammar of graphics, we want a column to represent a feature, so this data needs to get longer:\n\nbillboard %&gt;% \n  pivot_longer(starts_with(\"wk\"), names_to = \"week\", values_to = \"placement\") %&gt;% \n  mutate(week = parse_number(week)) %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  artist track                   date.entered  week placement\n  &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1        87\n2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2        82\n3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3        72\n4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4        77\n5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5        87\n6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6        94\n\n\nLet’s save this to a variable. And while we are at it, we can save the extra mutate-step by performing the transformation from text to numbers right inside of the pivot_longer function.\n\ntidy_bilboard &lt;- billboard %&gt;% \n  pivot_longer(starts_with(\"wk\"),\n    names_to = \"week\",\n    values_to = \"placement\",\n    names_prefix = \"wk\",\n    names_transform = list(week = as.integer)\n  )\n\ntidy_bilboard %&gt;% head(10)\n\n# A tibble: 10 × 5\n   artist track                   date.entered  week placement\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;int&gt;     &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1        87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2        82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3        72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4        77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5        87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6        94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26       7        99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26       8        NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26       9        NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26      10        NA\n\n\nYes, those pivot functions are really powerful!\nA notable difference that often happens between long- and wide-format data is the way missing data is handled.\nBecause every row needs to have the same number of columns, and in the wide format every column is a week, there are bound to be a lot of NA values wherever a song was simply no longer in the top 100 at the specified week. Those missing values are then very explicit.\nIn the long format we have the option to make the missing values implicit by simply omitting the row where there is no meaningful information. With the function na.omitt for example, we can remove all rows that have NA somewhere:\n\ntidy_bilboard %&gt;%\n  head(10) %&gt;% \n  na.omit()\n\n# A tibble: 7 × 5\n  artist track                   date.entered  week placement\n  &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;int&gt;     &lt;dbl&gt;\n1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1        87\n2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2        82\n3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3        72\n4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4        77\n5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5        87\n6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6        94\n7 2 Pac  Baby Don't Cry (Keep... 2000-02-26       7        99\n\n\nLet’s reward ourselves with a little visualization. Here, I am also introducing the plotly package, which has the handy function ggplotly to turn a regular ggplot into an interactive plot. Plotly also has its own way of building plots, which you might want to check out for advanced interactive or 3-dimensional plots: https://plotly.com/r/, but for the most part we don’t need to worry about it due to the amazingly simple ggplotly translation function.\n\nplt &lt;- tidy_bilboard %&gt;% \n  ggplot(aes(week, placement)) +\n  geom_point(aes(label = paste(artist, track))) +\n  geom_line(aes(group = paste(artist, track)))\n\nplotly::ggplotly(plt)\n\n\n\n\n\nThis whole tidy data idea might seem like just another way of moving numbers around. But once you build the mental model for it, it will truly transform the way you are able to think about data. Both for data wrangling with dplyr, as shown last week, and also for data visualization with ggplot, a journey we began in the first week and that is still well underway."
  },
  {
    "objectID": "03-tidy-data.html#more-shapes-for-data",
    "href": "03-tidy-data.html#more-shapes-for-data",
    "title": "\n3  Tidy data\n",
    "section": "\n3.2 More shapes for data",
    "text": "3.2 More shapes for data\nData comes in many shapes and R has more than just vectors and dataframes / tibbles. I this course we are omitting matrices, which store data of the same type in 2 dimensions, and it’s multi-dimensional equivalent arrays.\nWhat we are not omitting, and in fact have already teased but never properly defined is lists.\n\n3.2.1 Lists\nOn first glance, lists are very similar to atomic vectors, as they are both one dimensional data structures and both of them can have names.\n\nc(first = 1, second = 2)\n\n first second \n     1      2 \n\n\n\nlist(first = 1, second = 2)\n\n$first\n[1] 1\n\n$second\n[1] 2\n\n\nWhat sets them apart is that while atomic vectors can only contain data of the same type (like only numbers or only text), a list can contain anything, even other lists!\n\nx &lt;- list(first = 1, second = 2, \"some text\", list(1, 2), 1:5)\nx\n\n$first\n[1] 1\n\n$second\n[1] 2\n\n[[3]]\n[1] \"some text\"\n\n[[4]]\n[[4]][[1]]\n[1] 1\n\n[[4]][[2]]\n[1] 2\n\n\n[[5]]\n[1] 1 2 3 4 5\n\n\nAs it turns out, dataframes internally are also lists, namely a list of columns. They just have some more properties (which R calls attributes) that tell R to display it in the familiar rectangular shape.\n\npalmerpenguins::penguins %&gt;% head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n3.2.2 Nested data\nThe tidyr package provides more tools for dealing with data in various shapes. We just discovered the first set of operations called pivots and joins to get a feel for tidy data and to obtain it from various formats. But data is not always rectangular like we can show it in a spreadsheet. Sometimes data already comes in a nested form, sometimes we create nested data because it serves our purpose. So, what do I mean by nested?\nRemember that lists can contain elements of any type, even other lists. If we have a list that contains more lists, we call it nested e.g.\n\nlist(\n  c(1, 2),\n  list(\n    42, list(\"hi\", TRUE)\n  )\n)\n\nBut nested list are not always fun to work with, and when there is a straightforward way to represent the same data in a rectangular, flat format, we most likely want to do that. We will deal with data rectangling today was well. But first, there is another implication of nested lists:\nBecause dataframes (and tibbles) are built on top of lists, we can nest them to! This can sometimes come in really handy. We a dataframe contains a column that is not an atomic vector but a list (so it is a list in a list), we call it a list column:\n\nexample &lt;- tibble(\n  x = 1:3,\n  y = list(\n    \"hello\",\n    TRUE,\n    1:4\n  )\n)\n\nexample\n\n# A tibble: 3 × 2\n      x y        \n  &lt;int&gt; &lt;list&gt;   \n1     1 &lt;chr [1]&gt;\n2     2 &lt;lgl [1]&gt;\n3     3 &lt;int [4]&gt;\n\n# View(example)\n\nUse the View function, or the click in the environment panel to inspect the nested data with a better overview.\n\nOf course we are unlikely to build these nested tibbles by hand with the tibble function. Instead, our data usually comes from some dataset we are working with. Let’s take the familiar penguins dataset and nest it.\n\nnested &lt;- palmerpenguins::penguins %&gt;% \n  nest(data = -island)\n\nnested\n\n# A tibble: 3 × 2\n  island    data              \n  &lt;fct&gt;     &lt;list&gt;            \n1 Torgersen &lt;tibble [52 × 7]&gt; \n2 Biscoe    &lt;tibble [168 × 7]&gt;\n3 Dream     &lt;tibble [124 × 7]&gt;\n\n\nnest has a syntax similar to mutate, where we first specify the name of the column to create (we call it data here), followed by a specification of the columns to nest into that list column.\nOur data column is now a list of tibbles and each individual tibble in the list contains the data for the species of that row. Looking into the data column’s first element, we can see that it is indeed a regular tibble and didn’t take it personal to get stuffed into a list column.\n\nnested$data[[1]]\n\n# A tibble: 52 × 7\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 42 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\nTo unnest the column again we use the function unnest. Sometimes we need to be specific and use unnest_wider or unnest_longer, but here the automatic unnest makes the right choices already.\n\nnested %&gt;% \n  unnest(data)\n\n# A tibble: 344 × 8\n   island    species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Torgersen Adelie            39.1          18.7               181        3750\n 2 Torgersen Adelie            39.5          17.4               186        3800\n 3 Torgersen Adelie            40.3          18                 195        3250\n 4 Torgersen Adelie            NA            NA                  NA          NA\n 5 Torgersen Adelie            36.7          19.3               193        3450\n 6 Torgersen Adelie            39.3          20.6               190        3650\n 7 Torgersen Adelie            38.9          17.8               181        3625\n 8 Torgersen Adelie            39.2          19.6               195        4675\n 9 Torgersen Adelie            34.1          18.1               193        3475\n10 Torgersen Adelie            42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "03-tidy-data.html#exercises",
    "href": "03-tidy-data.html#exercises",
    "title": "\n3  Tidy data\n",
    "section": "\n3.3 Exercises",
    "text": "3.3 Exercises\n\n3.3.1 Tidy data\nFor the first set of exercises I am cheating a little and take those from the (absolutely brilliant) book R for Data Science (Wickham and Grolemund 2017) by the original creator of much of the tidyverse. So, for the first part, solve / answer the 4 questions found here: https://r4ds.had.co.nz/tidy-data.html#exercises-24\nI do have to give another hint, because I haven’t mentioned it so far: When I introduced variables I told you that those can only contain letters, underscores and numbers and are not allowed to start with a number. However, we can use “illegal” names for variables and columns if the surround them with backticks, e.g.:\n\n`illegal variable` &lt;- 42\n`illegal variable`\n\n[1] 42\n\n\nThis is how Hadley can refer to the columns named after years in pivot_longer in exercise 1.\n\n3.3.2 A new dataset: airlines\n\n\n\nImagine for a second this whole pandemic thing is not going on and we are planning a vacation. Of course, we want to choose the safest airline possible. So we download data about incident reports. You can find it in the ./data/03/ folder.\nInstead of the type_of_event and n_events columns we would like to have one column per type of event, where the values are the count for this event.\nWhich airlines had the least fatal accidents? What happens if we standardized these numbers to the distance theses airlines covered in the two time ranges?\nWhich airlines have the best record when it comes to fatalities per fatal accident?\nCreate informative visualizations and / or tables to communicate your discoveries. It might be beneficial to only plot e.g. the highest or lowest scoring Airlines. One of the slice_ functions will help you there. And to make your plot more organized, you might want to have a look into fct_reorder."
  },
  {
    "objectID": "03-tidy-data.html#resources",
    "href": "03-tidy-data.html#resources",
    "title": "\n3  Tidy data\n",
    "section": "\n3.4 Resources",
    "text": "3.4 Resources\n\ntidyr documentation\npurrr documentation\n\nstringr documentation for working with text and a helpful cheatsheet for the regular expressions mentioned in the video\n\n\n\n\n\nCodd, E. F. 1990. The Relational Model for Database Management: Version 2. USA: Addison-Wesley Longman Publishing Co., Inc.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1 edition. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "04-functional-programming.html#todays-goal",
    "href": "04-functional-programming.html#todays-goal",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.1 Todays goal",
    "text": "4.1 Todays goal\nMy goal today is to bring together everything we learned so far and solidify our understanding of wrangling data in the tidyverse. If all goes according to plan, we will then have more mental capacity freed up for the statistics starting next week. And our understanding of data will hopefully enable us to experiment and play with statistical concepts without getting stuck too much on data wrangling. This also means that today’s lecture might be the most challenging so far, because everything learned up until now will – in one way or another – be relevant.\nBut first, we load the libraries for today as usual. Note, that I am cheating a bit here by loading the gapminder package as well. Even though we will be reading in actual gapminder dataset again from files today, having access to the vector of country colors is nice to have.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nIn the lecture I also quickly go over some of the most important resources so far. This mostly concerns the pure R resources, I will cover the resources for statistics and some of the maths involved in the next couple of lectures.\nAs you might be able to tell, mental models are one of my favorite topics. We are starting today with a powerful mental model: iteration."
  },
  {
    "objectID": "04-functional-programming.html#iteration",
    "href": "04-functional-programming.html#iteration",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.2 Iteration",
    "text": "4.2 Iteration\nIteration is the basic idea of doing one thing multiple times. This is an area where computers shine, so in this chapter we will learn to fully utilize the power at our fingertips.\nAs an example, we will be reading in multiple similar files. Remember the gapminder dataset? Well, we are working with it again, but this time, our collaborator sent us one csv-file for each continent. You can find them in the data/04/ folder.\nWe already know how to read in one csv-file:\n\nread_csv(\"./data/04/Africa.csv\")\n\n\n4.2.1 The Imperative Programming Approach\nThe first solution to our problem is not my favorite one, but I want to show it anyway for the sake of completeness. In general, in Functional Programming, we tell the computer what we want, while in Imperative Programming, we tell the computer what steps to do. So here, we tell R what steps to perform. First, we get a vector of file-path’s with the fs package, which stands for file system. dir_ls means directory list, so we get the contents of a directory. We then create a list to store the dataframes that we are going to read in. We already define the length of the list because making a data structure longer is not R’s strong suit when it doesn’t know how much space to reserve for it. We then iterate over the numbers from 1 to the length of our paths. At each iteration we get the is path, read it and store it in our results list at position i. Finally we bind the list into one dataframe:\n\npaths &lt;- fs::dir_ls(\"./data/04/\")\n\nresult &lt;- vector(mode = \"list\", length = length(paths))\nfor (i in 1:length(paths)) {\n  result[[i]] &lt;- read_csv(paths[i])\n}\n\nbind_rows(result)\n\n# A tibble: 1,704 × 5\n   country  year lifeExp      pop gdpPercap\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Algeria  1952    43.1  9279525     2449.\n 2 Algeria  1957    45.7 10270856     3014.\n 3 Algeria  1962    48.3 11000948     2551.\n 4 Algeria  1967    51.4 12760499     3247.\n 5 Algeria  1972    54.5 14760787     4183.\n 6 Algeria  1977    58.0 17152804     4910.\n 7 Algeria  1982    61.4 20033753     5745.\n 8 Algeria  1987    65.8 23254956     5681.\n 9 Algeria  1992    67.7 26298373     5023.\n10 Algeria  1997    69.2 29072015     4797.\n# ℹ 1,694 more rows\n\n\nIn doing this we have lost the information about the Continent, which was in the file name, but before dwelling on this for too long, let’s leave the more convoluted and manual way behind and explore a, I dare say, more elegant approach.\n\n4.2.2 The Functional Programming Approach\n\n“Of course someone has to write for-loops. It doesn’t have to be you.” — Jenny Bryan\n\n \nWe have a function (read_csv) that takes a file path and returns (spits out) the data. In the Functional Programming style, the next idea is to now have a function, that takes two things: vector (atomic or list) and a function. And it feeds the individual elements of the vector to the function, one after another. In mathematics, the relation between a set of inputs and a set of outputs is called a map, which is where the name of the following family of functions comes from. In the tidyverse, these functional programming concepts live in the purrr package.\nIterating Explicitly with maps:\nFirst, we create the vector of things that we want to iterate over, the things that will be fed into our function one after the other:\n\npaths &lt;- fs::dir_ls(\"./data/04/\")\n\nThen we map the read_csv function over our vector and bind the resulting list of dataframes into one dataframe:\n\nresult &lt;- map(paths, read_csv)\nbind_rows(result)\n\nThe operation of mapping over a vector and combining the resulting list into one dataframe is actually so common that there is a variant of map that does this step automatically:\n\nmap_df(paths, read_csv, .id = \"continent\")\n\nThis distills everything our initial for-loop did into just one line of code. Using the .id argument we can save the name of the file path to a column in our dataset. This allows us to extract the continent from it:\n\ngapminder &lt;- map_df(paths, read_csv, .id = \"continent\") %&gt;% \n  mutate(continent = str_extract(continent, \"(?&lt;=/)\\\\w+(?=\\\\.csv)\"))\n\nhead(gapminder)\n\n# A tibble: 6 × 6\n  continent country  year lifeExp      pop gdpPercap\n  &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Africa    Algeria  1952    43.1  9279525     2449.\n2 Africa    Algeria  1957    45.7 10270856     3014.\n3 Africa    Algeria  1962    48.3 11000948     2551.\n4 Africa    Algeria  1967    51.4 12760499     3247.\n5 Africa    Algeria  1972    54.5 14760787     4183.\n6 Africa    Algeria  1977    58.0 17152804     4910.\n\n\nMy way of extracting the continent from the file path seems magical at first, and I still refer to the cheat sheet of the stringr package a lot when I am having to deal with text:\n \nIterating implicitly with vectorized functions:\nWe had our first encounter with iteration in a very implicit form. When we use R’s basic math operators, the computer is iterating behind the scenes. Take this expression:\n\n1:3 + 1:3\n\n[1] 2 4 6\n\n\nThis operation is vectorized. Without us having to tell R to do so, R will add the first element of the first vector to the first element of the second vector and so forth.\nNotice, how it looks like the operation happens all at the same time. But in reality, this is not what happens. The computer is just really fast at adding numbers, one after the other.\nThe mathematical operations in R call another programming language that does the actual addition. This other programming language is closer to the way computers think, making it less fun to write for us humans, but also faster because the instructions are easier to translate into actions for our computer processor.\nRemember, we only have to build our own iteration (e.g. with a map function), when we find a task that we want to apply to multiple things, and that is not already vectorized. And as it turns out, the fs and readr packages play very well together, because readr can also just take a vector of file paths and does the combining automatically!\n\nread_csv(paths, id = \"continent\")\n\n# A tibble: 1,704 × 6\n   continent            country  year lifeExp      pop gdpPercap\n   &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 ./data/04/Africa.csv Algeria  1952    43.1  9279525     2449.\n 2 ./data/04/Africa.csv Algeria  1957    45.7 10270856     3014.\n 3 ./data/04/Africa.csv Algeria  1962    48.3 11000948     2551.\n 4 ./data/04/Africa.csv Algeria  1967    51.4 12760499     3247.\n 5 ./data/04/Africa.csv Algeria  1972    54.5 14760787     4183.\n 6 ./data/04/Africa.csv Algeria  1977    58.0 17152804     4910.\n 7 ./data/04/Africa.csv Algeria  1982    61.4 20033753     5745.\n 8 ./data/04/Africa.csv Algeria  1987    65.8 23254956     5681.\n 9 ./data/04/Africa.csv Algeria  1992    67.7 26298373     5023.\n10 ./data/04/Africa.csv Algeria  1997    69.2 29072015     4797.\n# ℹ 1,694 more rows\n\n\nWhenever you encounter a new problem, ask yourself these questions:\n\nIs there a function that already does that?\nIs it already vectorized?\nIf not, is there a function that solves my problem for one instance?\nCan I map it over many things?\n\nThe purrr package contains various variants of the map function.\n\n\nmap itself will always return a list.\n\nmap_chr always returns an atomic character (=text) vector.\n\nmap_dbl always returns numbers (dbl = double precision).\n\nmap_lgl always returns logical (yes or no, TRUE / FALSE) vectors.\n\nmap_dfr always returns a dataframe.\n\n\n# map_\n\nThe for-loop-version had a lot more code, especially boilerplate, code that is just there to make the construct work and doesn’t convey our intentions with the code. Furthermore, the loop focuses the object that is iterated over (the file paths), while the map-version focuses on what is happening (the function, read_csv). But the loop still works. If you can’t think of a way to solve a problem with a map function, it is absolutely OK to use for-loops."
  },
  {
    "objectID": "04-functional-programming.html#if-you-copy-and-paste-the-same-code-more-than-three-times-write-a-function.",
    "href": "04-functional-programming.html#if-you-copy-and-paste-the-same-code-more-than-three-times-write-a-function.",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.3 “If you copy and paste the same code more than three times, write a function.”",
    "text": "4.3 “If you copy and paste the same code more than three times, write a function.”\nWriting our own functions can be very helpful for making our code more readable. It allows us to separate certain steps of your analysis from the rest, look at them in isolation to test and validate them, and also allows us to give them reasonable names.\nIt also allows us to re-use function across projects! Let’s imagine we have some experiment where the read-out of the machine is always in a certain format and needs some cleaning up. If we turn this into a function, e.g. like this:\n\nread_experiment_data &lt;- function(day) {\n  day &lt;- str_pad(day, pad = 0, width = 2)\n  paths &lt;- fs::dir_ls(paste0(\"./data/\",day,\"/\"))\n\n  gapminder &lt;- map_df(paths, read_csv, .id = \"continent\") %&gt;% \n    mutate(continent = str_extract(continent, \"(?&lt;=/)\\\\w+(?=\\\\.csv)\"))\n\n  gapminder\n}\n\nWe can put this function in a file (in my case R/my_functions) and source it, for example in multiple analysis Rmarkdown documents. The source function is nothing special, it just runs the R code in a file. And when we define functions in this file, those functions are then available to us:\n\nsource(\"R/my_functions.R\")\nread_experiment_data(4)\n\nNote: This example function only make sense to use with 4 as the input, as the data for the other days is of course different, but I hope you get the gist.\nI like to store my regular R files (as opposed to Rmd files) in a folder of my project called R. This makes it already look like an R package, in case I decide later on that the functions could be helpful for others as well or I want to share them more easily with colleagues. You can read more about creating your own R packages here (Wickham 2015)."
  },
  {
    "objectID": "04-functional-programming.html#implicit-iteration-with-dplyr-build-many-models",
    "href": "04-functional-programming.html#implicit-iteration-with-dplyr-build-many-models",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.4 Implicit iteration with dplyr: build many models",
    "text": "4.4 Implicit iteration with dplyr: build many models\ndplyrs idea of grouping allows us to express many ideas that are implicitly also iterations.\nLet us start by looking at just one country at first:\n\nalgeria &lt;- gapminder %&gt;% \n  filter(country == \"Algeria\")\n\nWe can plot the life expectancy over time with ggplot and add a linear model to our plot with geom_smooth using method = \"lm\".\n\nalgeria %&gt;% \n  ggplot(aes(year, lifeExp)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n\n\n\n\nHowever, while geom_smooth allows us to easily add smoothing lines or linear trends to our plot, it does not give us any information about the actual model. In order to do that we need to fit it ourselves with the lm function:\n\nmodel &lt;- lm(lifeExp ~ year, data = algeria)\n\nThe ~ symbol defines a formula, you can read it as: “lifeExp depending on year”. The data argument tells R where to look for the variables lifeExp and year, namely in our algeria tibble.\n\nsummary(model)\n\n\nCall:\nlm(formula = lifeExp ~ year, data = algeria)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3844 -0.5935 -0.2703  0.5339  2.4992 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.068e+03  4.380e+01  -24.38 3.07e-10 ***\nyear         5.693e-01  2.213e-02   25.73 1.81e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.323 on 10 degrees of freedom\nMultiple R-squared:  0.9851,    Adjusted R-squared:  0.9836 \nF-statistic: 661.9 on 1 and 10 DF,  p-value: 1.808e-10\n\n\nThat is a lot of information about our model! The broom package provides some functions for a cleaner and more specialized output:\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -1068.      43.8        -24.4 3.07e-10\n2 year            0.569    0.0221      25.7 1.81e-10\n\n\nEvery 1 year the life expectancy in Algeria went up by about half a year. Of course, this is only valid in the limited linear regime of our datapoints. We can’t extrapolate this indefinitely. After all, the intercept tells us that there would be a negative life expectancy at year 0.\nBut given the data that we have, how good does a line fit here?\n\nbroom::glance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.985         0.984  1.32      662. 1.81e-10     1  -19.3  44.6  46.0\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nR2 takes on values between 0 and 1, with 1 being a perfectly straight line connecting all the points and 0 for the points being all over the place. So 0.985 is a pretty good fit!\nUsing the dollar syntax, we get pull one column from a tibble, so we can write a function that, given a model, returns the R2\n\nget_r_squared &lt;- function(model) {\n  broom::glance(model)$r.squared\n}\n\nAnd now come the dplyr magic! If we group by country (and continent for good measuere just so that we don’t loose that column when summarizing), we can calculate a linear model for every country!\nNote two things: Firstly, we don’t use the data argument of lm here because the tidyverse functions already know where to look for lifeExp and year and they do so respecting the groups. So within each group (i.e. for each country), lifeExp will only contain the life expectancy for that country, not the life expectancy for all countries. Secondly, we wrap this part in list because we have to create a list column here (models don’t fit into an atomic vector). Otherwise, dplyr will complain.\nIn the second step we calculate the rsqured value for each model by mapping the function we created above over all models. We use the _dbl variant here because we want the values as an atomic vector of numbers, not as a list.\n\nall_models &lt;- gapminder %&gt;% \n  group_by(country, continent) %&gt;% \n  summarise(\n    model = list(lm(lifeExp ~ year)),\n    rsquared = map_dbl(model, get_r_squared)\n  )\n\nhead(all_models)\n\n# A tibble: 6 × 4\n# Groups:   country [6]\n  country     continent model  rsquared\n  &lt;chr&gt;       &lt;chr&gt;     &lt;list&gt;    &lt;dbl&gt;\n1 Afghanistan Asia      &lt;lm&gt;      0.948\n2 Albania     Europe    &lt;lm&gt;      0.911\n3 Algeria     Africa    &lt;lm&gt;      0.985\n4 Angola      Africa    &lt;lm&gt;      0.888\n5 Argentina   Americas  &lt;lm&gt;      0.996\n6 Australia   Oceania   &lt;lm&gt;      0.980\n\n\nFinally, we can add our calculated R2 values as a column to the original gapminder dataset so that we can use it in the following visualization.\n\ngapminder &lt;- gapminder %&gt;% \n  left_join(select(all_models, -model))\n\nHere, we highlight the irregularities (less linear countries) by making the transparency (= alpha value) depend on the R2 value.\n\ngapminder %&gt;% \n  ggplot(aes(year, lifeExp, color = country, alpha = 1/rsquared)) +\n  geom_line() +\n  guides(color = \"none\", alpha = \"none\") +\n  scale_color_manual(values = country_colors) +\n  facet_wrap(~continent, scales = \"free\") +\n  theme_minimal()\n\n\n\n\nWe can highlight one continent by filtering and use the ggrepel package to allow for more flexible labels. Furthermore check out the source document of this lecture and the video of the lecture to find out how you can add a figure caption and control the width and height of the plot via knitr chunk options. I also showcase a new way of writing chunk options in the latest version of the knitr package that is especially suited for longer captions.\n\ngapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(year, lifeExp, color = country, group = country)) +\n  geom_line(color = \"black\", alpha = 0.3) +\n  geom_line(data = filter(gapminder, rsquared &lt;= 0.4), size = 1.1) +\n  ggrepel::geom_text_repel(aes(label = country),\n                           data = filter(gapminder, rsquared &lt;= 0.4,\n                                         year == max(year)),\n                           nudge_x = 20,\n                           direction = \"y\"\n                           ) +\n  guides(color = \"none\", alpha = \"none\") +\n  scale_color_manual(values = country_colors) +\n  facet_wrap(~continent, scales = \"free\") +\n  labs(x = \"Year\", y = \"Life Expectancy at Birth\") +\n  theme_minimal() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.2)))\n\n\n\nFigure caption\n\n\n\nThe downward slope of our highlighted countries starting in the 1990s is a result of the ravaging AIDS pandemic. The prominent dips in two of the curves, orange for Rwanda and Cambodia in gray, are the direct consequences of genocides. These dire realities can in no way be summarized in just a couple of colorful lines. I am also in no way qualified to lecture on these topics. A good friend of mine, Timothy Williams, however is a researcher and teacher in the field of conflict and violence with a focus on genocides. He did field work in Cambodia and Rwanda and his book “The Complexity of Evil. Perpetration and Genocide” was published here on December 18 2020 (Williams, n.d.)."
  },
  {
    "objectID": "04-functional-programming.html#exercises",
    "href": "04-functional-programming.html#exercises",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.5 Exercises",
    "text": "4.5 Exercises\nI want to get you playing around with data, so keep in mind that the solutions for this exercise are not set in stone. There is often more than one viable way of graphing the same dataset and we will use the Office Hour to talk about the advantages and disadvantages of approaches that you came up with.\n\n4.5.1 Roman emperors\nThe first exercise uses a dataset about roman emperors from the tidytuesday project (link). You can import it with:\n\nemperors &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv\")\n\nThere is a slight error in the data because some of the dates are actually in BC time. In order to fix this we will be using the lubridate package, which is installed with the tidyverse, but not automatically loaded. For your convenience here is a function that you can use to fix the dataset:\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nfix_emperors &lt;- function(data) {\n  data %&gt;% \n    mutate(\n      birth = case_when(\n        index %in% c(1, 2, 4, 6) ~ update(birth, year = -year(birth)),\n        TRUE                     ~ birth\n      ),\n      reign_start = case_when(\n        index == 1 ~ update(reign_start, year = -year(reign_start)),\n        TRUE       ~ reign_start\n      )\n    )\n}\n\nHere are the questions to answer. Decide for yourself, if a particular question is best answered using a visualization, a table, a simple sentence or a combination of the three.\n\nWhat was the most popular way to rise to power?\nI what are the most common causes of death among roman emperors? What (or who) killed them?\nWhich dynasty was the most successful?\n\nFirstly, how often did each dynasty reign?\nSecondly, how long where the reigns?\nWhich dynasty would you rather be a part of, if your goal is to live the longest?\n\n\n\n4.5.2 Dairy Products in the US\nAnother dataset (link) concerns dairy product consumption per person in the US across a number of years. Load it with:\n\ndairy &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv\")\n\n\nAll masses are given in lbs (pounds), can you convert them to kg?\nWhich products lost their customer base over time, which ones won? Which products have the greatest absolute change in production when estimated with a straight line?\n\nAbove all, have some fun! If you make interesting findings along the way, go ahead and produce plots to highlight it."
  },
  {
    "objectID": "04-functional-programming.html#resources",
    "href": "04-functional-programming.html#resources",
    "title": "\n4  Functional Programming\n",
    "section": "\n4.6 Resources",
    "text": "4.6 Resources\n\npurrr documentation\nstringr documentation\ndplyr documentation\n\n\n\n\n\nWickham, Hadley. 2015. R Packages: Organize, Test, Document, and Share Your Code. 1 edition. Sebastopol, CA: O’Reilly Media.\n\n\nWilliams, Timothy. n.d. “The Complexity of Evil.” Rutgers University Press."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#motivation",
    "href": "05-probability-and-hypothesis-testing.html#motivation",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.1 Motivation",
    "text": "5.1 Motivation\n\nlibrary(tidyverse)\n\nIn the first four lectures we covered the fundamentals of handling data with R. Now, we will shift our focus away from the how and towards the why of data analysis. We will talk about different statistical tests, common mistakes, how to avoid them and how to spot them in other research. But of course, we will do so using R. So you will still learn one or the other useful function or technique along the way. In most instances it should be clear when I use R solely to demonstrate an idea from statistics and the code is just included for the curious, or whether the code is something you will likely also use for your own analysis. I am open for questions if things are unclear in any of the two cases. For purely aesthetic code I might also speed up the typing in the edit.\nFor the longer text parts it might be helpful to look at the script while watching the video or pause frequently to take your own notes (Rmarkdown is great for your lecture notes as well!)."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#statistically-significant",
    "href": "05-probability-and-hypothesis-testing.html#statistically-significant",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.2 Statistically Significant…",
    "text": "5.2 Statistically Significant…\n\n…you keep using that word. I don’t think it means what you think it means.\n\n\n\nstatistically significant\n\nYou will hear the phrases “statistically significant”, “significant” or even “very significant” thrown around quite a bit in academic literature . And while they are often used carelessly, they have a clearly defined meaning. A meaning we will uncover today. This meaning is related to the concept of so called p-values, which have an equally bad reputation for frequently being misused. The p in p-value stands for probability, so in order to understand p-values, we need to understand probability and learn how to deal with randomness, chance, or luck if you will. So…"
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#getting-our-hands-dirty-with-probability",
    "href": "05-probability-and-hypothesis-testing.html#getting-our-hands-dirty-with-probability",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.3 Getting our Hands dirty with Probability",
    "text": "5.3 Getting our Hands dirty with Probability\n\nTo understand statistics means understanding the nature of randomness first.\n\n\n\n\n\nA ggplot chessboard\n\n\n\nSay you and you friend are playing a game of chess, when your friend proudly proclaims:\n“I am definitely the better player!”.\n“Proof it!”, you reply.\n“That’s easy”, she says: “I won 7 out of the 8 rounds we played to today.”\n“Pah! That’s just luck.” is your less witty and slightly stubborn response.\nAs expected, we shall be using R to resolve this vital conflict.\n\n\nR rainbow\n\n\n5.3.1 Definitions: Hypothesis\nBoth of you involuntarily uttered an hypothesis, a testable assumption. And we want to test these hypothesis using statistics. The first hypothesis (“I am the better player.”) is what we call the alternative hypothesis (\\(H_1\\)). The name can be a bit confusing, because most often, this is your actual scientific hypothesis, the thing you are interested in. So, alternative to what? It is alternative to the so called null hypothesis (\\(H_0\\)), which is the second statement (“This is just luck”). The null hypothesis provides a sort of baseline for all our findings. It usually goes along the lines of “What if our observations are just based on chance alone?”, where “chance” can be any source of random variation in our system.\nThe tricky part is that there is no way to directly test the alternative Hypothesis, all we can test is the null hypothesis. Because for any null hypothesis we discard, there are always multiple alternative hypothesis that could explain our data. In our example, even if we end up discarding the idea of our friend’s chess success being only down to luck, this does not prove the alternative hypothesis that she is the better player (she could still be cheating for example). Do keep this in mind when we transfer this to a more scientific setting. Just because we show that something is unlikely to have arisen by chance does not mean that your favorite alternative hypothesis is automatically true.\nSo, after these words of warning, let’s test some null hypothesis!\n\n5.3.2 Testing the Null Hypothesis with a Simulation\nWe will start off by building a little simulation. Before testing any hypothesis, it is important to have defined \\(H_0\\) and \\(H_1\\) properly, which we did in the previous section. But we need to be a little more specific. Winning by chance would entail a completely random process, which we can model with a coin flip. R has the lovely function sample to take any number of things from a vector, with or without replacement after taking each thing:\n\ncoin &lt;- c(\"heads\", \"tails\")\nsample(coin)\n\n[1] \"tails\" \"heads\"\n\n\nNot giving it a number of things to draw just shuffles the vector, which is fairly boring in the case of just two tings. We can’t sample 10 things from a vector of only two elements\n\nsample(coin, size = 10)\n\nError in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\n\n\nBut we can, if we put the thing back every time:\n\nsample(coin, 10, replace = TRUE)\n\n [1] \"tails\" \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\"\n[10] \"heads\"\n\n\nSo, let’s make this a little more specific to our question:\n\nwinner &lt;- c(\"you\", \"friend\")\nrandom_winners &lt;- sample(winner, size = 8, replace = TRUE)\nrandom_winners\n\n[1] \"friend\" \"friend\" \"you\"    \"you\"    \"friend\" \"you\"    \"friend\" \"friend\"\n\nrandom_winners == \"friend\"\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n\n1 + TRUE\n\n[1] 2\n\n\n\n1 + FALSE\n\n[1] 1\n\n\n\nsum(random_winners == \"friend\")\n\n[1] 5\n\n\n\nmean(random_winners == \"friend\")\n\n[1] 0.625\n\n\nIf we were to run this script a million times, the resulting proportion of random wins for both of you would be very, very close to 50-50 because we used a fair coin. However, we don’t have the time to play this much Chess and we sure don’t have the money to run a million replicates for each experiment in the lab. But here, in our little simulated world, we have near infinite resources (our simulation is not to computationally costly).\n\nOne trick used above: When we calculate e.g. a sum or mean, R automatically converts TRUE to 1 and FALSE to 0.\n\nLet’s create a function that returns a random number of wins your friend would have gotten by pure chance for a number of rounds N.\n\nget_n_win &lt;- function(N) {\n  winner &lt;- c(\"you\", \"friend\")\n  random_winners &lt;- sample(winner, size = N, replace = TRUE)\n  sum(random_winners == \"friend\")\n}\n\nget_n_win(8)\n\n[1] 3\n\n\nThis number is different every time, so how does it change?\n\nresult &lt;- map_dbl(rep(8, 1000), get_n_win)\nhead(result)\n\n[1] 4 5 5 2 3 5\n\n\nA histogram is a type of plot that shows how often each value occurs in a vector. Usually, the values are put into bins first, grouping close values together for continuous values, but in this case it makes sense to just have one value per bin because we are dealing with discrete values (e.g. no half-wins). Histograms can either display the raw counts or the frequency e.g. as a percentage. In ggplot, we use geom_bar when we don’t need any binning, just counting occurrences, and geom_histogram when we need to bin continuous values.\n\ntibble(result) %&gt;% \n  ggplot(aes(x = result)) +\n  geom_bar() +\n  labs(x = \"N wins for friend\",\n       title = \"Throwing a coin 8 times\") +\n  scale_x_continuous(breaks = 0:8)\n\n\n\n\nAs expected, the most common number of wins out of 8 is 4 (unless I got really unlucky when compiling this script). Let us see, how this distribution changes for different values of N. First, we set up a grid of numbers (all possible combinations) so that we can run a bunch of simulations:\n\nsimulation &lt;- crossing(\n   N = 1:15,\n   rep = 1:1000\n) %&gt;% \n  mutate(\n    wins = map_dbl(N, get_n_win)\n  )\n\nAnd then we use our trusty ggplot to visualize all the distributions.\n\nsimulation %&gt;% \n  ggplot(aes(wins)) +\n  geom_bar() +\n  facet_wrap(~N, labeller = label_both) +\n  labs(title = \"Flipping a coin N times\")\n\n\n\n\nWith a fair coin, the most common number of wins should be half of the number of coin flips. Note, how it is still possible to flip a coin 15 times and and not win a single time. It is just very unlikely and the bars are so small that we can’t see them.\nLet us go back to the original debate. The first statement: “I am better.” is something that can never be definitively proven. Because there is always the possibility, no matter how small, that the same result could have arisen by pure chance alone. Even if she wins 100 times and we don’t take a single game from her, this sort of outcome is still not impossible to appear just by flipping a coin. But what we can do, is calculate, how likely a certain event is under the assumption of the null hypothesis (only chance). And we can also decide on some threshold \\(\\alpha\\) at which we reject the null hypothesis. This is called the significance threshold. When we make an observation and then calculate that the probability for an observation like this or more extreme is smaller than the threshold, we deem the result statistically significant. And the probability thus created is called the p-value.\nFrom our simulation, we find the that probability to win 7 out of 8 rounds under the null hypothesis is:\n\nsimulation %&gt;% \n  filter(N == 8) %&gt;% \n  summarise(\n    mean(wins &gt;= 7)\n  )\n\n# A tibble: 1 × 1\n  `mean(wins &gt;= 7)`\n              &lt;dbl&gt;\n1             0.038\n\n\nWhich is smaller than the commonly used significance threshold of \\(\\alpha=0.05\\) (i.e. \\(5\\%\\)). So with 7 out of 8 wins, we would reject the null hypothesis. Do note that this threshold, no matter how commonly and thoughtlessly it is used throughout academic research, is completely arbitrary.\n\n5.3.3 Getting precise with the Binomial Distribution\nNow, this was just from a simulation with 1000 trials, so the number can’t be arbitrarily precise, but there is a mathematical formula for this probability. What we created by counting the number of successes in a series of yes-no-trials is a binomial distribution. For the most common distributions, R provides a set of functions. the functions starting with d give us the probability density function. In the case of discrete values like counting wins, this is equivalent to the actual probability, but for continuous values we obtain the probability by taking the integral. We get these integrals with the corresponding functions starting with p (for probability).\n\ndbinom(x = 7, size = 8, prob = 0.5) \n\n[1] 0.03125\n\n\nThis is the probability to win exactly 7 out of 8 games. But what we wanted was the probability for 7 or more out of 8! So we move to the integral. This part can get a bit confusing, because the default for pbinom is lower.tail = TRUE, which according to the help page means that probabilities it returns \\(P[X \\le x]\\).\n\npbinom(q = 7, size = 8, prob = 0.5)\n\n[1] 0.9960938\n\n\nIf we set lower.tail to FALSE , we get \\(P[X &gt; x]\\), so the probability for a random variable X being bigger than a number x. So to get the probability that we are interested in, we need to replace the 7 with a 6 as well:\n\npbinom(q = 6, size = 8, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.03515625\n\n\nOur simulation was pretty close! So the exact values agrees and we reject the null hypothesis of both opponents being equally good. Here is the full graph for the probability density function of the binomial distribution.\n\nggplot() +\n  stat_function(fun = function(x) dbinom(x = x, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\n\n\n\n\nAnd the integral, the probability \\(P[X \\le x]\\).\n\nggplot() +\n  stat_function(fun = function(q) pbinom(q = q, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 9, limits = c(0, 8))\n\n\n\n\nThere is two more functions I want to showcase from this family. The third is the so called quantile function. Quantiles divide a probability distribution into pieces of equal probability. One example for a quantile is the 50th percentile, also known as the median, which divides the values such that half of the values are above and half are below. And we can keep dividing the two halves as well, so that we end up with more quantiles. Eventually, we arrive at the quantile function. It is the inverse of the probability function, so you obtain it by swapping the axis.\n\nggplot() +\n  stat_function(fun = function(p) qbinom(p = p, size = 8, prob = 0.5),\n                geom = \"step\",\n                n = 9) +\n  scale_x_continuous(n.breaks = 10, limits = c(0, 1))\n\n\n\n\nQuantiles will also be useful for deciding if a random sample follows a certain distribution with quantile-quantile plots.\nLastly, there is always also an r variant of the function, which gives us any number of random numbers from the distribution.\n\nrbinom(10, 8, 0.5)\n\n [1] 5 5 6 4 4 2 5 3 5 3\n\n\n\n5.3.4 But how much better? Understanding Effect Size and Power, False Positives and False Negatives\nWe decided to abandon the null hypothesis that both players are equally good, which equates to a 50% win-chance for each player. But we have not determined how much better she is. And how much better does she need to be for us to reliably discard the null hypothesis after just 8 games? The generalization of the how much better part, the true difference, is called the effect size.\nOur ability to decide that something is statistically significant when there is in fact a true difference is called the statistical power. It depends on the effect size, our significance threshold \\(\\alpha\\) and the sample size \\(n\\) (the number of games). We can explore the concept with another simulation.\n\nreps &lt;- 10000\nsimulation &lt;- crossing(\n  N = c(8, 100, 1000, 10000),\n  true_prob = c(0.5, 0.8, 0.9)\n) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    wins = list(rbinom(n = reps, size = N,  prob = true_prob)),\n  ) %&gt;% \n  unnest(wins) %&gt;% \n  mutate(\n    p = pbinom(q = wins - 1, size = N, prob = 0.5, lower.tail = FALSE)\n  )\n\nhead(simulation)\n\n# A tibble: 6 × 4\n      N true_prob  wins     p\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1     8       0.5     3 0.855\n2     8       0.5     3 0.855\n3     8       0.5     4 0.637\n4     8       0.5     1 0.996\n5     8       0.5     3 0.855\n6     8       0.5     6 0.145\n\n\nI also introduced a new piece of advanced dplyr syntax. rowwise is similar to group_by and essentially puts each row into its own group. This can be useful when working with list columns or running a function with varying arguments and allows us to treat the inside of mutate a bit like as if we where using one of the map functions. For more information, see the documentation article.\nIt leaves us with 10000 simulated numbers of wins at N games for different true probabilities of her winning (i.e. how much better our friend is). We then calculate the probability to have this or a greater number of wins under the null hypothesis (equal probability for win and loss), in other words: the p-value.\n\nsimulation %&gt;%\n  ggplot(aes(p)) +\n  geom_histogram() +\n  facet_wrap(~ true_prob + N,\n             labeller = label_both,\n             scales = \"free_y\",\n             ncol = 4) +\n  geom_vline(xintercept = 0.05, color = \"red\") +\n  labs(x = \"p-value\",\n       y = \"frequency\") +\n  scale_y_continuous(breaks = NULL)\n\n\n\n\nWe notice a couple of things in this plot. As the number of games played approaches very high numbers, the p-values for the case where the null hypothesis is in fact true (both players have the same chance of winning), start following a uniform distribution, meaning for a true null hypothesis, all p-values are equally likely. This seems counterintuitive at first, but is a direct consequence of the definition of the p-value. The consequence of this is, that if we apply our regular significance threshold of 5%, by definition we will say that there is a true difference, even though there is none (i.e. the null hypothesis is true but we falsely reject it and favor of our alternative hypothesis). This is called a false positive. By definition, we will get at least \\(\\alpha\\) false positives in all of our experiments. Later, we will learn, why the real number of false positives is even higher. Another name for false positives is Type I errors.\nOn the other side of the coin, there are also cases where there is a true difference (we used winning probabilities of 0.8 and 0.9), but we don’t reject the null hypothesis because we get a p-values larger than \\(alpha\\). These are all false negatives and their rate is sometimes referred to as \\(\\beta\\). Another name for false negatives is Type II errors. People don’t particularly like talking about negative things like errors, so instead you will often see the inverse of \\(\\beta\\), the Statistical Power \\(1-\\beta\\). The proportion of correctly identified positives out of the actual positives is also shown on the plot below. For example, say her true win probability is 90% and we play 8 games. If this experiment runs in an infinite number of parallel universes, we will conclude that she is better than chance in 80% of those. We could set our significance threshold higher to detect more of the true positives, but this would also increase our false positives.\n\nsimulation %&gt;%\n  group_by(true_prob, N) %&gt;%\n  summarise(signif = mean(p &lt;= 0.05)) %&gt;% \n  ggplot(aes(true_prob, signif, fill = true_prob == 0.5)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(label = signif), vjust = -0.2) +\n  facet_wrap(~N,\n             labeller = label_both) +\n  scale_y_continuous(expand = expansion(c(0, 0.1))) +\n  scale_fill_viridis_d() +\n  labs(y = \"Proportion of significant results\")\n\n\n\n\nThere are also packages out there, which have a function to compute the power for the binomial test, but I think the simulation was way more approachable. The cool thing about simulations is also, that they work even when there is no analytical solution, so you can use them to play around when planning an experiment."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#p-value-pitfalls",
    "href": "05-probability-and-hypothesis-testing.html#p-value-pitfalls",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.4 P-Value Pitfalls",
    "text": "5.4 P-Value Pitfalls\nLet us look into some of the pitfalls of p-values. Remember from the definition of p-values, that we will get a significant result even if there is no true difference in 5% of cases (assuming we use this as our alpha)? Well, what if we test a bunch of things? This is called Multiple Testing and there is a problem associated with it:\nIf you test 20 different things, and your statistical test will produce a significant result by chance alone in 5% of cases, the expected number of significant results is 1. So we are not very surprised. Speaking of surprised: In his book, available for free online, “Statistics done wrong”, Alex Reinhart describes p-values as a “measure of surprise”:\n\n»A p value is not a measure of how right you are, or how significant the difference is; it’s a measure of how surprised you should be if there is no actual difference between the groups, but you got data suggesting there is. A bigger difference, or one backed up by more data, suggests more surprise and a smaller p value.« — Alex Reinhart (Reinhart 2015)\n\nSo, we are not very surprised, but if you focus to hard on the one significant result, trouble ensues. In a “publish or perish” mentality, this can easily happen, and negative findings are not published nearly enough, so most published findings are likely exaggerated. John Bohannon showcased this beautifully by running a study on chocolate consumption and getting it published: I Fooled Millions Into Thinking Chocolate Helps Weight Loss. Here’s How.\nWhat can we do about this?\n\n5.4.1 Multiple Testing Correction\nThe simplest approach is to take all p-values calculate when running a large number of comparisons and dividing them by the number of tests performed. This is called the Bonferroni correction\n\np_values &lt;- c(0.5, 0.05, 0.3, 0.0001, 0.003)\np.adjust(p_values, method = \"bonferroni\")\n\n[1] 1.0000 0.2500 1.0000 0.0005 0.0150\n\n\nOf course, this looses some statistical power (remember, no free lunch). A slightly more sophisticated approach to controlling the false discovery rate (FDR) is the Benjamini-Hochberg procedure. It retains a bit more power. Here is what happens:\n\nSort all p-values in ascending order.\nChoose a FDR \\(q\\) you are willing to accept and call the number of tests done \\(m\\).\nFind the largest p-value with: \\(p \\leq iq/m\\) with its index \\(i\\).\nThis is your new threshold for significance\nScale the p-values accordingly\n\nAnd this is how you do it in R:\n\np.adjust(p_values, method = \"fdr\")\n\n[1] 0.50000000 0.08333333 0.37500000 0.00050000 0.00750000\n\n\n\n5.4.2 Other forms of p-hacking\nThis sort of multiple testing is fairly obvious. You will notice it, when you end up with a large number of p-values, for example when doing a genetic screening and testing thousands of genes. Other related problems are harder to spot. For a single research question there are often different statistical tests that you could run, but trying them all out and then choosing the one that best agrees with your hypothesis is not an option! Likewise, simply looking at your data is a form of comparison if it influences your choice of statistical test. Ideally, you first run some exploratory experiments that are not meant to test your hypothesis, then decide on the tests you need, the sample size you want for a particular power and then run the actual experiments designed to test your hypothesis.\nAt this point, here is another shout-out to Alex Reinharts book (Reinhart 2015). It is a very pleasant read and also shines more light on some of the other forms of p-hacking."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#bayesian-statistics-and-the-base-rate-fallacy",
    "href": "05-probability-and-hypothesis-testing.html#bayesian-statistics-and-the-base-rate-fallacy",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.5 Bayesian Statistics and the Base Rate Fallacy",
    "text": "5.5 Bayesian Statistics and the Base Rate Fallacy\nThere is another more subtle problem called the base rate fallacy. As an example, we assume a medical test, testing for a certain condition. In medical testing, different words are used for the same concepts we defined above1.1 This is a slightly annoying trend in statistics; as it enters different fields, people come up with new names for old things (perhaps the most notorious field for this is machine learning).\nHere, we have:\n\nSensitivity = Power = true positive rate = \\(1-\\beta\\)\n\nSpecificity = true negative rate = \\(1-\\alpha\\)\n\n\nLet us assume a test with a sensitivity of 90% and a specificity of 92%. When we visit the doctor to get a test, and get a positive result, what is the probability, that we are in fact positive (i.e. a true positive)? Well, the test has a specificity of 92%, so if we where negative, it would have detected that in 92% of cases, does this mean, that we can be 92% certain, that we are actually positive?\nWell, no. What we are ignoring here is base rate, which for diseases is called the prevalence. It is the proportion at which a disease exists in the general population.\nSo, let us say, we are picking 1000 people at random from the population and testing them. We are dealing with a hypothetical condition that affects 1% of people, so we assume 10 people in our sample to be positive. Of those 10 people, 9 will be tested positive (due to our sensitivity), those will be our true positives. The remaining 1 will be a false negative. However, we are of course also testing the negatives (if we knew ahead of time there would be no point in testing) and of those due to our specificity, 8% will also be tested positive, which is 0.08 * 990, so we get 79 false positives. Because there are so many negatives in our sample, even a relatively high specificity will produce a lot of false positives. So that actual probability of being positive with a positive test result is\n\\[\\frac{true~positives}{true~positives + false~positives}=10\\%\\]\n\n\n\n\n\n\nWarning\n\n\n\nTODO: fix the plot! (the waffle package is broken)\n\n\n\n\n\n\n\nFormally, this is described by Bayes’s Formula\n\\[P(A|B)=\\frac{P(B|A)*P(A)}{P(B)}\\]\nRead: The probability of A given B is the probability of B given A times the probability of A divided by the probability of B.\nIn bayesian statistics, the prevalences are known as priors."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#concepts-discussed-today",
    "href": "05-probability-and-hypothesis-testing.html#concepts-discussed-today",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.6 Concepts discussed today",
    "text": "5.6 Concepts discussed today\nAfter today you should be familiar with the following concepts:\n\nNull and alternative hypothesis\nP-values and statistical significance\nBinomial distribution\nProbability density, probability and quantile functions\nEffect size and statistical power\nFalse positives, false negatives\nMultiple testing and p-hacking\nBayes’s Theorem"
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#exercises",
    "href": "05-probability-and-hypothesis-testing.html#exercises",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.7 Exercises",
    "text": "5.7 Exercises"
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#a-fair-coin",
    "href": "05-probability-and-hypothesis-testing.html#a-fair-coin",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.8 A Fair Coin",
    "text": "5.8 A Fair Coin\nWe have a regular old coin and flip it 100 times. Given a significance threshold \\(\\alpha\\) of 0.05, with what probability do we (mistakenly) reject the null hypothesis i.e. conclude that the coin is not fair even though it is? Can you show this with a simulation? As a tip I can tell you that due to the vectorized nature of the functions involved you won’t need a map or loop. The shortest version I could think of uses only 3 functions."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#an-unfair-game",
    "href": "05-probability-and-hypothesis-testing.html#an-unfair-game",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.9 An Unfair Game",
    "text": "5.9 An Unfair Game\nWe are playing a game where you have to roll the most sixes in order to win. Someone is trying to fool us and is using a loaded die. The die is manipulated such that the chance of rolling a six is 35% instead of the usual 1/6. At the same significance threshold as above, what is the chance of us rejecting the null hypothesis (= a fair die) and thus concluding correctly that we are being tricked after rolling the die 20 times? You will have to run a simulation here again."
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#discovering-a-new-distribution",
    "href": "05-probability-and-hypothesis-testing.html#discovering-a-new-distribution",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.10 Discovering a new Distribution",
    "text": "5.10 Discovering a new Distribution\nThe binomial distribution was concerned with sampling with replacement (you can get head or tails any number of times without using up the coin). In this exercise you will explore sampling without replacement. The common model for this is an urn with two different colored balls in it. The resulting distribution is called the hypergeometric distribution and the corresponding R functions are &lt;r/d/p/q&gt;hyper\n\nImagine you are a zoo manager.\n\nWe got a gift from another zoo! It consists of 8 red pandas and 2 giant pandas. What is the probability that they end up properly separated, if we randomly take 8 animals, put them in one enclosure and put the rest in another?\nOur penguin colony hatched eggs and we have a bunch of newcomers. We have have 15 males and 10 females. If we look at a random subset of 12 penguins, what does the distribution of the number of males look like? Which number is most likely? How likely is it, to get at least 9 males in the sample?"
  },
  {
    "objectID": "05-probability-and-hypothesis-testing.html#resources",
    "href": "05-probability-and-hypothesis-testing.html#resources",
    "title": "\n5  Probability and Hypothesis Testing\n",
    "section": "\n5.11 Resources",
    "text": "5.11 Resources\n\nhttps://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108\nhttps://jimgruman.netlify.app/post/education-r/\nP-Value histograms blogpost by David Robinson\n“Statistics done wrong”\n\n\n\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete Guide. 1 edition. San Francisco: No Starch Press."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#some-preparation",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#some-preparation",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.1 Some Preparation",
    "text": "6.1 Some Preparation\nToday, we will explore the process of modeling and look at different types of models. In part, we will do so using the tidymodels framework. The tidymodels framework extends the tidyverse with specialized tools for all kinds of modeling tasks that fit neatly in with all the tools we already know. Go ahead and install them with:\n\ninstall.packages(\"tidymodels\")\n\nNow we are ready to get started\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n6.1.1 [Sidenote] on Reproducible Environments with renv\n\n \nAt this point, we have installed quite a lot of packages. On one hand, this is great fun because the extend what we can do and make tedious tasks fun. On the other hand, every package that we add introduces what is called a dependency. If a user doesn’t have the package installed, our analysis will not run. If we are feeling experimental and use functions from packages that are under active development and might change in the future, we will run into trouble when we update the package. But never updating anything ever again is no fun! I will show you, how to get the best of both worlds: All the packages and functions that your heart desires while maintaining complete reproducibility. This is to make sure that you can come back to your old projects 2 years from now and they still just run as they did at the time.\nThis solution is a package called renv. The idea is as follows: Instead of installing all your packages into one place, where you can only have one version of a package at a time, renv installs packages locally in your project folder. It also meticulously writes down the version numbers of all the packages you installed and keeps a cache, so it will not copy the same version twice.\nIt is an R package like any other, so first, we install it with:\n\ninstall.packages(\"renv\")\n\nThen, in our RStudio project in the R console, we initialize the project to use renv with:\n\nrenv::init()\n\nThis does a couple of things. It creates a file named .Rprofile, in which it writes source(\"renv/activate.R\"). The R-profile file is run automatically every time you start a R session in this folder, so it makes sure renv is active every time you open the project. It also creates a folder called renv. This is the where it will install packages you want to use in the project. The most important file is the renv.lock file. You can have a look at it, it is just a text file with all the packages and their exact versions.\nYou notice, that after initializing renv, we have no packages, so for example we can’t load the tidyverse as usual. We will have to install it again! However, in this case it should be fairly fast, because renv knows that it was already installed globally so it simply copies the files, which is fast. After having installed a new package, we call:\n\nrenv::snapshot()\n\nRenv tells us, what we changed in our environment and after we confirm, it notes down the changes.\nNot it is also really easy to collaborate with other people. Because after we send them our project folder, all they have to do is run:\n\nrenv::restore()\n\nTo install all packages noted down in the lockfile. We can also use this ourselves if we installed a few to many packages or did an update we regret and want to go back to what is written in the lockfile.\nFinally, renv also provides functions to update or install new packages. They work like install.packages, but a bit more versatile. For example, let me show you the different locations from which we can install packages.\n\nThe main location is CRAN (The Comprehensive R Archive Network). This is also from where you installed R itself. R packages on there are subject to certain standards and usually stable and tested.\nWe can also install packages directly from the source code other people uploaded. GitHub is a platform where you can upload code and track changes to it. A lot of times, you can find the current developement version of an R package, or packages that are not yet on CRAN on GitHub.\n\nrenv can install packages from GitHub as well, for example let us say, we want to test out the latest version of the purrr package to give feedback to the developers.\nhttps://github.com/tidyverse/purrr here it says:\n\n# ...\n# Or the the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"tidyverse/purrr\")\n\nWell, we don’t need devtools for this, because renv can do this with the regular install function:\n\nrenv::install(\"tidyverse/purrr\")\n\nGiving it just a package name installs a package from CRAN, a pattern of \"username/packgename\" installs from GitHub. Now, back to the actual topic of today!\nAfter having initialized renv we need to install the packages that we need for the project even if we already have them in our global package cache, just so that renv knows about them."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#all-models-are-wrong-but-some-are-useful",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#all-models-are-wrong-but-some-are-useful",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.2 All models are wrong, but some are useful",
    "text": "6.2 All models are wrong, but some are useful\n\n“All models are wrong, but some are useful” — George Box\n\nWhat this means is that any model is but a simplification of reality and must always omit details. No model can depict the complete underlying reality. However, models are useful, and to understand what they are useful for, we must first look at the different types of models out there.\n\n6.2.1 Types of Models\n \nThe tidymodels book names three types of models, where any particular model can fall into multiple categories at once:\n\n\nDescriptive Models  are purely used to describe the underlying data to make patters easier to see. When we add a smooth line to a ggplot with geom_smooth, the default method is a so called LOESS curve, which stands for Locally Estimated Scatterplot Smoothing. It does produce insights by revealing patterns to us, but by itself can not be used e.g. to make predictions. It is just a pretty looking smooth line.\n\n\n\n\n\n\n\nInferential Models  are designed to test hypothesis or make decisions. They rely heavily on our assumptions about the data (e.g. what probability distribution the populations follows) and will be most likely encountered by you to answer research questions. They are the models that typically produce a p-value, which you compare to a threshold like we did last week.\nPredictive Models  are designed to process the data we have and make predictions about some response variable upon receiving new data. When done correctly, we also hold out on some of the data that our model never gets to see, until it is time to evaluate and test how it performs on unseen data. Depending on how much we know (or want to know) about the underlying processes, we differentiate between mechanistic models like fitting a physically meaningful function to data and empirically driven models, which are mainly concerned with creating good predictions, no matter the underlying mechanism.\n\nWe will now explore different examples. First, let me introduce our dataset for today:"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#say-hello-to-spotify-data",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#say-hello-to-spotify-data",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.3 Say Hello to Spotify Data",
    "text": "6.3 Say Hello to Spotify Data\nI created a playlist on spotify, which is quite diverse so that we can look at a range of features. You can even listen to it here while you do the exercises if you want. I am doing so, as I write this. The cool thing about spotify is, that they have an API, an Application Interface. APIs are ways for computer programs to talk to each other. So while we use the spotify app to look up songs, computers use the API to talk to the spotify server. And because R has a rich ecosystem of packages, someone already wrote a package that allows R to talk to this API: spotifyr.\nIf you check out the R folder in this lecture, you can see how I downloaded and processed that data about the playlist. Note that the script will not work for you right away, because you first need to register with spotify as a developer and then get a so called token, like a username and password in one long text, to be allowed to send bots their way. You probably just want to download the data from my github repository.\nLet’s have a look, shall we?\n\nsongs &lt;- read_csv(\"data/06/spotify_playlist.csv\")\n\nWe can get a quick overview of all columns with:\n\nglimpse(songs)\n\nRows: 393\nColumns: 18\n$ track_name        &lt;chr&gt; \"Africa\", \"Take on Me\", \"Wake Me Up Before You Go-Go…\n$ track_artists     &lt;chr&gt; \"TOTO\", \"a-ha\", \"Wham!\", \"Elton John\", \"The HU;Jacob…\n$ danceability      &lt;dbl&gt; 0.671, 0.573, 0.620, 0.504, 0.373, 0.624, 0.789, 0.7…\n$ energy            &lt;dbl&gt; 0.373, 0.902, 0.573, 0.904, 0.895, 0.857, 0.789, 0.6…\n$ key               &lt;dbl&gt; 9, 6, 0, 6, 8, 10, 2, 4, 4, 6, 6, 4, 4, 11, 10, 10, …\n$ loudness          &lt;dbl&gt; -18.064, -7.638, -11.893, -6.863, -3.846, -6.250, -4…\n$ mode              &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1…\n$ speechiness       &lt;dbl&gt; 0.0323, 0.0540, 0.0423, 0.1790, 0.0610, 0.0542, 0.29…\n$ acousticness      &lt;dbl&gt; 0.257000, 0.018000, 0.271000, 0.356000, 0.022100, 0.…\n$ instrumentalness  &lt;dbl&gt; 8.01e-05, 1.25e-03, 0.00e+00, 1.21e-01, 2.45e-04, 2.…\n$ liveness          &lt;dbl&gt; 0.0481, 0.0928, 0.0607, 0.1400, 0.6610, 0.1100, 0.09…\n$ valence           &lt;dbl&gt; 0.7320, 0.8760, 0.8970, 0.7720, 0.6680, 0.3240, 0.37…\n$ tempo             &lt;dbl&gt; 92.718, 84.412, 81.548, 176.808, 172.392, 131.926, 1…\n$ time_signature    &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ track_duration_ms &lt;dbl&gt; 295893, 225280, 231333, 183440, 317077, 282920, 2480…\n$ track_popularity  &lt;dbl&gt; 82, 84, 79, 80, 57, 60, 74, 84, 84, 82, 82, 81, 81, …\n$ track_uri         &lt;chr&gt; \"spotify:track:2374M0fQpWi3dLnB54qaLX\", \"spotify:tra…\n$ track_year        &lt;dbl&gt; 1982, 1985, 1984, 1983, 2020, 2019, 2019, 2015, 2015…\n\n\nFinally some decent numbers! Not just these measly discrete values we had last week. For each song in the playlist, we get the artist, the year it arrived and a number of features like how danceable, how loud or fast the song is. You can easily imagine spotify using these numbers to suggest new songs based on the features of those that you have listened to. And in fact, we are going to lay the foundations for such an algorithm today."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#visualising-continuous-distributions",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#visualising-continuous-distributions",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.4 Visualising Continuous Distributions",
    "text": "6.4 Visualising Continuous Distributions\nWhen dealing with a continuous distribution, like we have for a lot of our features in the spotify songs dataset, there are always multiple ways to represent the same data. First, we just look at the numbers. We will use the valence values for our songs:\n\nhead(songs$valence)\n\n[1] 0.732 0.876 0.897 0.772 0.668 0.324\n\n\nNotice anything interesting in the numbers? I don’t either. Our brain is way better suited for looking at graphical representations, so: To the ggplot cave!\n\nsongs %&gt;% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_point()\n\n\n\n\nThis is kind of hard to see, because points overlap. We can get a better picture of the distribution by using transparency or a bit of jitter:\n\nsongs %&gt;% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_jitter(width = 0.1)\n\n\n\n\nUsing a histogram, we can put the points into bins and get a plot similar to what we got for discrete values. Note that the plot is flipped on it’s side now.\n\nsongs %&gt;% \n  ggplot(aes(valence)) +\n  geom_histogram()\n\n\n\n\nAnd we might want to play around with the bin size to get a better feel for the distribution. Another way is to apply a smoothing function and estimate the density of points along a continuous range, even in places where we originally had no points:\n\nsongs %&gt;% \n  ggplot(aes(valence)) +\n  geom_density(fill = \"midnightblue\", alpha = 0.6)\n\n\n\n\nBoth of these plots can be misleading, if the original number of points is quite small, and in most cases, we are better off, showing the actual individual points as well. This is the reason, why the first plots I did where vertical, because there is a cool way of showing both the points and the distribution, while still having space to show multiple distributions next to each other. Imagine taking the density plot, turning it 90 degrees and then mirroring through the middle. What we get is a so called violin plot. To overlay the points on top, we will use something a little more predictable than jitter this time: From the ggbeeswarm package I present: geom_quasirandom.\n\nsongs %&gt;% \n  ggplot(aes(x = \"\", y = valence)) +\n  geom_violin(fill = \"midnightblue\", alpha = 0.6) +\n  ggbeeswarm::geom_quasirandom(width = 0.35)\n\n\n\n\nThis is cool, because now we can easily compare two different distributions next to each other and still see all the individual points. For example, we might ask:\n\n“Do songs in major cord have a higher valence than songs in minor cord in our dataset?”\n\n\nsongs %&gt;% \n  filter(!is.na(mode), !is.na(valence)) %&gt;% \n  ggplot(aes(x = factor(mode), y = valence)) +\n  geom_violin(fill = \"midnightblue\", alpha = 0.6) +\n  ggbeeswarm::geom_quasirandom(width = 0.35) +\n  scale_x_discrete(labels = c(`0` = \"minor\", `1` = \"major\"))\n\n\n\n\n\nNote: This jittering only works, because the feature on the x-axis is discrete. If it where continuous, we would be changing the data by jittering on the x-axis.\n\nWe might also want to add summaries like the mean for each group to the plot with an additional marker. This leads us to the general concept of summary statistics. There is a number of them, and they can be quite useful to, well, summarise a complex distribution. But they can also be very misleading, as can any simplification be."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#summary-statistics",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#summary-statistics",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.5 Summary Statistics…",
    "text": "6.5 Summary Statistics…"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#mean-median-and-other-quartiles-range",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#mean-median-and-other-quartiles-range",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.6 Mean, Median (and other Quartiles), Range",
    "text": "6.6 Mean, Median (and other Quartiles), Range\nLet us start by considering different things we can say about our distribution in one number. First, we might look at the range of our numbers, the maximum and minimum. We will do this per mode, so we can compare the values. Next, we want to know the centers of the points. There are different notions of being at the center of the distribution. The mean or average is the sum of all values divided by the number of values. The median is what we call a quantile, a point that divides a distribution in equally sized parts, specifically such that 50% values are below and 50% are above the median.\n\nsongs %&gt;% \n  drop_na(valence, mode) %&gt;% \n  group_by(mode) %&gt;% \n  summarise(\n    min = min(valence),\n    max = max(valence),\n    mean = mean(valence),\n    median = median(valence)\n  )\n\n# A tibble: 2 × 5\n   mode    min   max  mean median\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     0 0.0271 0.966 0.448  0.447\n2     1 0.0258 0.965 0.475  0.477\n\n\nIt appears the valence can assume values between 0 and 1. A shortcut for this is the range function:\n\nrange(songs$valence)\n\n[1] NA NA\n\n\nThe median is just one of the many percentiles we can think of. If we display the 50th as well as the 25th and 75th percentile on one plot, we get what is called a boxplot:\n\n# in the lecture I used filter(!is.na(mode), !is.na(valence)),\n# but drop_na is more elegant.\n\nsongs %&gt;% \n  drop_na(valence, mode) %&gt;% \n  ggplot(aes(x = factor(mode), y = valence)) +\n  geom_boxplot(fill = \"midnightblue\", alpha = 0.6, outlier.color = NA) +\n  ggbeeswarm::geom_quasirandom(width = 0.35) +\n  scale_x_discrete(labels = c(`0` = \"minor\", `1` = \"major\"))\n\n\n\n\nThe “whiskers” of the box extend to 1.5 times the box size or to the last data point, whichever makes smaller whiskers. Points that are more extreme than the whiskers are labeled outliers by the boxplot and usually displayed as their own points. Like with the violin plot, we also have the option to plot the original un-summarized points on top. In this case, we need to make sure to change the outlier color for the boxplot to NA, because otherwise we are plotting them twice.\nThis hints at one downside of boxplots (when used without adding the raw datapoints as well): The box is a very prominent focus point of the plot, but by definition, it only contains 50% of all datapoints. The rest is delegated to thin whiskers.\n\n6.6.1 Variance\nFinally, we want to know, how far the values scatter around their means and the potential population mean. This is encompassed in two closely related measures: the variance and the standard deviation.\nFor illustrative purposes, we can plot all datapoints for e.g the valence in the order in which they appear in the data and add a line for the mean.\n\nsongs %&gt;%\n  filter(!is.na(valence)) %&gt;% \n  mutate(index = 1:n()) %&gt;% \n  ggplot(aes(index, valence)) +\n  geom_segment(aes(y = mean(valence),\n                   yend = mean(valence),\n                   x = 0,\n                   xend = length(valence))) +\n  geom_segment(aes(xend = index, yend = mean(valence)),\n               color = \"darkred\", alpha = 0.6) +\n  annotate(x = length(songs$valence),\n           y = mean(songs$valence, na.rm = TRUE),\n           label = \"Mean\",\n           geom = \"text\") +\n  geom_point()\n\n\n\n\n\nThe variance is the expected value of the squared deviation of a random variable from its mean.\n\nIn other words: Take the distance of all points to the mean and sum them (add all red lines in the plot above together) and then divide by \\(n-1\\).\n\\[var(X) = \\frac{\\sum_{i=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]\n“Hang on!”, I hear you saying: “Why \\(n-1\\)?”. And it is an excellent question. The first statement talked about an expected value. (One example of an expected value is the mean, which is the expected value of… well, the values). And indeed, and expected value often has the term \\(1/n\\). But the statement was talking about the expected value (of the squared deviation) for the whole population. We can only use the uncorrected version when we have the whole population (e.g. all songs that ever existed) and want to talk about that population. But usually, all we have is a sample, from which we want to draw conclusions about the population. But when we are using the sample to estimate the variance of the population, it will be biased. We can correct for this bias by using \\(n-1\\) instead of \\(n\\). This is known as Bessel’s correction. I am yet to come by a really intuitive explanation, but here is one idea: The thing we are dividing by is not necessarily the sample size any time we want to try to calculate the expected value of an estimator, it just happens to be the sample size in a bunch of cases. What the term really represents here is the degrees of freedom (DF) of the deviations. DFs can be thought of as the number of independent things. The degrees of freedom are \\(n\\) reduced by \\(1\\), because if we know the mean of a sample (we use it in our calculation), once we know all but \\(1\\) of the individual values, the last value is automatically known and thus doesn’t count towards the degrees of freedom.\n\n6.6.2 Standard deviation\nNext up: The Standard Deviation (SD) is the square root of the variance. Which is more commonly used on error bars, because the square root inverts the squaring that was done to get the variance. So we are back in the dimensions of the data.\n\\[\\sigma_X=\\sqrt{var(X)}\\]\n\n6.6.3 Standard Error of the Mean\nFinally, we have the Standard Error of the Mean, sometimes only called Standard Error (SEM, SE). It is also used very commonly in error bars. The reason for a lot of people to favor it over the SD might just be, that it is smaller, but they have distinct use-cases.\n\\[SEM=\\sigma / \\sqrt{n}\\]\nWe take the standard deviation and divide it by the square-root of \\(n\\). Imagine this: We actually have the whole population available. Like for example all penguins on earth. And then we repeatedly take samples of size \\(n\\). The means of these individual samples will vary, so it will have it’s own mean, standard deviation and variance. The standard error is the standard deviation of these means. So it is a measure of how far the means of repeated samples scatter around the true population mean. However, we don’t usually have the whole population! Measuring some property of all penguins in the world takes a long time, and running an experiment in the lab for all cells that exist and will ever exist takes an infinite amount of time. This is probably more than our research grant money can finance. So, instead, the Standard Error of the Mean used the standard deviation of our sample in the formula above. It is our best estimate for the standard deviation of the whole population. So, when you are trying so make inferences about the mean of the whole population based on your sample, it makes sense to also give the SEM as a way of quantifying the uncertainty.\nWhile R has functions for sd, mean and var, there is not built in function for the sem, but we can easily write one ourselves:\n\nsem &lt;- function(x) sd(x) / sqrt(length(x))\n\n\nsongs %&gt;% \n  drop_na(mode, valence) %&gt;% \n  group_by(mode) %&gt;% \n  summarise(\n    mean = mean(valence),\n    var = var(valence),\n    sd = sd(valence),\n    sem = sem(valence)\n  )\n\n# A tibble: 2 × 5\n   mode  mean    var    sd    sem\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     0 0.448 0.0498 0.223 0.0170\n2     1 0.475 0.0653 0.255 0.0182"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#or-how-to-lie-with-graphs",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#or-how-to-lie-with-graphs",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.7 … or: How to Lie with Graphs",
    "text": "6.7 … or: How to Lie with Graphs\nHowever, be very wary of simple bar graphs with error bars; there is a lot that can be misleading about them.\n\nsongs %&gt;% \n  drop_na(speechiness, mode) %&gt;% \n  group_by(mode) %&gt;%\n  summarise(across(speechiness, list(m = mean, sd = sd, sem = sem))) %&gt;% \n  ggplot(aes(factor(mode), speechiness_m, fill = factor(mode))) +\n  geom_errorbar(aes(ymin = speechiness_m - speechiness_sem,\n                    ymax = speechiness_m + speechiness_sem,\n                    color = factor(mode)),\n                size = 1.3, width = 0.3, show.legend = FALSE) +\n  geom_col(size = 1.3, show.legend = FALSE) +\n  coord_cartesian(ylim = c(0.06, 0.08)) +\n  scale_fill_manual(values = c(\"#1f6293\", \"#323232\")) +\n  scale_color_manual(values = c(\"#1f6293\", \"#323232\")) +\n  labs(title = \"Don't Do This at Home!\",\n       y = \"Speechiness\",\n       x = \"Mode (Minor / Major)\") +\n  theme(\n    plot.title = element_text(size = 44, family = \"Daubmark\",\n                              color = \"darkred\")\n  )\n\n\n\n\nWhen people say “The y-axis has to include 0”, this is the reason for it. It is no always true, when there is another sensible baseline that is not 0, but especially for barplots not having the y-axis start at 0 is about the most misleading thing you can do. The main reason for this is that humans perceive the height of the bars via their area, and this is no longer proportional when the bars don’t start at 0. This plot also makes no indication of the type of error-bars used or the sample size in each group. It uses the speechiness feature, but it hides the actual distribution behind just 2 numbers (mean and SEM) per group.\n\nsongs %&gt;% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\n\n\n\nSo the next time you see a barplot ask the question:\n\n\nSummary statistics by Horst (2020)\n\nI hope you can take some inspiration from this chapter and now have the vocabulary to know where to look when it comes to your own data."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#graphic-devices-fonts-and-the-ggplot-book",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#graphic-devices-fonts-and-the-ggplot-book",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.8 Graphic Devices, Fonts and the ggplot Book",
    "text": "6.8 Graphic Devices, Fonts and the ggplot Book\n\n6.8.1 ggplot book\nFirstly, for all things ggplot, the third edition of the ggplot book is currently being worked on by three absolute legends of their craft (“Welcome | Ggplot2,” n.d.; Wickham 2016). Hadley Wickham is the author of the original ggplot and ggplot2 package, Danielle Navaro makes amazing artwork with and teaches ggplot and Thomas Lin Pedersen is the current maintainer of ggplot2 and constantly makes cool features for it. The under-development book is already available online for free: https://ggplot2-book.org/.\n\n6.8.2 [Sidenote] Graphics Devices\nSecondly, we need to briefly talk about a concept we have only brushed by: graphics devices are to R what your printer is to your computer. When we create a plot in R, it starts out as mere numbers, but something has to turn these numbers into pixels (in the case of raster-images) or vectors (in the case of vector images; you might know svg or pdf files. Sorry, but these are not the vectors in R but rather descriptions of lines). This is the job ob the graphics device. When we use the ggsave function for example, it figures out what to use based on the file extension, but we can also specify it manually. I am mentioning this here, because in the plot I just showed you, I used a different font than the default. This is something that can be incredibly tricky for graphics devices, because fonts are handled differently on every operating system. Luckily, it is about to get way easier, because Thomas Lin Pedersen is working on another package, a graphics device, that is both really fast and works well with fonts. You can check the current development version here: https://ragg.r-lib.org/\nHere are some examples of using graphics devices manually by opening the device first and then finalizing the plot by closing the device:\n\npng(\"myplot.png\")\n\nsongs %&gt;% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\ndev.off()\n\n\nsvg(\"myplot.svg\")\n\nsongs %&gt;% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\ndev.off()\n\nOr by manually specifying the device in ggsave.\n\nplt &lt;- songs %&gt;% \n  ggplot(aes(speechiness, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\nggsave(\"newplot.png\", plt, device = ragg::agg_png)"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#the-normal-distribution-and-the-central-limit-theorem",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#the-normal-distribution-and-the-central-limit-theorem",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.9 The Normal Distribution and the Central Limit Theorem",
    "text": "6.9 The Normal Distribution and the Central Limit Theorem\nThere are many different distributions out there. Luckily, one of them is quite special and can be used in a multitude of settings. It is the harmlessly named Normal Distribution. R has the usual functions for it (density, probability, quantile, random).\n\ntibble(x = seq(-3, 3, 0.01)) %&gt;% \n  ggplot(aes(x)) +\n  geom_function(fun = dnorm) +\n  stat_function(geom = \"area\", fun = dnorm,\n              fill = \"darkblue\", alpha = 0.3) +\n  labs(y = \"density\", title = \"Normal Distribution Density\")\n\ntibble(x = seq(-3, 3, 0.01)) %&gt;% \n  ggplot(aes(x)) +\n  geom_function(fun = pnorm) +\n  labs(y = \"probability\", title = \"Cummulative Probability\")\n\n\n\n\n\n\n\nNow, why is is distribution so special?\n\nThe Central Limit Theorem (CLT) states that the sample mean of a sufficiently large number of independent random variables is approximately normally distributed. The larger the sample, the better the approximation.\n\nFor a great visualization of the central limit theorem, check out this interactive tutorial by Seeing Theory.\nBecause a lot of values we measure are actually the sum of many random processes, distributions of things we measure can often be approximated with a normal distribution.\nWe can visually test if some values follow the normal distribution by using a quantile-quantile plot, which plots the quantiles of our sample against where the quantiles should be on the normal distribution. A straight line means it is perfectly normal.\n\nqqnorm(songs$valence)\nqqline(songs$valence, col = \"red\")\n\n\n\n\nThe values close to the mean are pretty normal, but the tails of the distribution stray further from the normal distribution. There are way more very small and very large values than would be expected from a normal distribution.\n\n6.9.1 Log-normality\nThere is one thing that comes up a lot in biological data: because a lot of processes in biology are reliant on signal cascades, they tend to be the result of many multiplicative effects, rather than additive effects, as would be required for the Central Limit Theorem. As a result, they are not distributed normally, but rather log-normally, because taking the logarithm of all values transforms multiplicative effects into additive effects!"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#the-t-distribution",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#the-t-distribution",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.10 The T-Distribution",
    "text": "6.10 The T-Distribution\nThe CLT is only valid for large sample sizes. For smaller sample sizes, the distribution of means has fatter tails than a normal distribution. This is why for most statistical tests, we use the t-distribution instead of the normal distribution. As the degrees of freedom get higher, the t-distribution approaches the normal distribution.\n\nbase &lt;- ggplot() + xlim(-5, 5)\n\nbase +\n  geom_function(aes(colour = \"t, df = 1\"), fun = dt, args = list(df = 1), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 3\"), fun = dt, args = list(df = 3), size = 1.2) +\n  geom_function(aes(colour = \"t, df = 30\"), fun = dt, args = list(df = 30), size = 1.2) +\n  geom_function(aes(colour = \"normal\"), fun = dnorm, size = 1.2) +\n  guides(color = guide_legend(title = \"\")) +\n  scale_color_viridis_d()\n\n\n\nt-distributions; normal distribution in black.\n\n\n\nRemember the valence plot by mode?\n\nsongs %&gt;% \n  ggplot(aes(factor(mode), valence)) +\n  geom_violin(fill = \"darkblue\", alpha = 0.3) +\n  ggbeeswarm::geom_quasirandom(alpha = 0.6)\n\n\n\n\nFor our purposes we are going to treat these two distributions as close enough to a normal distribution at first so that we can look at some hypothesis tests:"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#students-t-test",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#students-t-test",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.11 Student’s T-Test",
    "text": "6.11 Student’s T-Test\nThe first test is called student’s t-test. “Student” was the pseudonym of it’s inventor. And the “t” stands for the t-distribution. We can use it to test the null hypothesis, that two samples come from the same (approximately normal) distribution.\n\nt.test(valence ~ mode, data = songs)\n\n\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -1.0857, df = 365.99, p-value = 0.2783\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.07614235  0.02197185\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.4751015 \n\n\nThe two samples are so similar that is is quite likely for those values to have come form the same distribution, so we would not reject the null hypothesis.\nLet us pretend for a moment that there is in fact a difference by creating some fake data (don’t do this in the lab…).\n\nfake_songs &lt;- songs %&gt;% \n  drop_na(mode, valence) %&gt;% \n  mutate(valence = if_else(mode == 1, valence + 0.2, valence))\n\n\nfake_songs %&gt;% \n  ggplot(aes(valence, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\n\n\n\nNow we end up with a statistically significant p-value.\n\nt.test(valence ~ mode, data = fake_songs)\n\n\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -9.1028, df = 365.99, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2761424 -0.1780282\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.6751015 \n\n\nNote, that the p-value itself says nothing about the effect size, the difference in means between the sample. You can get a significant p-value either by showing a tiny difference with lot’s of data points or by showing a larger difference with less data points.\nTests, that rely on the assumption of normality are called parametric tests, but when this assumption can not be met, we need non-parametric tests."
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#wilcoxon-rank-sum-test",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#wilcoxon-rank-sum-test",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.12 Wilcoxon rank-sum test",
    "text": "6.12 Wilcoxon rank-sum test\nThe Wilcoxon rank-sum test, or Mann–Whitney U test, is one of these. I get’s around the assumption of normality by transforming the data into ranks first. i.e. all points (independent of group) are ordered and their values replaced by their position in the ordering (their rank). If we think of the t-test as testing for a difference in means, we can think of the Wilcoxon rank-sum test as testing for a difference in medians.\n\nx &lt;- c(1, 3, 2, 42, 5, 1000)\nx\n\n[1]    1    3    2   42    5 1000\n\nrank(x)\n\n[1] 1 3 2 5 4 6\n\n\n\nwilcox.test(valence ~ mode, data = songs)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  valence by mode\nW = 15834, p-value = 0.3155\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n6.12.1 Direction of Testing\nBoth tests have the argument alternative, which can be any of c(\"two.sided\", \"less\", \"greater\"). This is the direction of our alternative hypothesis. Are we testing, for x being greater or less than y? Or are we testing for a difference in any direction (the default)? Having a hypothesis about the direction beforehand will result in smaller p-values (half of the two-sided ones), but you need to have this hypothesis before looking at the data, and especially not after running e.g. the two sided test and then deciding, that you want a smaller p-value! This is not how p-values work.\n\nt.test(valence ~ mode, data = fake_songs, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  valence by mode\nt = -9.1028, df = 365.99, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is less than 0\n95 percent confidence interval:\n       -Inf -0.1859473\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4480163       0.6751015 \n\n\n\nfake_songs %&gt;% \n  ggplot(aes(valence, color = factor(mode), fill = factor(mode))) +\n  geom_density(alpha = 0.3)\n\n\n\n\nIf you are unsure about how to tell the functions, which of two groups is supposed to be greater or lesser, you can also supply the data as x and y instead of using the formula interface as I did above:\n\nvalence_major &lt;- fake_songs %&gt;% filter(mode == 1) %&gt;% pull(valence)\nvalence_minor &lt;- fake_songs %&gt;% filter(mode == 0) %&gt;% pull(valence)\n\nt.test(valence_major, valence_minor, alternative = \"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  valence_major and valence_minor\nt = 9.1028, df = 365.99, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.1859473       Inf\nsample estimates:\nmean of x mean of y \n0.6751015 0.4480163 \n\n\nIf we save the result of the test, we can inspect the object further and extract information from it.\n\ntest &lt;- t.test(valence_major, valence_minor, alternative = \"greater\")\ntest$p.value\n\n[1] 2.893211e-18\n\n\n\n6.12.2 Confidence Intervals\nThe t.test on a lonely sample can also be used to create confidence intervals around a mean. In short for example a 95% confidence interval is the range in which we would expect the mean of a sample to fall in 95% of cases when we repeat an experiment an infinite amount of times. These confidence intervals are also sometimes used as error bars in plots.\n\ntest &lt;- t.test(songs$valence)\ntest$conf.int\n\n[1] 0.4377443 0.4871400\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#chrunching-dimensions-with-dimensionality-reduction-pca",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#chrunching-dimensions-with-dimensionality-reduction-pca",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.13 Chrunching Dimensions with Dimensionality Reduction: PCA",
    "text": "6.13 Chrunching Dimensions with Dimensionality Reduction: PCA\nLastly for today, we are going a bit out of scope. We are leaving the realm of looking at individual features and try to condense all the information into as little space as possible.\nThe general notion of Dimensionality Reduction is to take all the features that we have and construct new features from them, so that we can represent our data with fewer features while loosing little information.\nFor example, when two features are highly correlated i.e. one changes when the other does, we might be better off replacing them with a single new feature, that goes along the axis of maximum variance between the two. A number along this line accounts for most of the variance in these points, and the rest can be accounted for by a number describing the distance to that line (a perpendicular axis), which is less important than the first axis we found.\n\nsongs %&gt;% \n  ggplot(aes(x = energy,\n             y = loudness,\n             label = track_name)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nImagine you are whale shark!\n\n\nWhale shark by Horst (2020)\n\nAnd want to orient your mouth in such a way that you can eat the greatest amount of krill in one sweep.\n\n\nKrill by Horst (2020)\n\nThis is your first principal component. The second is perpendicular to the first. This is a throwback to “Math for Natural Scientists” and linear algebra, we are defining a new coordinate system here.\nBut whale sharks swim in 3 dimensions, not 2, and our data has even more dimensions, with features being represented dimensions.\n\nIt can be quite hard for humans to imaging being an N-dimensional whale shark.\n\nBut R and tidymodels has us covered.\nPCA is not a model in itself, but rather a data preprocessing step that generates new features (the principal components), which we can later use for other models. But today, we will do just the preprocessing by itself.\nIn tidymodels, preprocessing is done by defining a recipe:\n\nsongs_rec &lt;- recipe( ~ ., data = songs) %&gt;% \n  update_role(track_name, track_artists, track_uri, new_role = \"id variable\") %&gt;% \n  step_naomit(all_predictors()) %&gt;% \n  step_normalize(all_predictors()) %&gt;% \n  step_pca(all_predictors(), id = \"pca\")\n\nsongs_rec\n\nWe then take the recipe and prepare it.\n\nsongs_prep &lt;- prep(songs_rec)\nsongs_prep\n\nWe can now explore, how the data looks like in these new dimensions. We do so, by baking the prepared recipe.\n\nsongs_baked &lt;- bake(songs_prep, songs)\nsongs_baked\n\n# A tibble: 393 × 8\n   track_name      track_artists track_uri    PC1    PC2     PC3     PC4     PC5\n   &lt;fct&gt;           &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Africa          TOTO          spotify:…  1.21  -3.32  -0.808   0.0396  0.811 \n 2 Take on Me      a-ha          spotify:… -1.43  -1.84  -0.680  -0.400   0.376 \n 3 Wake Me Up Bef… Wham!         spotify:…  0.257 -3.23   0.397   0.242  -1.09  \n 4 I'm Still Stan… Elton John    spotify:… -1.08  -1.21   1.61    1.76    1.24  \n 5 Wolf Totem (fe… The HU;Jacob… spotify:… -1.71   1.26   2.24   -1.74    1.47  \n 6 Yuve Yuve Yu    The HU        spotify:… -0.774  0.792 -1.31   -0.446   0.743 \n 7 The Search      NF            spotify:… -0.826 -0.511 -0.0294  2.62   -0.0331\n 8 Stressed Out    Twenty One P… spotify:… -1.78  -0.647 -0.506   1.66    0.734 \n 9 Stressed Out    Twenty One P… spotify:… -1.78  -0.647 -0.506   1.66    0.734 \n10 Ride            Twenty One P… spotify:… -1.08  -1.23  -0.907  -0.507  -1.22  \n# ℹ 383 more rows\n\n\nThe original features where replace by Principal Components that explain most of the variance. From the prepared recipe, we extract a tidy form of the step we care about (usually the last one) to see, what happened to our data. We can see, which features ended up contributing to which components by getting the results of the pca step of our recipe.\n\nterms &lt;- tidy(songs_prep, id = \"pca\") %&gt;% \n  mutate(component = parse_number(component))\n\nterms\n\n# A tibble: 225 × 4\n   terms              value component id   \n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n 1 danceability     -0.226          1 pca  \n 2 energy           -0.423          1 pca  \n 3 key              -0.0765         1 pca  \n 4 loudness         -0.429          1 pca  \n 5 mode              0.0942         1 pca  \n 6 speechiness      -0.121          1 pca  \n 7 acousticness      0.399          1 pca  \n 8 instrumentalness  0.370          1 pca  \n 9 liveness         -0.118          1 pca  \n10 valence          -0.295          1 pca  \n# ℹ 215 more rows\n\n\nLet’s make this a plot!\n\ncolors &lt;- fishualize::fish_pal(option = \"Centropyge_loricula\")(5)[3:4]\n\nterms %&gt;% \n  filter(component &lt;= 3) %&gt;% \n  mutate(terms = tidytext::reorder_within(terms, by = value, within = component)) %&gt;% \n  ggplot(aes(value, terms, fill = factor(sign(value)))) +\n  geom_col() +\n  scale_fill_manual(values = colors) +\n  facet_wrap(~component, labeller = label_both, scales = \"free\") +\n  tidytext::scale_y_reordered() +\n  guides(fill = \"none\")\n\n\n\n\nWe had to use 2 little helper functions from the tidytext package to properly order the bar. The first component is largely comprised of a high acousticness and instrumentalness and less energy in the positive direction. So we expect e.g. classical music to be very high on that axis. A high value on the second component means a high danceability while being low in tempo.\nSo where do our songs end up in principle component space?\n\nplt &lt;- songs_baked %&gt;% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(text = paste(track_name, \",\", track_artists)))\n\nplotly::ggplotly(plt)\n\n\n\n\n\nYou can now imagine, using this simpler representation of the songs in principal component space, to for example propose new songs to users based on songs that are close to songs they listened to in this representation.\nLastly, I want to stress, that the principal components are not created equal. The first component is always the most important. Here, we see, that almost 27% of the variance can be explained just by the first component, so exploring more than 2 really makes little sense here.\n\ntidy(songs_prep)\n\n# A tibble: 3 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      naomit    TRUE    TRUE  naomit_nB6On   \n2      2 step      normalize TRUE    FALSE normalize_n2kdf\n3      3 step      pca       TRUE    FALSE pca            \n\ntibble(\n  sdev = songs_prep$steps[[3]]$res$sdev,\n  explained_variance = sdev^2 / sum(sdev^2),\n  pc = 1:length(sdev)\n) %&gt;% \n  ggplot(aes(pc, explained_variance)) +\n  geom_col(fill = \"darkgreen\") +\n  geom_text(aes(label = scales::percent_format()(explained_variance)),\n            size = 3,\n            vjust = 1.1,\n            color = \"white\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x = NULL, y = \"Percent variance explained by each PCA component\")"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#exercises",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#exercises",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.14 Exercises",
    "text": "6.14 Exercises\nThe tidytuesday project also had a spotify dataset. This one es even more interesting, because it ranges across different playlists of various genres and is annotated with said genres. And it has more data (Over 30000 songs)! Download it here:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md\n\n6.14.1 The Plotty Horror Picture Show\nSometimes we have to experience true horror to see the light in the darkness. Take the spotify data and make a plot that is truly horrible! I would appreciate a couple of sentences about your thought process and what makes your plot particularly bad. You can strike terror into the reader’s heart in multiple ways. Here are some ideas, mix and match what suits you:\n\nMake it really ugly by experimenting with different theme options.\nMake it really misleading by defying viewer expectations and breaking all norms. You are an artist now, norms don’t apply to your art.\nWhat even are axis labels?\nExperiment with different (manual) color schemes! Using Red and Green is an excellent choice if you want to make sure your plot is unreadable for every 12th man (due to the high prevalence of red-green-blindness).\nTry out different geoms and combinations of aesthetics, maybe find the ones that are the worst possible choice for the features.\n\n6.14.2 Take a Sad Plot and Make it Better\nThe title of this exercise is stolen from this talk by Alison Hill.\nNow use what you learned to make a great plot! Pick some features that you are interested in and visualize them as informative and beautiful as possible, while still staying honest to the data. Maybe you are interested in changes over time, maybe you find your favorite artist and want to situate them in the context of other works. Maybe you want to explore how different features relate to each other or even want to attempt to recreate the PCA to see, if you can find clusters of genres. It is your call.\nI am curious to see, what you come up with!"
  },
  {
    "objectID": "06-distributions-summaries-and-dimensionality-reduction.html#resources",
    "href": "06-distributions-summaries-and-dimensionality-reduction.html#resources",
    "title": "\n6  Distributions, Summaries and Dimensionality Reduction\n",
    "section": "\n6.15 Resources",
    "text": "6.15 Resources\n\nTidymodels website\nTidymodels book\nggplot book\nragg graphics device\n\n\n\n\n\nHorst, Allison. 2020. “Artwork by @Allison_horst.” https://github.com/allisonhorst/stats-illustrations. https://github.com/allisonhorst/stats-illustrations.\n\n\n“Welcome | Ggplot2.” n.d. https://ggplot2-book.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. 2nd ed. 2016 edition. New York, NY: Springer."
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#setup",
    "href": "07-fallacies-correlation-and-regression.html#setup",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.1 Setup",
    "text": "7.1 Setup\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(broom)"
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#data-considerations",
    "href": "07-fallacies-correlation-and-regression.html#data-considerations",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.2 Data Considerations",
    "text": "7.2 Data Considerations\n\n7.2.1 1943\nIt is 1943. The second World War is well underway, ravaging large parts of Europe. Military aircraft that had first entered the stage in World War I are now reaching their peak importance as they rain fire from the skies. But the Allied forces are facing a problem. As warplanes get better, so do anti-aircraft systems. In an effort to improve the survival of their fleet, the US military starts examining the planes returning from skirmishes with the opposing forces. They characterize the pattern of bullet holes in the metal hull, meticulously noting down each hit that the plane sustained. The resulting picture is better summarized in the modern, redrawn version below.\n\n\nFigure from Wikipedia “Survivorship Bias” (2020).\n\nAfter taking a look at the data they gathered, the military is ready to rush into action. To improve the endurance of their aircraft, the plan is to reinforce the parts of the plane that were most often hit by bullets. With stronger wings and a sturdier body of the plane, they think, surely more pilots will come back from their missions safely. They were wrong.\nBut the pilots where in luck. The military also consulted with the Statistics Research Group at Columbia University. A man named Abraham Wald worked there. In his now unclassified report “A method of estimating plane vulnerability based on damage of survivors”, he argued against the generals’ conclusion (Wald 1980). Instead of the most-hit parts of the planes, the least-hit parts are to be reinforced.\n\n\nCover of “A method of estimating plane vulnerability based on damage of survivors” Wald (1980)\n\n\nInstead of the most-hit parts, the least-hit parts are to be reinforced.\n\nThe reason for this seemingly counterintuitive result is what is now known as survivorship bias. The data that was collected contained only survivors, those planes that sustained damage not severe enough to hinder them from coming back after their mission. The aircraft that where hit in other places simply didn’t make it back. Consequently, Wald advised to reinforce the engines and the fuel tanks.\n\n7.2.2 Thinking further\nThis is but one of a multitude of biases, specifically a selection bias, that will influence the quality of the inferences you can draw from available data. Keep in mind, data is not objective and never exists in a vacuum. There is always context to consider. The way the data was collected is just one of them. A lot of these ideas seem obvious in hindsight, which incidentally is another bias that social psychologists call hindsight bias, but they can sometimes be hard to spot.\nA common saying is that music was better back in the days, or that all the old music still holds up while the new stuff on the radio just sounds the same. Well, not quite. This is also survivorship bias at work. All the bad and forgettable songs from the past just faded into oblivion, never to be mentioned again, while the songs people generally agreed to be good survived the ravages of time unscathed. A similar thing happens with success in general, not just songs. If you ask any CEO high up the corporate ladder, a millionaire, or the author of a book that reads “How to get rich”, they are sure to have a witty anecdote about how their persistence, or their brilliance, or charisma got them to where they are now. What we are not seeing is all the people just as witty, just as charismatic or even just as persistent that where simply not as lucky. Very few people will tell you this. Because it takes a whole lot of courage to admit that ones success is based on luck and privilege.\nAnd to take it back to the scientific context: When you are planning an experiment for the lab, always ask whether your data collection process can in some way be biased towards what you are trying to show.\nI leave you with this:\n\nAnd from this cautionary tale we jump straight back into RStudio."
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#sidenotes",
    "href": "07-fallacies-correlation-and-regression.html#sidenotes",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.3 Sidenotes",
    "text": "7.3 Sidenotes\n\n7.3.1 Glue and Inline R Code\nUsing paste to create a text in which the values of variables are inserted can be painful.\n\nname &lt;- \"Jannik\"\nage &lt;- 26\ntext &lt;- paste(name, \"is\", age, \"years old.\")\ntext\n\n[1] \"Jannik is 26 years old.\"\n\n\nThe glue package makes it a breeze. Everything inside of curly braces in the text inside of the glue function will be evaluated as regular R code, enabling us to write text quite naturally:\n\ntext &lt;- glue(\"{name} is {age} years old.\")\ntext\n\nJannik is 26 years old.\n\n\nI hope you are not too confused by the package and it’s main function having the same name.\n\nglue(\"{name} is {age + 10} years old.\")\n\nJannik is 36 years old.\n\n\n\n7.3.2 Inline R code\nUsing the a backtick followed by the letter r we can add the results of code right into the text sections of Rmarkdown reports:\n1 + 1 = 2.\nJannik is 26 years old.\n\n7.3.3 Best Practices\nSpeaking of being careful. There is one rule I can give you to make your data analysis more secure:\n\nYour raw data is sacred! Do not ever modify it or save over it.\n\nThis is even more important when, for example, using excel to preview a csv file. Under no circumstances should you hit the save button in excel when you are looking at the raw data. With approximately one-fifth of genomic research papers containing errors in the gene lists, because excel converted genes such as SEPT2 (Septin 2) into dates, you can see why (Ziemann, Eren, and El-Osta 2016). Biologists have since given up and renamed the genes that where commonly converted into dates… but the point still stands. This caution is of course also necessary when analyzing data with R, not just excel. When we read in the raw data and save a processed version, we create a new file, or even better, a new folder for it. A good convention for example would be do divide your data into a raw and derived folder."
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#covariance-correlation-and-regression",
    "href": "07-fallacies-correlation-and-regression.html#covariance-correlation-and-regression",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.4 Covariance, Correlation and Regression",
    "text": "7.4 Covariance, Correlation and Regression\n\n\nSource: https://xkcd.com/552/\n\nLast week, we talked about a measure of the spread of a random variable called the variance.\n\\[var(X) = \\frac{\\sum_{i=0}^{n}{(x_i-\\bar x)^2}}{(n-1)}\\]\nToday, we are extending this idea to 2 random variables. Because the normal distribution is so common, we are using two normally distributed variables.\n\nN &lt;- 50\ndf &lt;- tibble(\n  x = rnorm(N),\n  y = rnorm(N)\n)\n\nm_x &lt;- mean(df$x)\nm_y &lt;- mean(df$y)\n\nggplot(df, aes(x, y)) +\n  geom_vline(xintercept = m_x, alpha = 0.8, color = \"midnightblue\") +\n  geom_hline(yintercept = m_y, alpha = 0.8, color = \"midnightblue\") +\n  geom_point(fill = \"white\", color = \"black\")\n\n\n\n\nWe also added lines for the means of the two random variables. Maybe I should have mentioned this more clearly earlier on, but the general convention in statistics is that random variables are uppercase and concrete values from the distribution have the same letter but lowercase.\nWe now get the covariance of X and Y as:\n\\[cov(X,Y)=\\text{E}\\left[(X-\\text{E}\\left[X\\right])(Y-\\text{E}\\left[Y\\right])\\right]\\]\nThe expected value \\(E[X]\\) is just a fancy way of saying the mean of X. If we asses the contribution of individual points towards the covariance, we can understand it quite intuitively. A point that has a higher x than the mean of X and a higher y than the mean of Y (top right quadrant) will push the covariance towards positive values. Likewise, a point in the bottom left quadrant will have negative differences with the X and Y mean, which cancel each other out to result in a positive covariance. The bottom right and top left quadrants push towards a negative covariance. A mix of positive and negative contributions will result in a covariance with a small absolute value.\nThe covariance has one problem: It will have weird units (X times Y) and the scale is different depending on the random variables. So what we do is standardize it by dividing by both standard deviations and get the correlation coefficient:\n\\[cor(X,Y)=\\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}\\]\nIt can assume values between -1 and 1. It’s full name is Pearson product-moment correlation coefficient, or pearsons R. We can square it to get \\(R^2\\) (obviously), which indicates the strength of the correlation with values between 0 and 1 independent of the direction. We will meet it again later.\nLet us apply our knowledge to a new dataset.\n\n7.4.1 Introducing the Dataset\nThe dplyr package includes and example dataset of Star Wars characters. Unfortunately, it was created a while ago, so the is no baby yoda, but 87 other characters are present.\n\n\nI guess it is the baby yoda show now.\n\n\nstarwars\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nLet’s look at some correlations:\n\n7.4.2 Pearson vs. Spearman (not a Boxing Match)\nTo compute pearsons correlation, we use the cor function in R. Instead of filtering out NA, we can use use = \"complete.obs\" to ignore rows with missing values in the computation.\n\npearson &lt;- cor(starwars$height, starwars$mass, use = \"complete.obs\")\npearson\n\n[1] 0.1338842\n\n\nWhen I first did this I was surprised that the correlation was so low. We are after all talking about height and mass, which I assumed to be highly correlated. Let us look at the data to see what is going on.\n\nlabel_text &lt;- glue(\"Pearson correlation: {round(pearson, 2)}\")\n\njabba &lt;- filter(starwars, str_detect(name, \"Jabba\"))\njabba_text &lt;- list(x = 1100, y = 120)\n\nstarwars %&gt;% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 500, y = 75, label = label_text,\n           hjust = 0) +\n  annotate(geom = \"curve\",\n           x = jabba_text$x, y = jabba_text$y,\n           xend = jabba$mass, yend = jabba$height,\n           curvature = .3,\n           arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\",\n           x = jabba_text$x,\n           y = jabba_text$y, label = \"Jabba the Hutt\",\n           hjust = 1.1) +\n  xlim(0, 1500) +\n  labs(x = \"mass [kg]\",\n       y = \"height [cm]\")\n\n\n\n\nThis is the culprit! We have a massive outlier, in all senses of the word “massive”. Luckily, there is another method to asses correlation. Spearman’s method is more resistant to outliers, because the data is transformed into ranks first, which negates the massive effect of outliers.\n\nspearman &lt;- cor(starwars$height, starwars$mass,\n                use = \"complete.obs\", method = \"spearman\")\nspearman\n\n[1] 0.7516794\n\n\nVisually, this is what the points look like after rank transformation:\n\nlabel_text &lt;- glue(\"Spearman rank correlation: {round(spearman, 2)}\")\n\nstarwars %&gt;% \n  mutate(mass = rank(mass),\n         height = rank(height)) %&gt;% \n  ggplot(aes(mass, height)) +\n  geom_point() +\n  annotate(geom = \"text\", x = 0, y = 75, label = label_text,\n           hjust = 0) +\n  labs(x = \"rank(mass)\",\n       y = \"rank(height)\")\n\n\n\n\nApart from cor, there is also cor.test, which gives more information. If we so fancy, we can use broom to turn the test output into a tidy format as well.\n\ncortest &lt;- cor.test(starwars$mass, starwars$height,\n                # method = \"spearman\",\n                use = \"complete.obs\")\n\ncortest\n\n\n    Pearson's product-moment correlation\n\ndata:  starwars$mass and starwars$height\nt = 1.02, df = 57, p-value = 0.312\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1265364  0.3770395\nsample estimates:\n      cor \n0.1338842 \n\n\n\ntidy(cortest)\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.134      1.02   0.312        57   -0.127     0.377 Pearson's… two.sided  \n\n\nThere is another way we can specify which features to correlate. corr also takes a matrix or data frame as it’s x argument instead of x and y. We then end up with the pairwise correlation coefficients for all columns of the dataframe.\nThis is known as a correlation matrix, and we can create it for more than two features, as long as all features are numeric (after all, what is the correlation between 1,4 and “cat” “dog”?). Unfortunately there are only three numeric columns in the starwars dataset, which makes for a pretty boring correlation matrix.\n\nstarwars %&gt;%\n  select(where(is.numeric)) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  height  mass birth_year\n   &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1    172    77       19  \n2    167    75      112  \n3     96    32       33  \n4    202   136       41.9\n5    150    49       19  \n6    178   120       52  \n\n\nSo let’s look at another built-in dataset instead. mtcars has some data about cars, like their engine displacement or miles per gallon.\n\nmtcars %&gt;% \n  ggplot(aes(disp, mpg)) +\n  geom_point()\n\n\n\n\nThis makes for a much more interesting correlation matrix:\n\ncor(mtcars) %&gt;% \n  as_tibble(rownames = \"feature\") %&gt;% \n  pivot_longer(-feature) %&gt;% \n  ggplot(aes(feature, name, fill = value)) +\n  geom_raster() +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"blue\", high = \"red\",\n                       mid = \"white\", midpoint = 0)\n\n\n\n\nIf you are working a lot with correlations, it is certainly worth checking out the corrr package from the tidymodels framework:\n \nIts functions make these steps easier.\n\ncorrr::correlate(mtcars) %&gt;% \n  corrr::stretch()\n\n# A tibble: 121 × 3\n   x     y          r\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 mpg   mpg   NA    \n 2 mpg   cyl   -0.852\n 3 mpg   disp  -0.848\n 4 mpg   hp    -0.776\n 5 mpg   drat   0.681\n 6 mpg   wt    -0.868\n 7 mpg   qsec   0.419\n 8 mpg   vs     0.664\n 9 mpg   am     0.600\n10 mpg   gear   0.480\n# ℹ 111 more rows\n\n\nAnd give use access to two different types of plots out of the box.\n\ncorrr::correlate(mtcars) %&gt;% \n  corrr::rplot()\n\n\n\n\n\ncorrr::correlate(mtcars) %&gt;% \n  corrr::network_plot()\n\n\n\n\n\n7.4.3 Difference to Linear Regression\nFinally, linear regression is a related concept, because both correlation and linear regression quantify the strength of a linear relationship. However, there are key differences. When we fit a linear model like:\n\\[y \\sim a + x * b\\]\nthere is no error in x. We assume x is something that is fixed, like the temperature we set for an experiment or the dosage we used. Y on the other hand is a random variable. In cov(X,Y) and cor(X,Y), X and Y are both random variables, usually things we observed, not set ourselves.\nWhile the correlation coefficient is symmetrical and translation-scale-invariant:\n\\[cor(X,Y)=cor(Y,X)\\]\n\\[cor(X,Y)=cor(X * a +b,Y * c + d)\\]\nThe same is not true for linear models!\nLet us look at an example where linear regression is more appropriate than correlation. In the data folder we find the IMDB ratings for 10 Star Wars movies (plus more features).\n\nratings &lt;- read_rds(\"data/07/starwars_movies.rds\")\nratings\n\n# A tibble: 10 × 25\n   Title    Rated Released   Runtime Genre Director Writer Actors Plot  Language\n   &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n 1 Star Wa… PG    1977-05-25 121 min Acti… George … Georg… Mark … Luke… English \n 2 Star Wa… PG    1980-06-20 124 min Acti… Irvin K… Leigh… Mark … Afte… English \n 3 Star Wa… PG    1983-05-25 131 min Acti… Richard… Lawre… Mark … Afte… English \n 4 Star Wa… PG-13 2015-12-18 138 min Acti… J.J. Ab… Lawre… Daisy… As a… English \n 5 Star Wa… PG    1999-05-19 136 min Acti… George … Georg… Ewan … Two … English…\n 6 Star Wa… PG-13 2005-05-19 140 min Acti… George … Georg… Hayde… Thre… English \n 7 Star Wa… PG    2002-05-16 142 min Acti… George … Georg… Hayde… Ten … English \n 8 Star Wa… PG-13 2017-12-15 152 min Acti… Rian Jo… Rian … Daisy… The … English \n 9 Rogue O… PG-13 2016-12-16 133 min Acti… Gareth … Chris… Felic… In a… English \n10 Star Wa… PG-13 2019-12-20 141 min Acti… J.J. Ab… Chris… Daisy… In t… English \n# ℹ 15 more variables: Country &lt;chr&gt;, Awards &lt;chr&gt;, Poster &lt;chr&gt;,\n#   Ratings &lt;list&gt;, Metascore &lt;chr&gt;, imdbRating &lt;dbl&gt;, imdbVotes &lt;dbl&gt;,\n#   imdbID &lt;chr&gt;, Type &lt;chr&gt;, DVD &lt;date&gt;, BoxOffice &lt;chr&gt;, Production &lt;chr&gt;,\n#   Website &lt;chr&gt;, Response &lt;chr&gt;, year &lt;dbl&gt;\n\n\nWe can fit a linear model to see if the production year has an effect on the rating.\n\nmodel &lt;- lm(imdbRating ~ year, data = ratings)\n\naugment(model) %&gt;% \n  ggplot(aes(year, imdbRating)) +\n  geom_smooth(method = \"lm\", alpha = 0.3, color = \"midnightblue\") +\n  geom_segment(aes(x = year, y = .fitted,\n                   xend = year, yend = imdbRating),\n               alpha = 0.4) +\n  geom_point()\n\n\n\n\nWhat I added here as gray segments are the so called residuals. They are what makes linear regression work. It’s full name is Ordinary Least Squares and the squares in question are the squares of these residuals, the word least indicates that these squares are minimized in order to find the best fit line.\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  77.1      28.3         2.73  0.0260\n2 year         -0.0348    0.0141     -2.46  0.0393\n\n\nLooks like every year decreases the estimated rating by 0.03.\nOne thing however is the same between correlation and linear regression, and that is the \\(R^2\\) value we get from both calculations:\n\nsummary(model)\n\n\nCall:\nlm(formula = imdbRating ~ year, data = ratings)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1000 -0.2467  0.1261  0.3880  0.7913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 77.13043   28.29937   2.726   0.0260 *\nyear        -0.03478    0.01414  -2.460   0.0393 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6872 on 8 degrees of freedom\nMultiple R-squared:  0.4306,    Adjusted R-squared:  0.3595 \nF-statistic: 6.051 on 1 and 8 DF,  p-value: 0.03933\n\n\nWe can interpret \\(R^2\\) as the fraction of the variance of the response variable y that can be explained by the predictor x."
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#non-linear-least-squares",
    "href": "07-fallacies-correlation-and-regression.html#non-linear-least-squares",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.5 Non-linear Least Squares",
    "text": "7.5 Non-linear Least Squares\nSo far, we only properly dealt with linear relationships and now it is time to get non-linear. We will be creating a mechanistically driven predictive model, so we have a formula of which we want to adjust the parameters so that it fits our data.\nLet’s take classical Michaelis-Menten-Kinetics There is a dataset for enzyme reaction rates included in R. But we convert it from a dataframe to a tibble so that it prints nicer:\n\npuromycin &lt;- as_tibble(Puromycin)\npuromycin\n\n# A tibble: 23 × 3\n    conc  rate state  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1  0.02    76 treated\n 2  0.02    47 treated\n 3  0.06    97 treated\n 4  0.06   107 treated\n 5  0.11   123 treated\n 6  0.11   139 treated\n 7  0.22   159 treated\n 8  0.22   152 treated\n 9  0.56   191 treated\n10  0.56   201 treated\n# ℹ 13 more rows\n\n\nThe initial rate \\(v_0\\) of the an enzymatic reaction was measured for a control and a sample treated with puromycin at different substrate concentrations. For every concentration we have two replicates except for one missing replicate.\n\npuromycin %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point()\n\n\n\n\nFrom our Biochemistry studies, we know that we can express the rate depending on the concentration with the following formula:\n\\[rate=\\frac{(Vm * conc)}{(K + conc)}\\]\nTo make it easier to work with, let’s turn it into a function.\n\nrate &lt;- function(conc, Vm, K) {\n  Vm * conc / (K + conc)\n}\n\nLet’s pick some arbitrary starting values. For example, we see that the maximal velocity could be around 200. We also know that K is the concentration at which the half-maximal velocity is reached.\n\npuromycin %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ rate(conc = .x, Vm = 200, K = 0.2),\n                color = \"black\")\n\n\n\n\ngeom_function expects a function of x or an anonymous function where the first argument is the values on the x-axis, so this is what we did. Well, I bet we can do better than guessing the function! What R can do for us is the same it did for linear least squares and that is minimizing the distance of our curve to the datapoints. This is the job of the nls function, which stands for Nonlinear Least Squares.\n\n7.5.1 One model\nLet’s look at just the “treated” data first.\n\ntreated &lt;- filter(puromycin, state == \"treated\")\nmodel &lt;- nls(rate ~ rate(conc, Vm, K),\n             data = treated,\n             start = list(Vm = 200, K = 0.3)\n             )\n\nmodel\n\nNonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: treated\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06\n\n\nNlS needs starting values, so we use any guess that isn’t too far off. If it is completely wrong, the model doesn’t know in which direction it should move the parameters to improve the fit and we get an error like this: Error in nls(rate ~ rate(conc, Vm, K), data = puro, subset = state ==  : singular gradient\n\nFor this special case, R also has a self-starting model. I won’t go into it because it is not as useful as the general concept of fitting arbitry functions, but you can check out SSmicmen for a model that estimes the starting values automatically.\n\nAdditionally, nls takes an argument subset, which works like the dplyr verb filter so that we can fit the model on a subset of the data without having to create it beforehand.\nWe use the broom package to display our model parameters in a tidy tibble.\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Vm    213.       6.95        30.6  3.24e-11\n2 K       0.0641   0.00828      7.74 1.57e- 5\n\n\nWith the base-R function predict we can make new predictions based on a model and new data:\n\nhead(predict(model, newdata = list(conc = seq(0, 1, 0.01))))\n\n[1]  0.00000 28.69405 50.56602 67.79038 81.70621 93.18326\n\n\nWe can use the same function inside of geom_function:\n\ntreated %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_function(fun = ~ predict(model, newdata = list(conc = .x)),\n                color = \"black\")\n\n\n\n\nOr alternatively create a new dataset of predictions beforehand and use that with geom_line:\n\npredictions &lt;- tibble(\n  conc = seq(0, 1, 0.01),\n  rate = predict(model, newdata = list(conc = conc))\n)\n\ntreated %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions, color = \"black\")\n\n\n\n\n\naugment(model) %&gt;% \n  ggplot(aes(conc, .resid)) +\n  geom_point()\n\n\n\n\n\n7.5.2 Multiple models\nNow, what if we want to fit the model for both states? We can resort back to our trusty purrr package like we did in an earlier lecture.\nWe start out by creating a function that takes a dataframe and fits our model:\n\nfit_micmen &lt;- function(data) {\n  nls(rate ~ rate(conc, Vm, K),\n      data = data,\n      start = list(Vm = 200, K = 0.3)\n  ) \n}\n\nAnd by nesting the data (grouped by state) into a list column we can map this function over each dataset. And to get the fitted parameters we map the tidy function from broom over the fitted models.\n\nmodels &lt;- puromycin %&gt;% \n  group_by(state) %&gt;% \n  nest() %&gt;% \n  mutate(\n    model = map(data, fit_micmen),\n    params = map(model, tidy)\n  )\n\nLet’s inspect the fitted parameters.\n\nmodels %&gt;% \n  select(state, params) %&gt;% \n  unnest(params) %&gt;% \n  select(state, term, estimate) %&gt;% \n  pivot_wider(names_from = term, values_from = estimate)\n\n# A tibble: 2 × 3\n# Groups:   state [2]\n  state        Vm      K\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 treated    213. 0.0641\n2 untreated  160. 0.0477\n\n\nTo plot our fitted models we have two options. Firstly, we could generate the predicted values for a number of concentrations beforehand and then plot these:\n\nmake_predicions &lt;- function(model) {\n  tibble(\n    conc = seq(0, 1.2, 0.01),\n    rate = predict(model, newdata = list(conc = conc))\n  )\n}\n\npredictions &lt;- models %&gt;% \n  mutate(\n    preds = map(model, make_predicions)\n  ) %&gt;% \n  select(state, preds) %&gt;% \n  unnest(preds)\n\npuromycin %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_line(data = predictions)\n\n\n\n\nOr we use geom_smooth, which can take “nls” as a method as well. We just need to make sure to pass the correct arguments. And it can be confusing, because when we are specifying the formula in geom_smooth, it always needs to be a formula of y ~ x, whereas in the normal nls we did earlier, we specified the variables in terms of their actual names (rate and conc).\n\npuromycin %&gt;% \n  ggplot(aes(conc, rate, color = state)) +\n  geom_point() +\n  geom_smooth(method = \"nls\",\n              formula = y ~ rate(conc = x, Vm, K),\n              method.args = list(start = list(Vm = 200, K = 0.3)),\n              se = FALSE\n              )\n\n\n\n\nWe also need se = FALSE, because by default R would try to plot a confidence interval around the fit-line like it did for the linear model, but nls doesn’t return one, so we would get an error.\nThe unfortunate thing about this method is that we end up fitting the model twice, once to get the estimated parameters and the likes for ourselves and a second time in ggplot to display the fitted lines. But in most cases this is not a problem, because the model is not very computationally expensive.\n\n7.5.3 Excursion: A weird error message\nFinally, I want to take minute to mention another approach which we took earlier in the series when we where fitting many linear models and show you, why it unfortunately does not work here.\n\nnewmodels &lt;- puromycin %&gt;% \n  group_by(state) %&gt;% \n  summarise(\n    model = list(nls(rate ~ rate(conc, Vm, K),\n                start = list(Vm = 200, K = 0.3))\n    )\n  )\n\nAt first it looks like everything is fine. Because we are inside of a dplyr verb nls know where to look for the columns rate and conc that it should fit, so we are not specifying its data argument. However, this fails in an unexpected way when we later try to make predictions with one of the models:\n\nmake_predicions(newmodels$model[[1]])\n\nError:\n! Obsolete data mask.\n✖ Too late to resolve `rate` after the end of `dplyr::summarise()`.\nℹ Did you save an object that uses `rate` lazily in a column in the\n  `dplyr::summarise()` expression ?\n\n\nThe reason for this as follows: When nls fit the model it didn’t remember the actual values of rate and conc, it just made a note that these are columns available in the data. And because the data was not passed to it explicitly it just wrote down that the columns are available in the environment in which it was called, which at that time was inside of summarise. Check out the data argument here:\n\nnewmodels$model[[1]]\n\nNonlinear regression model\n  model: rate ~ rate(conc, Vm, K)\n   data: parent.frame()\n       Vm         K \n212.68368   0.06412 \n residual sum-of-squares: 1195\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.528e-06\n\n\nIt just says parent.frame, meaning “the environment around me”. But once it has left the context of summarise, this is no longer available, so it can’t find the rate column. This is why is is always safer to pass the data explicitly like we did in the approach that worked."
  },
  {
    "objectID": "07-fallacies-correlation-and-regression.html#exercises",
    "href": "07-fallacies-correlation-and-regression.html#exercises",
    "title": "\n7  Fallacies, Correlation and Regression\n",
    "section": "\n7.6 Exercises",
    "text": "7.6 Exercises\n\n7.6.1 The Datasaurus Dozen\nThe Datasaurus Dozen (Matejka and Fitzmaurice 2017) is a dataset crafted to illustrate certain concepts. It can be accessed from R via the datasauRus package.\n\ndatasauRus::datasaurus_dozen\n\n\nExplore the dataset before looking at the publication above (it contains spoilers…):\n\nIt actually contains 13 different datasets, denoted by the column dataset, in one tibble. What are the means for x and y for the different datasets? What are the standard deviations for x and y for the different datasets? What are the correlations coefficients for the different datasets? I bet you notice a pattern by now.\nNow create one (or multiple) scatterplots of the data. What do you notice? what conclusions do you draw from this observation?\n\n\n\nThere is another dataset in the package to illustrate a different point:\n\ndatasauRus::box_plots\n\n\nFirst, turn it into a tidy format, much like the datasaurus_dozen tibble.\nNow, visualize the distributions of the values for the 5 different groups. Try out different versions of your plot until you are satisfied, but be sure to also include a boxplot and compare it to your approaches. What do you find?\n\n7.6.2 Fit a non-linear model\nI found this gloriously 2000s website for “Statistical Reference Datasets”: https://www.itl.nist.gov/div898/strd/index.html by the Information Technology Laboratory. Not only has this official website of the United Stats Government amazing unapologetic Word-Art, it also features some handy datasets to practice fitting non-linear models (https://itl.nist.gov/div898/strd/nls/nls_main.shtml)!\nOf these I chose one for you to explore: https://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Chwirut2.dat\nBecause you might come across some challenges, I am leaving some tips below, hidden behind details panels, so you can choose if and when you need them:\n\nTip 1\n\nYou can read in data that is separated by whitespace with readrs function read_table.\n\nTip 2\n\nYou have to skip the first 60 lines and set column names manually.\n\nTip 3\n\nThe description in the dataset header also contains the function to fit and potential starting values to try out. Note, the e in the function refers to the remaining error of the fit, so you don’t need it in your function.\n\n\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “The Datasaurus Dozen - Same Stats, Different Graphs | Autodesk Research.” https://doi.org/http://dx.doi.org/10.1145/3025453.3025912.\n\n\n“Survivorship Bias.” 2020. Wikipedia, December.\n\n\nWald, Abraham. 1980. “A Reprint of ’A Method of Estimating Plane Vulnerability Based on Damage of Survivors.” CRC-432. CENTER FOR NAVAL ANALYSES ALEXANDRIA VA OPERATIONS EVALUATION GROUP.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "08-freestyle.html#setup",
    "href": "08-freestyle.html#setup",
    "title": "\n8  Freestyle\n",
    "section": "\n8.1 Setup",
    "text": "8.1 Setup\nToday we are using quite a bunch of packages. In the lecture these are of course introduced one by one, but because it is a good habit to have all your imports and dependencies near the top of your analysis they show up here already.\n\nlibrary(multcomp)\nlibrary(patchwork)\nlibrary(gganimate)\nlibrary(palmerpenguins)\nlibrary(broom)\nlibrary(tidyverse)"
  },
  {
    "objectID": "08-freestyle.html#anova",
    "href": "08-freestyle.html#anova",
    "title": "\n8  Freestyle\n",
    "section": "\n8.2 ANOVA",
    "text": "8.2 ANOVA\nLet’s get started. ANOVA stands for Analysis of Variance. We start with our familiar penguins dataset and ask the question: “Is there a difference in bill length between the species?”\n\npenguins %&gt;% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2)\n\n\n\n\nOptically, it certainly looks like it. We could run a bunch of t-tests to compare between the groups, which would be 3 in total (Adelie–Chinstrap, Adelie–Gentoo, Gentoo–Chinstrap). And then we would have to correct for multiple testing. At this point, only running one of tests (like Adelie vs. Chinstrap) because it looks so promising is not an option! Looking at the data visually is a form of comparison, so we would be cheating if we formed our hypothesis only after looking at the data.\nANOVA is a way of comparing multiple groups and tells us, if there is a difference at all between the groups. It does not however tell us, between which groups the difference exists, just that there is an overall difference. In order to get p-values for direct comparisons between the groups we run a post-hoc (Latin for “after the fact”) on our anova result.\nFirst I am sliding in a little extra code chunk, because later down the line we might want to compare all groups to e.g. a baseline or control group (see the section about Dunnet below) and this baseline is decided to be the first factor-level of the grouping variable. With fct_relevel we can move a lovel to the first (or any other) position.\n\npenguins &lt;- penguins %&gt;% \n  mutate(species = fct_relevel(species, \"Gentoo\"))\n\nNot we create an anova fit with aov.\n\nanova_fit &lt;- aov(bill_length_mm ~ species, data = penguins)\nanova_fit\n\nCall:\n   aov(formula = bill_length_mm ~ species, data = penguins)\n\nTerms:\n                 species Residuals\nSum of Squares  7194.317  2969.888\nDeg. of Freedom        2       339\n\nResidual standard error: 2.959853\nEstimated effects may be unbalanced\n2 observations deleted due to missingness\n\n\nAnd explore if further with various functions.\n\nsummary(anova_fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspecies       2   7194    3597   410.6 &lt;2e-16 ***\nResiduals   339   2970       9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\n\n\ntidy(anova_fit)\n\n# A tibble: 2 × 6\n  term         df sumsq  meansq statistic   p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 7194. 3597.        411.  2.69e-91\n2 Residuals   339 2970.    8.76       NA  NA       \n\n\n\n8.2.1 Tukey Post-Hoc Test\nThe most straighforward post-hoc test, which is already built into R, is “Tukey’s Honest Significant Differences”, which compares all groups with all other groups.\n\nTukeyHSD(anova_fit) %&gt;% \n  tidy()\n\n# A tibble: 3 × 7\n  term    contrast         null.value estimate conf.low conf.high adj.p.value\n  &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 species Adelie-Gentoo             0    -8.71   -9.56      -7.87     0      \n2 species Chinstrap-Gentoo          0     1.33    0.276      2.38     0.00890\n3 species Chinstrap-Adelie          0    10.0     9.02      11.1      0      \n\n\n\n8.2.2 Dunnet Post-Hoc Test\nIf we have a control group we use Dunnet’s test to compare all other groups to the control group. This means we have to do less comparisons and end up with a higher statistical power.\nUnfortunately, Dunnet is not built into R, but we can get the necessary functions from the multcomp package. When loading the multcomp package we have to be careful to load it before the tidyverse! Because multcomp also loads a bunch of other packages and one of them brings it’s own select function, so loading it after the tidyverse would overwrite tidyverse select and make our life very hard.\n\nglht(anova_fit, mcp(species = \"Dunnet\")) %&gt;% \n  tidy()\n\n# A tibble: 2 × 7\n  term    contrast           null.value estimate std.error statistic adj.p.value\n  &lt;chr&gt;   &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 species Adelie - Gentoo             0    -8.71     0.360    -24.2    -4.44e-16\n2 species Chinstrap - Gentoo          0     1.33     0.447      2.97    6.20e- 3\n\n\nNote, that this package can also do Tukey’s test by changing “Dunnet” to “Tukey”."
  },
  {
    "objectID": "08-freestyle.html#ggplot-extensions",
    "href": "08-freestyle.html#ggplot-extensions",
    "title": "\n8  Freestyle\n",
    "section": "\n8.3 ggplot extensions",
    "text": "8.3 ggplot extensions\nIn order to plot these nice significance stars on top of of our comparisons let me first introduce you to ggplot extensions:\nhttps://exts.ggplot2.tidyverse.org/\nThis is just a collection of packages that extend ggplot in some shape or form or simply work very well with it.\n\n8.3.1 ggsignif, ggpubr\n\nOne of these is ggsignif, which we can use to add comparison brackets to our plot.\n\npenguins %&gt;% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2) +\n  ggsignif::geom_signif(\n    comparisons = list(c(\"Gentoo\", \"Adelie\")),\n    annotations = c(\"hello\")\n  )\n\n\n\n\nggsignif can also run it’s own comparisons, which is a wilcoxon rank sum test by default, but especially for statistical tests I prefer to run them myself first before trusting more packages. Additionally, we have of course a different type of test (ANOVA), and we already have our values, so we use it just to manually add text to the annotation:\n\nstars &lt;- function(p) {\n  case_when(\n    p &lt;= 0.001 ~ \"***\",\n    p &lt;= 0.01  ~ \"**\",\n    p &lt;= 0.05  ~ \"*\",\n    TRUE       ~ \"ns\"\n  )\n}\n\ndunnet &lt;- glht(anova_fit, mcp(species = \"Dunnet\")) %&gt;% \n  tidy() %&gt;% \n  mutate(contrast = str_split(contrast, \" - \"),\n         stars    = stars(adj.p.value))\n\nplt &lt;- penguins %&gt;% \n  ggplot(aes(species, bill_length_mm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.2, width = 0.2) +\n  ggsignif::geom_signif(\n    comparisons = dunnet$contrast,\n    annotations = dunnet$stars,\n    y_position = c(60, 65)\n  )\n\nplt\n\n\n\n\n\n8.3.2 patchwork\n\nI also saved the plot to a variable and will now create a second one just to show you the patchwork package, which can combine plots into neat layouts:\n\nplt2 &lt;- ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +\n  geom_point()\n\n((plt | plt2) / plt2 ) + plot_layout(guides = 'collect')\n\n\n\n\n\n8.3.3 ggfortify\n\nggfortiy might not strictly be necessary but I want to mention it to talk about autoplot. autoplot is in ggplot2 already by default. It creates automatic plot for certain objects, like models for example, or an anova result. ggfortify makes more objects have the ability to generate an autoplot.\n\nlibrary(ggfortify)\nautoplot(anova_fit)\n\n\n\n\nI want to mention this here because it has an important implication for communication. There are two purposes too plots. The first purpose is to generate insight for you, the data analyst. Autoplots are often very helpful for this. But the second purpose is to communicate your findings to a reader! It takes time and effort to refine a plot from something that generated and insight for you after having spent a lot of time with your data to also generating insights for a reader that sees the data for the first time. Too many people stop at the first step and publish their plots as is, without thinking much about the reader. Keep this in mind as you advance in your scientific career.\n\nCommunication is key! Both code and plots are means of communicating.\n\n\n8.3.4 esquisse\n\nTwo more cool packages: The esquisse package let’s you create ggplots interactively with a gui! But make sure to always save the code this created to ensure reproducibility!\n\n8.3.5 gganimate\n\nAnd gganimate extends ggplot with the ability to use time as a dimension:\n\ngapminder::gapminder %&gt;% \n  ggplot(aes(gdpPercap, lifeExp, size = pop, color = country)) +\n  geom_point() +\n  scale_x_log10() +\n  transition_time(year) +\n  scale_color_manual(values = gapminder::country_colors) +\n  guides(color = \"none\",\n         size = \"none\")\n\nNULL"
  },
  {
    "objectID": "08-freestyle.html#build-apps-with-shiny",
    "href": "08-freestyle.html#build-apps-with-shiny",
    "title": "\n8  Freestyle\n",
    "section": "\n8.4 Build Apps with shiny\n",
    "text": "8.4 Build Apps with shiny\n\nYou can find the code written during the lecture in: example-app/."
  },
  {
    "objectID": "08-freestyle.html#exercises",
    "href": "08-freestyle.html#exercises",
    "title": "\n8  Freestyle\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n\n8.5.1 The Whole Deal\nData analysis takes practice. For this last exercise you get the option to show what you have learned on a fresh dataset. But people have different interests, so I am leaving the choice open.\nChoose a dataset from the TidyTuesday project:\nhttps://github.com/rfordatascience/tidytuesday#datasets\nand write your data analysis report. Aim to write down some questions you have about the topic and answer them with the available data. Explore it with plots and tables and try to end up with 1 or 2 high quality plots that clearly communicate you findings. Above all, have fun!"
  },
  {
    "objectID": "08-freestyle.html#feedback",
    "href": "08-freestyle.html#feedback",
    "title": "\n8  Freestyle\n",
    "section": "\n8.6 Feedback",
    "text": "8.6 Feedback\nI will send round a link for a feedback form. It is anonymous."
  },
  {
    "objectID": "resources.html#learning-the-tidyverse",
    "href": "resources.html#learning-the-tidyverse",
    "title": "Resources",
    "section": "Learning the tidyverse",
    "text": "Learning the tidyverse\n\nR for Data Science (Wickham and Grolemund 2017)\nThe ggplot2 book\nR4DS online Community\nRStudio Cheat Sheets!\nThe Modern Dive (Kim 2019)\nRStudio Education"
  },
  {
    "objectID": "resources.html#learning-quarto",
    "href": "resources.html#learning-quarto",
    "title": "Resources",
    "section": "Learning Quarto",
    "text": "Learning Quarto\n\nquarto website\nmarkdown syntax reference\npandoc manual (advanced)"
  },
  {
    "objectID": "resources.html#learning-r-in-general",
    "href": "resources.html#learning-r-in-general",
    "title": "Resources",
    "section": "Learning R in general",
    "text": "Learning R in general\n\nAdvanced R (Wickham 2019)\nHands on Programming with R (Grolemund and Wickham 2014)\nR Packages (Wickham 2015)\nData Visualization: A Practical Introduction (Healy 2018)\nGraph Cookbook (Chang 2013)"
  },
  {
    "objectID": "resources.html#learning-statistics",
    "href": "resources.html#learning-statistics",
    "title": "Resources",
    "section": "Learning Statistics",
    "text": "Learning Statistics\n\nIntuitive Biostatistics (Motulsky 2017)\nStatistics Done Wrong (Reinhart 2015)\nStatQuest!!! with Josh Starner\nModern Statistics for Modern Biology"
  },
  {
    "objectID": "resources.html#helpful-tools",
    "href": "resources.html#helpful-tools",
    "title": "Resources",
    "section": "Helpful tools",
    "text": "Helpful tools\n\nGenerate ggplots via graphical user interface with esquisee"
  },
  {
    "objectID": "resources.html#talks-podcasts-blogs-videos",
    "href": "resources.html#talks-podcasts-blogs-videos",
    "title": "Resources",
    "section": "Talks, Podcasts, Blogs, Videos",
    "text": "Talks, Podcasts, Blogs, Videos\nJust some of the people with inspiring blogposts, videos and the likes.\n\nDavid Robinson\n\nYouTube\nwebsite\n\nJulia Silge\n\nYouTube\nwebsite\n\nAlison Hill\n\nwebsite\n\nThomas Lin Pedersen\n\nwebsite"
  },
  {
    "objectID": "resources.html#misc",
    "href": "resources.html#misc",
    "title": "Resources",
    "section": "Misc",
    "text": "Misc\n\nCute and insightful illustrations (Horst 2020)\nHappy Git with R"
  },
  {
    "objectID": "resources.html#package-documentation",
    "href": "resources.html#package-documentation",
    "title": "Resources",
    "section": "Package Documentation",
    "text": "Package Documentation\n\ntidyverse\ntidymodels\nrmarkdown\nreadr\ndplyr\nggplot\ntidyr\nstringr\npurrr\nragg"
  },
  {
    "objectID": "resources.html#books-and-manuals",
    "href": "resources.html#books-and-manuals",
    "title": "Resources",
    "section": "Books and Manuals",
    "text": "Books and Manuals\n\nTidymodels book\nggplot book"
  },
  {
    "objectID": "resources.html#getting-help",
    "href": "resources.html#getting-help",
    "title": "Resources",
    "section": "Getting Help",
    "text": "Getting Help\n\nHow to find help\nR4DS online learning community"
  },
  {
    "objectID": "resources.html#lists-of-resources",
    "href": "resources.html#lists-of-resources",
    "title": "Resources",
    "section": "Lists of Resources",
    "text": "Lists of Resources\nThe meta section. This is a list of lists:\n\nbig book of R\nr for the rest of us"
  },
  {
    "objectID": "resources.html#packages-that-enable-this-lecture-format",
    "href": "resources.html#packages-that-enable-this-lecture-format",
    "title": "Resources",
    "section": "Packages that enable this lecture format",
    "text": "Packages that enable this lecture format\n\nR (R Core Team 2020)\nQuarto (J. J. Allaire et al. 2022)\nknitr (Xie 2021)\nrmarkdown (J. Allaire et al. 2021)\nxaringan (Xie 2020)\n\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2021. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nChang, Winston. 2013. R Graphics Cookbook: Practical Recipes for Visualizing Data. 1 edition. Beijing Cambridge Farnham Köln Sebastopol Tokyo: O’Reilly Media.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2014. Hands-On Programming with R: Write Your Own Functions and Simulations. Sebastopol, CA: O’Reilly Media.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. 1 edition. Princeton, NJ: Princeton University Press.\n\n\nHorst, Allison. 2020. “Artwork by @Allison_horst.” https://github.com/allisonhorst/stats-illustrations. https://github.com/allisonhorst/stats-illustrations.\n\n\nKim, Chester Ismay and Albert Y. 2019. Statistical Inference via Data Science. CRC Press.\n\n\nMotulsky, Harvey. 2017. Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking. 4 edition. New York: Oxford University Press.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Manual. Vienna, Austria: R Foundation for Statistical Computing.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete Guide. 1 edition. San Francisco: No Starch Press.\n\n\nWickham, Hadley. 2015. R Packages: Organize, Test, Document, and Share Your Code. 1 edition. Sebastopol, CA: O’Reilly Media.\n\n\n———. 2019. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1 edition. Sebastopol, CA: O’Reilly Media.\n\n\nXie, Yihui. 2020. Xaringan: Presentation Ninja. Manual.\n\n\n———. 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and\nChristophe Dervieux. 2022. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin\nUshey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and\nRichard Iannone. 2021. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nBryan, Jennifer. 2017. Gapminder: Data from\nGapminder. Manual.\n\n\nChang, Winston. 2013. R Graphics Cookbook:\nPractical Recipes for Visualizing Data. 1\nedition. Beijing Cambridge Farnham Köln Sebastopol Tokyo:\nO’Reilly Media.\n\n\nCodd, E. F. 1990. The Relational Model for Database Management:\nVersion 2. USA: Addison-Wesley Longman\nPublishing Co., Inc.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2014. Hands-On\nProgramming with R: Write Your Own\nFunctions and Simulations. Sebastopol,\nCA: O’Reilly Media.\n\n\nHealy, Kieran. 2018. Data Visualization: A\nPractical Introduction. 1 edition. Princeton,\nNJ: Princeton University Press.\n\n\nHorst, Allison. 2020. “Artwork by @Allison_horst.”\nhttps://github.com/allisonhorst/stats-illustrations. https://github.com/allisonhorst/stats-illustrations.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin\nData. Manual.\n\n\nKim, Chester Ismay and Albert Y. 2019. Statistical\nInference via Data Science. CRC\nPress.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “The\nDatasaurus Dozen - Same Stats, Different\nGraphs | Autodesk Research.”\nhttps://doi.org/http://dx.doi.org/10.1145/3025453.3025912.\n\n\nMotulsky, Harvey. 2017. Intuitive Biostatistics:\nA Nonmathematical Guide to Statistical\nThinking. 4 edition. New York: Oxford\nUniversity Press.\n\n\nR Core Team. 2020. R: A Language and Environment for\nStatistical Computing. Manual. Vienna, Austria:\nR Foundation for Statistical Computing.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The\nWoefully Complete Guide. 1 edition. San\nFrancisco: No Starch Press.\n\n\n“Survivorship Bias.” 2020. Wikipedia, December.\n\n\nWald, Abraham. 1980. “A Reprint of ’A\nMethod of Estimating Plane Vulnerability Based on\nDamage of Survivors.” CRC-432.\nCENTER FOR NAVAL ANALYSES ALEXANDRIA VA OPERATIONS EVALUATION\nGROUP.\n\n\n“Welcome | Ggplot2.” n.d. https://ggplot2-book.org/.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of\nGraphics.” Journal of Computational and\nGraphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2015. R Packages: Organize,\nTest, Document, and Share Your\nCode. 1 edition. Sebastopol, CA: O’Reilly\nMedia.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. 2016 edition. New York, NY:\nSpringer.\n\n\n———. 2019. Advanced R, Second\nEdition. 2 edition. Boca Raton: Chapman\nand Hall/CRC.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience: Import, Tidy,\nTransform, Visualize, and Model\nData. 1 edition. Sebastopol, CA: O’Reilly\nMedia.\n\n\nWilkinson, Leland, D. Wills, D. Rope, A. Norton, and R. Dubbs. 2005.\nThe Grammar of Graphics. 2nd edition.\nNew York: Springer.\n\n\nWilliams, Timothy. n.d. “The Complexity of\nEvil.” Rutgers University Press.\n\n\nXie, Yihui. 2020. Xaringan: Presentation Ninja.\nManual.\n\n\n———. 2021. Knitr: A General-Purpose Package for Dynamic Report\nGeneration in r. https://yihui.org/knitr/.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name\nErrors Are Widespread in the Scientific Literature.” Genome\nBiology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What You will Learn\nThroughout your scientific career — and potentially outside of it — you will encounter various forms of data. Maybe you do an experiment and measured the fluorescence of a molecular probe, or you simply count the penguins at your local zoo. Everything is data in some form or another. But raw numbers without context are meaningless and tables of numbers are not only boring to look at, but often hide the actual structure in the data.\nIn this course you will learn to handle different kinds of data. You will learn to create pretty and insightful visualizations, compute different statistics on your data and also what these statistical concepts mean. From penguins to p-values, I got you covered.\nThe course will be held in English, as the concepts covered will directly transfer to the research you do, where the working language is English. That being said, feel free to ask questions in any language that I understand, so German is also fine. My Latin is a little rusty, thought.\nIn this course, we will be using the programming language R. R is a language particularly well suited for data analysis, because it was initially designed by statisticians and because of the interactive nature of the language, which makes it easier to get started. So don’t fret if this is your first encounter with programming, we will take one step at a time.\nThe datasets chosen to illustrate the various concepts and tools are not particularly centered around Biology. Rather, I chose general datasets that require less introduction and enable us to focus on learning R and statistics. This is why we will be talking about penguins, racing games or life expectancy instead of intricate molecular measurements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]